{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import utils\n",
    "import math\n",
    "import argparse\n",
    "import requests\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.cuda import amp\n",
    "import torch.distributed.optim\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from spikingjelly.activation_based import layer,functional,neuron,surrogate\n",
    "\n",
    "from scipy.io import loadmat,savemat\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "train_tb_writer = None\n",
    "te_tb_writer = None\n",
    "device = torch.device('cuda:0')\n",
    "deviceIds = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Classification Training')\n",
    "\n",
    "    parser.add_argument('--device', default='cuda:0', help='device')\n",
    "    parser.add_argument('-b', '--batch-size', default=32, type=int)\n",
    "    parser.add_argument('--data-path', default='./data/', help='dataset')\n",
    "    parser.add_argument('--epochs', default=320, type=int, metavar='N',\n",
    "                        help='number of total epochs to pre-train')\n",
    "    parser.add_argument('-j', '--workers', default=16, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 16)')\n",
    "    parser.add_argument('--lr', default=0.0025, type=float, help='initial learning rate')\n",
    "\n",
    "    parser.add_argument('--print-freq', default=10, type=int, help='print frequency')\n",
    "    parser.add_argument('--output-dir', default='./logs', help='path where to save')\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--tb', action='store_true',\n",
    "                        help='Use TensorBoard to record logs')\n",
    "    parser.add_argument(\n",
    "        \"--cache-dataset\",\n",
    "        dest=\"cache_dataset\",\n",
    "        help=\"Cache the datasets for quicker initialization. It also serializes the transforms\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(args=['--data-path','../data','--lr','0.1','-b','128','--epochs','100','--print-freq','100','--tb','--cache-dataset'])\n",
    "    return args\n",
    "\n",
    "args = parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkB(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkB, self).__init__()\n",
    "        self.T = 10\n",
    "        self.conv1 = nn.Conv2d(1, 96, kernel_size=3, padding=1) \n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "        self.sn1 = neuron.IFNode(detach_reset=True)\n",
    "\n",
    "        self.conv2 = layer.SeqToANNContainer(nn.Conv2d(96, 96, kernel_size=3, padding=1),nn.BatchNorm2d(96)) \n",
    "        self.sn2 = neuron.IFNode(detach_reset=True)\n",
    "        self.pool1 = layer.SeqToANNContainer(nn.MaxPool2d(2,2)) \n",
    "\n",
    "        self.conv3 = layer.SeqToANNContainer(nn.Conv2d(96, 128, kernel_size=3, padding=1),nn.BatchNorm2d(128)) \n",
    "        self.sn3 = neuron.IFNode(detach_reset=True)\n",
    "      \n",
    "        self.conv4 = layer.SeqToANNContainer(nn.Conv2d(128, 128, kernel_size=3, padding=1),nn.BatchNorm2d(128)) \n",
    "        self.sn4 = neuron.IFNode(detach_reset=True)\n",
    "        self.pool2 = layer.SeqToANNContainer(nn.MaxPool2d(2,2)) \n",
    "\n",
    "        self.conv5 = layer.SeqToANNContainer(nn.Conv2d(128, 256, kernel_size=3, padding=1),nn.BatchNorm2d(256)) \n",
    "        self.sn5 = neuron.IFNode(detach_reset=True)\n",
    "\n",
    "        self.pool3 = layer.SeqToANNContainer(nn.MaxPool2d(2,2)) \n",
    "\n",
    "        self.conv8 = layer.SeqToANNContainer(nn.Conv2d(256, 512, kernel_size=3, padding=1),nn.BatchNorm2d(512)) \n",
    "        self.sn8 = neuron.IFNode(detach_reset=True)\n",
    "\n",
    "        self.conv10 = layer.SeqToANNContainer(nn.Conv2d(512, 512, kernel_size=3, padding=1),nn.BatchNorm2d(512)) \n",
    "        self.sn10 = neuron.IFNode(detach_reset=True)\n",
    "\n",
    "        self.pool4 = layer.SeqToANNContainer(nn.MaxPool2d(2,2)) \n",
    "\n",
    "        self.conv11 = layer.SeqToANNContainer(nn.Conv2d(512, 256, kernel_size=3, padding=1),nn.BatchNorm2d(256)) \n",
    "        self.sn11 = neuron.IFNode(detach_reset=True)\n",
    "\n",
    "        self.conv13 = layer.SeqToANNContainer(nn.Conv2d(256, 128, kernel_size=3, padding=1),nn.BatchNorm2d(128)) \n",
    "        self.sn13 = neuron.IFNode(detach_reset=True)\n",
    "\n",
    "        self.pool5 = layer.SeqToANNContainer(nn.MaxPool2d(2,2)) \n",
    "\n",
    "        self.fc1 = layer.SeqToANNContainer(nn.Linear(256, 35),nn.BatchNorm1d(35))\n",
    "        self.sn14 = neuron.IFNode(detach_reset=True)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = self.T\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x.unsqueeze_(0)\n",
    "        x = x.repeat(10, 1, 1, 1, 1)\n",
    "        x = self.sn1(x)\n",
    "\n",
    "        x = self.sn2(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "      \n",
    "        x = self.sn3(self.conv3(x))\n",
    "        x = self.sn4(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "\n",
    "        x = self.sn5(self.conv5(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "\n",
    "        x = self.sn8(self.conv8(x))\n",
    "        x = self.sn10(self.conv10(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.sn11(self.conv11(x))\n",
    "        \n",
    "        x = x[:T, :, :, :, :]\n",
    "        x = self.sn13(self.conv13(x))\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        x = torch.flatten(x,2)\n",
    "\n",
    "        x = self.sn14(self.fc1(x))\n",
    "       \n",
    "        return x.mean(0)\n",
    "    \n",
    "    def set_T(self, T):\n",
    "        self.T = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "max_test_acc1 = 0.\n",
    "test_acc5_at_max_test_acc1 = 0.\n",
    "\n",
    "utils.init_distributed_mode(args)\n",
    "print(args)\n",
    "output_dir = os.path.join(args.output_dir, f'b_{args.batch_size}_lr{args.lr}')\n",
    "\n",
    "time_now = datetime.datetime.now()\n",
    "output_dir += f'_{time_now.year}_{time_now.month}_{time_now.day}_{time_now.hour}__{time_now.minute}'\n",
    "\n",
    "\n",
    "if output_dir:\n",
    "    utils.mkdir(output_dir)\n",
    "    \n",
    "class SpeechDataLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, mfccs, labels, label_dict):\n",
    "        self.mfccs = mfccs\n",
    "        self.labels = labels\n",
    "        self.label_dict = label_dict\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.mfccs)    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels[idx] in self.label_dict:\n",
    "            out_labels = self.label_dict.index(self.labels[idx])\n",
    "            return self.mfccs[idx], out_labels\n",
    "        else:\n",
    "            raise ValueError(\"Label not found in label_dict.\")\n",
    "\n",
    "def save_dataset(mfccs, labels, label_dict, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump((mfccs, labels, label_dict), f)\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# 预处理和加载数据\n",
    "def preprocess_and_load_data():\n",
    "    train_audio_path = './data/SpeechCommands/speech_commands_v0.02/'\n",
    "    labels_dict = [label for label in os.listdir(train_audio_path) if label not in ['_background_noise_', 'LICENSE', 'README.md', 'validation_list.txt', 'testing_list.txt', '.DS_Store']]\n",
    "\n",
    "    a = torchaudio.datasets.SPEECHCOMMANDS('./data/', url='speech_commands_v0.02', \n",
    "                                            folder_in_archive='SpeechCommands', download=True)\n",
    "    \n",
    "    mfccs = []\n",
    "    labels = []\n",
    "    transform = nn.Sequential(\n",
    "        torchaudio.transforms.MFCC(log_mels=False)\n",
    "    )\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        if a[i][0].shape == (1, 16000):\n",
    "            waveform = a[i][0]\n",
    "            mfcc = transform(waveform)\n",
    "            mfccs.append(mfcc)\n",
    "            labels.append(a[i][2])\n",
    "\n",
    "    return mfccs, labels, labels_dict\n",
    "\n",
    "dataset_filename = 'processed_dataset.pkl'\n",
    "\n",
    "if os.path.exists(dataset_filename):\n",
    "    mfccs, labels, labels_dict = load_dataset(dataset_filename)\n",
    "else:\n",
    "    mfccs, labels, labels_dict = preprocess_and_load_data()\n",
    "    save_dataset(mfccs, labels, labels_dict, dataset_filename)\n",
    "\n",
    "dataset = SpeechDataLoader(mfccs, labels, labels_dict)\n",
    "traindata, testdata = random_split(dataset, [round(len(dataset) * 0.8), round(len(dataset) * 0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(traindata, batch_size=args.batch_size, shuffle=True)\n",
    "data_loader_test = torch.utils.data.DataLoader(testdata, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_dir:\n",
    "    utils.mkdir(output_dir)\n",
    "print(\"Creating model\")\n",
    "\n",
    "net = NetworkB().to(device)\n",
    "functional.set_step_mode(net,step_mode='m')\n",
    "functional.set_backend(net, backend='cupy')\n",
    "net.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.4, last_epoch=-1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if args.resume:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    args.start_epoch = checkpoint['epoch'] + 1\n",
    "    max_test_acc1 = checkpoint['max_test_acc1']\n",
    "    test_acc5_at_max_test_acc1 = checkpoint['test_acc5_at_max_test_acc1']\n",
    "\n",
    "if args.tb and utils.is_main_process():\n",
    "    purge_step_train = args.start_epoch\n",
    "    purge_step_te = args.start_epoch\n",
    "    train_tb_writer = SummaryWriter(output_dir + '_logs/train', purge_step=purge_step_train)\n",
    "    te_tb_writer = SummaryWriter(output_dir + '_logs/te', purge_step=purge_step_te)\n",
    "    with open(output_dir + '_logs/args.txt', 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write(str(args))\n",
    "\n",
    "    with open(output_dir + '_logs/resluts.txt', 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write('Results\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.int = int\n",
    "def train_one_epoch(net, criterion, data_loader, device, epoch, print_freq, scaler=None,lr = 1e-2):\n",
    "    net.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value}'))\n",
    "    metric_logger.add_meter('img/s', utils.SmoothedValue(window_size=10, fmt='{value}'))\n",
    "\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "\n",
    "    for image, target in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        start_time = time.time()\n",
    "        image, target = image.to(device), target.to(device)\n",
    "\n",
    "        output = net(image)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        functional.reset_net(net)\n",
    "\n",
    "        acc1, acc5 = utils.accuracy(output, target, topk=(1, 5))\n",
    "        batch_size = image.shape[0]\n",
    "        loss_s = loss.item()\n",
    "        if math.isnan(loss_s):\n",
    "            raise ValueError('loss is Nan')\n",
    "        acc1_s = acc1.item()\n",
    "        acc5_s = acc5.item()\n",
    "\n",
    "        metric_logger.update(loss=loss_s, lr=lr)\n",
    "\n",
    "        metric_logger.meters['acc1'].update(acc1_s, n=batch_size)\n",
    "        metric_logger.meters['acc5'].update(acc5_s, n=batch_size)\n",
    "        metric_logger.meters['img/s'].update(batch_size / (time.time() - start_time))\n",
    "\n",
    "   \n",
    "    metric_logger.synchronize_between_processes()\n",
    "    return metric_logger.loss.global_avg, metric_logger.acc1.global_avg, metric_logger.acc5.global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, criterion, data_loader, device, print_freq=100, header='Test:'):\n",
    "    net.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    with torch.no_grad():\n",
    "        for image, target in metric_logger.log_every(data_loader, print_freq, header):\n",
    "            image = image.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "            output = net(image)\n",
    "            loss = criterion(output, target)\n",
    "            functional.reset_net(net)\n",
    "\n",
    "            acc1, acc5 = utils.accuracy(output, target, topk=(1, 5))\n",
    "           \n",
    "            batch_size = image.shape[0]\n",
    "            metric_logger.update(loss=loss.item())\n",
    "            metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "   \n",
    "    metric_logger.synchronize_between_processes()\n",
    "\n",
    "    loss, acc1, acc5 = metric_logger.loss.global_avg, metric_logger.acc1.global_avg, metric_logger.acc5.global_avg\n",
    "    print(f' * Acc@1 = {acc1}, Acc@5 = {acc5}, loss = {loss}')\n",
    "    return loss, acc1, acc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.int = int\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    save_max = False\n",
    "\n",
    "    train_loss, train_acc1, train_acc5 = train_one_epoch(net, criterion,data_loader, device, epoch, args.print_freq,lr=args.lr)\n",
    "\n",
    "    if utils.is_main_process():\n",
    "        train_tb_writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        train_tb_writer.add_scalar('train_acc1', train_acc1, epoch)\n",
    "        train_tb_writer.add_scalar('train_acc5', train_acc5, epoch)\n",
    "\n",
    "    test_loss, test_acc1, test_acc5 = evaluate(net, criterion, data_loader_test, device=device, header='Test:')\n",
    "    if te_tb_writer is not None:\n",
    "        if utils.is_main_process():\n",
    "            te_tb_writer.add_scalar('test_loss', test_loss, epoch)\n",
    "            te_tb_writer.add_scalar('test_acc1', test_acc1, epoch)\n",
    "            te_tb_writer.add_scalar('test_acc5', test_acc5, epoch)\n",
    "\n",
    "\n",
    "    if max_test_acc1 < test_acc1:\n",
    "        max_test_acc1 = test_acc1\n",
    "        test_acc5_at_max_test_acc1 = test_acc5\n",
    "        save_max = True\n",
    "\n",
    "    if output_dir:\n",
    "\n",
    "        checkpoint = {\n",
    "            'model': net.state_dict(),\n",
    "            'pre-train_epoch': epoch,\n",
    "            'args': args,\n",
    "            'max_test_acc1': max_test_acc1,\n",
    "            'test_acc5_at_max_test_acc1': test_acc5_at_max_test_acc1,\n",
    "        }\n",
    "\n",
    "        utils.save_on_master(\n",
    "            checkpoint,\n",
    "            os.path.join(output_dir, 'checkpoint_latest.pth'))\n",
    "        save_flag = False\n",
    "\n",
    "        if epoch % 64 == 0 or epoch == args.epochs - 1:\n",
    "            save_flag = True\n",
    "\n",
    "\n",
    "        if save_flag:\n",
    "            utils.save_on_master(\n",
    "                checkpoint,\n",
    "                os.path.join(output_dir, f'checkpoint_{epoch}.pth'))\n",
    "\n",
    "        if save_max:\n",
    "            utils.save_on_master(\n",
    "                checkpoint,\n",
    "                os.path.join(output_dir,f'train_maxacc1_{max_test_acc1}_checkpoint_max_test_acc1_all_pretrain.pth'))\n",
    "            torch.save(net,os.path.join(output_dir,f'train_maxacc1_{max_test_acc1}_checkpoint_max_test_acc1_all_pretrain.pth'))\n",
    "            torch.save(net.state_dict(),os.path.join(output_dir,f'train_maxacc1_{max_test_acc1}_checkpoint_max_test_acc1_state_pretrain.pth'))\n",
    "    print(args)\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print(output_dir)\n",
    "\n",
    "    print('Training time {}'.format(total_time_str), 'max_test_acc1', max_test_acc1,\n",
    "            'test_acc5_at_max_test_acc1', test_acc5_at_max_test_acc1,'train_acc1', train_acc1,\n",
    "            'train_acc5', train_acc1)\n",
    "\n",
    "    Train_logs= {\n",
    "        'Epoch:': epoch,\n",
    "        'max_test_acc1 ': max_test_acc1 ,\n",
    "        'test_acc5_at_max_test_acc1 ': test_acc5_at_max_test_acc1,\n",
    "        'train_acc1 ': train_acc1,\n",
    "        'train_acc5 ': train_acc1,\n",
    "        'args': args\n",
    "    }\n",
    "    with open(output_dir + '_logs/args.txt', 'a', encoding='utf-8') as args_txt:\n",
    "        args_txt.write('\\n')\n",
    "        args_txt.write(str(Train_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(output_dir,f'train_maxacc1_{max_test_acc1}_checkpoint_max_test_acc1_state_pretrain.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [100/150]  eta: 0:00:05  loss: 3.1798 (3.1796)  acc1: 8.5938 (8.8567)  acc5: 52.3438 (51.5857)  time: 0.0996  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 8.721631112691773, Acc@5 = 51.58551286745074, loss = 3.1793286403020224\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 3.1978 (3.1978)  acc1: 9.3750 (9.3750)  acc5: 47.6562 (47.6562)  time: 0.1005  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 3.1576 (3.1780)  acc1: 9.3750 (8.7639)  acc5: 52.3438 (51.7095)  time: 0.0996  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 8.721631112641786, Acc@5 = 51.585512867350765, loss = 3.1771752500534056\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 3.1464 (3.1464)  acc1: 7.8125 (7.8125)  acc5: 57.0312 (57.0312)  time: 0.1003  data: 0.0017  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 3.1629 (3.1769)  acc1: 7.8125 (8.6943)  acc5: 52.3438 (51.4774)  time: 0.0996  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 8.721631112741758, Acc@5 = 51.585512867350765, loss = 3.17815017859141\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 3.1478 (3.1478)  acc1: 8.5938 (8.5938)  acc5: 56.2500 (56.2500)  time: 0.1004  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 3.1729 (3.1816)  acc1: 7.8125 (8.6247)  acc5: 53.1250 (51.4619)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 8.721631112741758, Acc@5 = 51.58551286745074, loss = 3.179367291132609\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 3.1971 (3.1971)  acc1: 12.5000 (12.5000)  acc5: 51.5625 (51.5625)  time: 0.1004  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 3.1712 (3.1750)  acc1: 7.8125 (8.8645)  acc5: 50.7812 (51.9106)  time: 0.0996  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 8.721631112741758, Acc@5 = 51.585512867500725, loss = 3.177696002324422\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 3.1557 (3.1557)  acc1: 7.8125 (7.8125)  acc5: 53.9062 (53.9062)  time: 0.1003  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 3.1526 (3.1732)  acc1: 7.0312 (8.8026)  acc5: 52.3438 (52.2123)  time: 0.0996  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 8.7216311129417, Acc@5 = 51.585512867350765, loss = 3.1766664218902587\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 3.1851 (3.1851)  acc1: 7.8125 (7.8125)  acc5: 50.0000 (50.0000)  time: 0.1004  data: 0.0017  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 3.1690 (3.1812)  acc1: 7.8125 (8.5009)  acc5: 50.7812 (51.1525)  time: 0.0998  data: 0.0012  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 8.721631112741758, Acc@5 = 51.58551286835048, loss = 3.1765066480636595\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 3.1414 (3.1414)  acc1: 10.9375 (10.9375)  acc5: 53.1250 (53.1250)  time: 0.1005  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 3.1692 (3.1767)  acc1: 7.8125 (8.8103)  acc5: 50.0000 (51.7249)  time: 0.0996  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 8.721631112691773, Acc@5 = 51.585512867350765, loss = 3.178344348271688\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 3.1174 (3.1174)  acc1: 8.5938 (8.5938)  acc5: 56.2500 (56.2500)  time: 0.1004  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 3.1875 (3.1795)  acc1: 7.8125 (8.6866)  acc5: 50.7812 (51.4619)  time: 0.0996  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 8.721631112741758, Acc@5 = 51.585512867350765, loss = 3.1763172737757364\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 3.1494 (3.1494)  acc1: 7.0312 (7.0312)  acc5: 51.5625 (51.5625)  time: 0.1006  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 3.1747 (3.1748)  acc1: 8.5938 (8.5087)  acc5: 52.3438 (51.7636)  time: 0.0998  data: 0.0012  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 8.721631112691773, Acc@5 = 51.58551286775065, loss = 3.178243565559387\n",
      "Test:  [  0/150]  eta: 0:00:16  loss: 2.9300 (2.9300)  acc1: 56.2500 (56.2500)  acc5: 77.3438 (77.3438)  time: 0.1100  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.9338 (2.9289)  acc1: 51.5625 (53.0167)  acc5: 80.4688 (81.4124)  time: 0.0998  data: 0.0012  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 53.19985324218405, Acc@5 = 81.62377483136586, loss = 2.928017692565918\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.9025 (2.9025)  acc1: 56.2500 (56.2500)  acc5: 82.8125 (82.8125)  time: 0.1006  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.9217 (2.9302)  acc1: 52.3438 (52.9780)  acc5: 82.0312 (81.4279)  time: 0.0996  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 53.19985324218405, Acc@5 = 81.62377483096598, loss = 2.9273099152247113\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.9192 (2.9192)  acc1: 53.9062 (53.9062)  acc5: 85.9375 (85.9375)  time: 0.1011  data: 0.0023  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.9192 (2.9280)  acc1: 53.9062 (53.7361)  acc5: 81.2500 (81.5439)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 53.1998532419841, Acc@5 = 81.62377483136586, loss = 2.9281008036931357\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.9270 (2.9270)  acc1: 50.7812 (50.7812)  acc5: 82.8125 (82.8125)  time: 0.1006  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.9146 (2.9275)  acc1: 54.6875 (53.2874)  acc5: 82.8125 (81.9075)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 53.19985324168419, Acc@5 = 81.62377483176576, loss = 2.929582691192627\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.9439 (2.9439)  acc1: 53.1250 (53.1250)  acc5: 78.1250 (78.1250)  time: 0.1006  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.9341 (2.9273)  acc1: 52.3438 (53.2024)  acc5: 78.9062 (81.5594)  time: 0.0995  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 53.199853241584215, Acc@5 = 81.62377483176576, loss = 2.9280622227986655\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.9329 (2.9329)  acc1: 52.3438 (52.3438)  acc5: 80.4688 (80.4688)  time: 0.1008  data: 0.0021  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.9330 (2.9309)  acc1: 52.3438 (52.6686)  acc5: 80.4688 (81.5207)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 53.19985324258393, Acc@5 = 81.62377483096598, loss = 2.927850481669108\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.9549 (2.9549)  acc1: 51.5625 (51.5625)  acc5: 78.9062 (78.9062)  time: 0.1006  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.9213 (2.9283)  acc1: 50.7812 (53.4344)  acc5: 81.2500 (81.7450)  time: 0.0998  data: 0.0012  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 53.199853241584215, Acc@5 = 81.62377483176576, loss = 2.9287480767567953\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.9595 (2.9595)  acc1: 51.5625 (51.5625)  acc5: 74.2188 (74.2188)  time: 0.1004  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.9253 (2.9282)  acc1: 52.3438 (53.1791)  acc5: 82.0312 (81.7218)  time: 0.0996  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 53.1998532419841, Acc@5 = 81.62377483076604, loss = 2.928865048090617\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.9517 (2.9517)  acc1: 50.7812 (50.7812)  acc5: 80.4688 (80.4688)  time: 0.1010  data: 0.0022  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.9353 (2.9292)  acc1: 52.3438 (52.9007)  acc5: 79.6875 (81.5981)  time: 0.0996  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 53.19985324168419, Acc@5 = 81.62377483176576, loss = 2.928900682131449\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.9200 (2.9200)  acc1: 50.7812 (50.7812)  acc5: 82.8125 (82.8125)  time: 0.1007  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.9172 (2.9299)  acc1: 53.1250 (52.9162)  acc5: 82.0312 (81.3660)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 53.199853241584215, Acc@5 = 81.62377483076604, loss = 2.9284426259994505\n",
      "Test:  [  0/150]  eta: 0:00:16  loss: 2.8385 (2.8385)  acc1: 76.5625 (76.5625)  acc5: 92.9688 (92.9688)  time: 0.1103  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.8292 (2.8344)  acc1: 78.9062 (79.5483)  acc5: 95.3125 (94.6627)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 79.46433251198623, Acc@5 = 94.65380785236431, loss = 2.8350489791234335\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.8354 (2.8354)  acc1: 80.4688 (80.4688)  acc5: 93.7500 (93.7500)  time: 0.1006  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.8218 (2.8330)  acc1: 81.2500 (79.7416)  acc5: 96.0938 (94.7324)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 79.46433251198623, Acc@5 = 94.65380785196443, loss = 2.8349737119674683\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.8354 (2.8354)  acc1: 78.9062 (78.9062)  acc5: 95.3125 (95.3125)  time: 0.1006  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.8301 (2.8341)  acc1: 79.6875 (79.4013)  acc5: 94.5312 (94.6473)  time: 0.0996  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 79.46433251258605, Acc@5 = 94.65380785156455, loss = 2.8337835311889648\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.8082 (2.8082)  acc1: 80.4688 (80.4688)  acc5: 96.0938 (96.0938)  time: 0.1006  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.8335 (2.8320)  acc1: 78.1250 (79.4400)  acc5: 95.3125 (94.7942)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 79.46433251218617, Acc@5 = 94.65380785156455, loss = 2.8334448289871217\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.8599 (2.8599)  acc1: 77.3438 (77.3438)  acc5: 92.1875 (92.1875)  time: 0.1005  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.8304 (2.8332)  acc1: 81.2500 (79.7494)  acc5: 95.3125 (94.8175)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 79.46433251218617, Acc@5 = 94.65380785156455, loss = 2.8340226141611735\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.8495 (2.8495)  acc1: 78.1250 (78.1250)  acc5: 93.7500 (93.7500)  time: 0.1006  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.8347 (2.8323)  acc1: 78.1250 (79.5637)  acc5: 94.5312 (94.7169)  time: 0.0997  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 79.46433251218617, Acc@5 = 94.65380785156455, loss = 2.833983612060547\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.8174 (2.8174)  acc1: 82.0312 (82.0312)  acc5: 98.4375 (98.4375)  time: 0.1006  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.8266 (2.8345)  acc1: 80.4688 (79.4090)  acc5: 94.5312 (94.5777)  time: 0.0996  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 79.46433251258605, Acc@5 = 94.65380785156455, loss = 2.834044788678487\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.8104 (2.8104)  acc1: 79.6875 (79.6875)  acc5: 96.0938 (96.0938)  time: 0.1008  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.8366 (2.8354)  acc1: 79.6875 (79.3394)  acc5: 94.5312 (94.5467)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 79.46433251258605, Acc@5 = 94.65380785156455, loss = 2.834275261561076\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.8751 (2.8751)  acc1: 75.0000 (75.0000)  acc5: 91.4062 (91.4062)  time: 0.1006  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.8188 (2.8343)  acc1: 78.9062 (79.2698)  acc5: 95.3125 (94.6241)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 79.46433251218617, Acc@5 = 94.65380785156455, loss = 2.8345990562438965\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.8189 (2.8189)  acc1: 85.9375 (85.9375)  acc5: 94.5312 (94.5312)  time: 0.1007  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.8253 (2.8334)  acc1: 79.6875 (79.4787)  acc5: 95.3125 (94.8175)  time: 0.0995  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 79.46433251198623, Acc@5 = 94.65380785196443, loss = 2.8351310745875042\n",
      "Test:  [  0/150]  eta: 0:00:17  loss: 2.7841 (2.7841)  acc1: 86.7188 (86.7188)  acc5: 96.0938 (96.0938)  time: 0.1136  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7826 (2.7862)  acc1: 85.1562 (86.0381)  acc5: 97.6562 (96.9214)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 86.12610723872474, Acc@5 = 96.89187064311547, loss = 2.785791277885437\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7705 (2.7705)  acc1: 87.5000 (87.5000)  acc5: 97.6562 (97.6562)  time: 0.1006  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7800 (2.7859)  acc1: 85.9375 (86.1696)  acc5: 96.8750 (96.8054)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 86.12610723872474, Acc@5 = 96.89187064351535, loss = 2.7866150919596353\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.8240 (2.8240)  acc1: 86.7188 (86.7188)  acc5: 93.7500 (93.7500)  time: 0.1006  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7754 (2.7845)  acc1: 87.5000 (86.4325)  acc5: 97.6562 (97.0452)  time: 0.0996  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 86.12610723872474, Acc@5 = 96.89187064351535, loss = 2.7861390495300293\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7747 (2.7747)  acc1: 89.8438 (89.8438)  acc5: 96.8750 (96.8750)  time: 0.1006  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7890 (2.7846)  acc1: 85.9375 (86.5176)  acc5: 96.0938 (96.9678)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 86.12610723832486, Acc@5 = 96.89187064311547, loss = 2.7851244036356606\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.8394 (2.8394)  acc1: 81.2500 (81.2500)  acc5: 92.9688 (92.9688)  time: 0.1014  data: 0.0025  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7804 (2.7853)  acc1: 86.7188 (86.1928)  acc5: 96.8750 (96.8982)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 86.12610723872474, Acc@5 = 96.89187064311547, loss = 2.785701038042704\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7788 (2.7788)  acc1: 85.9375 (85.9375)  acc5: 97.6562 (97.6562)  time: 0.1005  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7743 (2.7856)  acc1: 86.7188 (86.3320)  acc5: 97.6562 (97.0220)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 86.12610723872474, Acc@5 = 96.89187064351535, loss = 2.7861087767283124\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7778 (2.7778)  acc1: 84.3750 (84.3750)  acc5: 97.6562 (97.6562)  time: 0.1008  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7814 (2.7869)  acc1: 85.9375 (85.8447)  acc5: 97.6562 (96.7590)  time: 0.0996  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 86.12610723832486, Acc@5 = 96.89187064311547, loss = 2.7852793741226196\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7827 (2.7827)  acc1: 85.1562 (85.1562)  acc5: 96.0938 (96.0938)  time: 0.1007  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7873 (2.7867)  acc1: 85.9375 (85.6358)  acc5: 96.8750 (96.8518)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 86.12610723832486, Acc@5 = 96.89187064311547, loss = 2.7852122290929158\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7543 (2.7543)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 0.1006  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7893 (2.7857)  acc1: 86.7188 (86.2005)  acc5: 97.6562 (96.8827)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 86.12610723872474, Acc@5 = 96.89187064351535, loss = 2.78632213751475\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7825 (2.7825)  acc1: 89.0625 (89.0625)  acc5: 95.3125 (95.3125)  time: 0.1007  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7774 (2.7857)  acc1: 86.7188 (86.1541)  acc5: 97.6562 (96.8209)  time: 0.0997  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 86.12610723872474, Acc@5 = 96.89187064311547, loss = 2.7858806975682575\n",
      "Test:  [  0/150]  eta: 0:00:16  loss: 2.7571 (2.7571)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 0.1106  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7601 (2.7579)  acc1: 88.2812 (88.4514)  acc5: 97.6562 (97.8110)  time: 0.0997  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 88.68913465066304, Acc@5 = 97.7724199381519, loss = 2.757323457400004\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7434 (2.7434)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.1006  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7518 (2.7578)  acc1: 88.2812 (88.6448)  acc5: 96.8750 (97.7027)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 88.68913465106291, Acc@5 = 97.7724199381519, loss = 2.757401943206787\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7532 (2.7532)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.1007  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7667 (2.7580)  acc1: 88.2812 (88.5056)  acc5: 97.6562 (97.8032)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 88.68913465066304, Acc@5 = 97.7724199381519, loss = 2.7572369654973348\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7410 (2.7410)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.1006  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7554 (2.7566)  acc1: 90.6250 (88.7454)  acc5: 97.6562 (97.8342)  time: 0.0997  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 88.68913465066304, Acc@5 = 97.7724199381519, loss = 2.7579805056254068\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7575 (2.7575)  acc1: 85.9375 (85.9375)  acc5: 97.6562 (97.6562)  time: 0.1005  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7550 (2.7570)  acc1: 89.0625 (88.9619)  acc5: 97.6562 (97.8883)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 88.68913465066304, Acc@5 = 97.7724199381519, loss = 2.7574751059214275\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7531 (2.7531)  acc1: 92.1875 (92.1875)  acc5: 99.2188 (99.2188)  time: 0.1006  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7514 (2.7567)  acc1: 88.2812 (88.6139)  acc5: 97.6562 (97.8342)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 88.68913465066304, Acc@5 = 97.7724199381519, loss = 2.757697885831197\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7530 (2.7530)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.1007  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7537 (2.7583)  acc1: 89.0625 (88.7144)  acc5: 97.6562 (97.7723)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 88.68913465066304, Acc@5 = 97.7724199381519, loss = 2.7576894664764406\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7592 (2.7592)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.1008  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7531 (2.7575)  acc1: 89.0625 (88.9851)  acc5: 97.6562 (97.7336)  time: 0.0995  data: 0.0009  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 88.68913465066304, Acc@5 = 97.7724199381519, loss = 2.758100768725077\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7650 (2.7650)  acc1: 85.1562 (85.1562)  acc5: 96.8750 (96.8750)  time: 0.1006  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7591 (2.7568)  acc1: 88.2812 (88.7222)  acc5: 96.8750 (97.7181)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 88.68913465106291, Acc@5 = 97.7724199381519, loss = 2.7576254908243816\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7553 (2.7553)  acc1: 88.2812 (88.2812)  acc5: 97.6562 (97.6562)  time: 0.1006  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7544 (2.7576)  acc1: 89.8438 (88.6757)  acc5: 97.6562 (97.7104)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 88.68913465146281, Acc@5 = 97.77241993855178, loss = 2.758248219490051\n",
      "Test:  [  0/150]  eta: 0:00:16  loss: 2.7335 (2.7335)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.1116  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7426 (2.7393)  acc1: 89.0625 (89.4802)  acc5: 97.6562 (98.1822)  time: 0.0997  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.4805807440253, Acc@5 = 98.14455684260182, loss = 2.739119841257731\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7221 (2.7221)  acc1: 92.9688 (92.9688)  acc5: 99.2188 (99.2188)  time: 0.1006  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7317 (2.7397)  acc1: 90.6250 (89.0934)  acc5: 98.4375 (98.0585)  time: 0.0998  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.48058074362542, Acc@5 = 98.14455684260182, loss = 2.738379766146342\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7187 (2.7187)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.1007  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7376 (2.7365)  acc1: 90.6250 (89.9366)  acc5: 98.4375 (98.2364)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.48058074362542, Acc@5 = 98.14455684260182, loss = 2.7391610240936277\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7394 (2.7394)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 0.1006  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7306 (2.7380)  acc1: 90.6250 (89.4183)  acc5: 98.4375 (98.0894)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.48058074322553, Acc@5 = 98.14455684260182, loss = 2.7381812572479247\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7389 (2.7389)  acc1: 91.4062 (91.4062)  acc5: 97.6562 (97.6562)  time: 0.1007  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7364 (2.7387)  acc1: 89.0625 (89.5189)  acc5: 98.4375 (98.2596)  time: 0.0997  data: 0.0009  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.48058074322553, Acc@5 = 98.14455684260182, loss = 2.738901572227478\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7336 (2.7336)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.1009  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7359 (2.7378)  acc1: 89.0625 (89.6117)  acc5: 97.6562 (98.2209)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.48058074362542, Acc@5 = 98.14455684260182, loss = 2.7385984547932942\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7376 (2.7376)  acc1: 86.7188 (86.7188)  acc5: 98.4375 (98.4375)  time: 0.1006  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7349 (2.7373)  acc1: 89.8438 (89.6736)  acc5: 98.4375 (98.0894)  time: 0.0997  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.48058074322553, Acc@5 = 98.14455684260182, loss = 2.7384371264775593\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7257 (2.7257)  acc1: 92.1875 (92.1875)  acc5: 97.6562 (97.6562)  time: 0.1007  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7352 (2.7391)  acc1: 89.0625 (89.3951)  acc5: 98.4375 (98.1126)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.48058074322553, Acc@5 = 98.14455684260182, loss = 2.7386839723587038\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7452 (2.7452)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 0.1021  data: 0.0030  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7370 (2.7390)  acc1: 88.2812 (89.4879)  acc5: 97.6562 (98.0817)  time: 0.1004  data: 0.0017  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.4805807440253, Acc@5 = 98.1445568430017, loss = 2.7394250949223835\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7534 (2.7534)  acc1: 85.1562 (85.1562)  acc5: 97.6562 (97.6562)  time: 0.1011  data: 0.0024  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7378 (2.7375)  acc1: 89.0625 (89.4570)  acc5: 98.4375 (98.1977)  time: 0.1003  data: 0.0016  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.48058074322553, Acc@5 = 98.14455684260182, loss = 2.7387241411209104\n",
      "Test:  [  0/150]  eta: 0:00:17  loss: 2.7186 (2.7186)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.1157  data: 0.0028  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7239 (2.7247)  acc1: 89.0625 (90.0139)  acc5: 98.4375 (98.3524)  time: 0.1003  data: 0.0015  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.99423449946322, Acc@5 = 98.34372870735517, loss = 2.7265544033050535\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7359 (2.7359)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.1013  data: 0.0024  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7239 (2.7247)  acc1: 89.8438 (90.1222)  acc5: 97.6562 (98.2905)  time: 0.1003  data: 0.0015  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.99423449866345, Acc@5 = 98.34372870695529, loss = 2.725230091412862\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7608 (2.7608)  acc1: 83.5938 (83.5938)  acc5: 94.5312 (94.5312)  time: 0.1010  data: 0.0022  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7248 (2.7245)  acc1: 89.0625 (90.1222)  acc5: 98.4375 (98.3137)  time: 0.1001  data: 0.0014  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.99423449866345, Acc@5 = 98.34372870695529, loss = 2.7252207740147907\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7420 (2.7420)  acc1: 82.8125 (82.8125)  acc5: 96.8750 (96.8750)  time: 0.1010  data: 0.0021  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7224 (2.7252)  acc1: 89.8438 (90.0294)  acc5: 98.4375 (98.3524)  time: 0.1002  data: 0.0014  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.99423449906334, Acc@5 = 98.34372870695529, loss = 2.725394461949666\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7395 (2.7395)  acc1: 89.0625 (89.0625)  acc5: 96.0938 (96.0938)  time: 0.1011  data: 0.0021  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7177 (2.7249)  acc1: 90.6250 (90.0990)  acc5: 98.4375 (98.3369)  time: 0.1002  data: 0.0014  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.99423449866345, Acc@5 = 98.34372870695529, loss = 2.724969229698181\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7365 (2.7365)  acc1: 90.6250 (90.6250)  acc5: 97.6562 (97.6562)  time: 0.1009  data: 0.0021  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7268 (2.7255)  acc1: 89.0625 (89.8205)  acc5: 98.4375 (98.2983)  time: 0.1001  data: 0.0013  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.99423449906334, Acc@5 = 98.34372870695529, loss = 2.725836855570475\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7262 (2.7262)  acc1: 91.4062 (91.4062)  acc5: 98.4375 (98.4375)  time: 0.1009  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7143 (2.7252)  acc1: 90.6250 (90.2305)  acc5: 98.4375 (98.2441)  time: 0.1000  data: 0.0012  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.99423449946322, Acc@5 = 98.34372870695529, loss = 2.7256316979726156\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7365 (2.7365)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 0.1010  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7286 (2.7259)  acc1: 89.0625 (89.7741)  acc5: 98.4375 (98.3679)  time: 0.1001  data: 0.0013  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.99423449946322, Acc@5 = 98.34372870695529, loss = 2.7255800040562947\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7135 (2.7135)  acc1: 92.9688 (92.9688)  acc5: 98.4375 (98.4375)  time: 0.1010  data: 0.0022  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7251 (2.7261)  acc1: 89.0625 (89.9598)  acc5: 97.6562 (98.1900)  time: 0.0999  data: 0.0012  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.99423449906334, Acc@5 = 98.34372870695529, loss = 2.7253549846013385\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7478 (2.7478)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.1009  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7298 (2.7259)  acc1: 90.6250 (89.9752)  acc5: 97.6562 (98.2673)  time: 0.1000  data: 0.0012  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 89.99423449946322, Acc@5 = 98.34372870695529, loss = 2.725587959289551\n",
      "Test:  [  0/150]  eta: 0:00:16  loss: 2.7290 (2.7290)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.1127  data: 0.0022  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7065 (2.7150)  acc1: 91.4062 (90.4626)  acc5: 99.2188 (98.5535)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.32968184955341, Acc@5 = 98.46952146338907, loss = 2.7158056990305584\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7507 (2.7507)  acc1: 89.0625 (89.0625)  acc5: 97.6562 (97.6562)  time: 0.1007  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7114 (2.7153)  acc1: 90.6250 (90.3465)  acc5: 99.2188 (98.5458)  time: 0.0999  data: 0.0012  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.32968184995329, Acc@5 = 98.46952146338907, loss = 2.7160982513427734\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.6989 (2.6989)  acc1: 92.1875 (92.1875)  acc5: 99.2188 (99.2188)  time: 0.1010  data: 0.0021  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7151 (2.7169)  acc1: 90.6250 (90.3233)  acc5: 98.4375 (98.4762)  time: 0.0999  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.32968184995329, Acc@5 = 98.46952146378895, loss = 2.7166066789627075\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7466 (2.7466)  acc1: 83.5938 (83.5938)  acc5: 95.3125 (95.3125)  time: 0.1007  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7130 (2.7169)  acc1: 90.6250 (90.2073)  acc5: 98.4375 (98.3292)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.32968184915352, Acc@5 = 98.46952146338907, loss = 2.7155917008717854\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7285 (2.7285)  acc1: 92.9688 (92.9688)  acc5: 97.6562 (97.6562)  time: 0.1007  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7125 (2.7139)  acc1: 90.6250 (90.4162)  acc5: 98.4375 (98.6231)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.32968184915352, Acc@5 = 98.46952146338907, loss = 2.7157497628529867\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7002 (2.7002)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.1010  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7069 (2.7156)  acc1: 90.6250 (90.1996)  acc5: 98.4375 (98.5303)  time: 0.0998  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.32968184955341, Acc@5 = 98.46952146338907, loss = 2.715739197731018\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7081 (2.7081)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.1035  data: 0.0044  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:05  loss: 2.7127 (2.7148)  acc1: 90.6250 (90.2769)  acc5: 98.4375 (98.4839)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.32968184955341, Acc@5 = 98.46952146378895, loss = 2.716336104075114\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7094 (2.7094)  acc1: 92.9688 (92.9688)  acc5: 99.2188 (99.2188)  time: 0.1007  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7175 (2.7153)  acc1: 89.8438 (90.4626)  acc5: 98.4375 (98.5303)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.32968184955341, Acc@5 = 98.46952146378895, loss = 2.7163055038452146\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7154 (2.7154)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.1008  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7148 (2.7153)  acc1: 89.8438 (90.2847)  acc5: 98.4375 (98.5149)  time: 0.0998  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.32968184915352, Acc@5 = 98.46952146338907, loss = 2.7158396752675373\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7052 (2.7052)  acc1: 92.9688 (92.9688)  acc5: 99.2188 (99.2188)  time: 0.1007  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7145 (2.7154)  acc1: 90.6250 (90.4162)  acc5: 98.4375 (98.5149)  time: 0.0999  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.32968184915352, Acc@5 = 98.46952146338907, loss = 2.715308411916097\n",
      "Test:  [  0/150]  eta: 0:00:16  loss: 2.7032 (2.7032)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.1132  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7078 (2.7095)  acc1: 89.8438 (90.1841)  acc5: 98.4375 (98.4839)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.3139577545993, Acc@5 = 98.50621101734892, loss = 2.709201176961263\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7120 (2.7120)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.1009  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7057 (2.7083)  acc1: 90.6250 (90.3697)  acc5: 99.2188 (98.4839)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.3139577545993, Acc@5 = 98.50621101734892, loss = 2.709306559562683\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7049 (2.7049)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.1009  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7063 (2.7097)  acc1: 89.8438 (90.3465)  acc5: 98.4375 (98.4220)  time: 0.0998  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.31395775499918, Acc@5 = 98.50621101734892, loss = 2.709112826983134\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7045 (2.7045)  acc1: 85.1562 (85.1562)  acc5: 98.4375 (98.4375)  time: 0.1007  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7122 (2.7090)  acc1: 89.0625 (90.3388)  acc5: 98.4375 (98.5303)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.3139577545993, Acc@5 = 98.50621101734892, loss = 2.7090196084976195\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7461 (2.7461)  acc1: 85.1562 (85.1562)  acc5: 96.8750 (96.8750)  time: 0.1008  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7074 (2.7101)  acc1: 90.6250 (90.2305)  acc5: 98.4375 (98.4530)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.31395775539907, Acc@5 = 98.5062110177488, loss = 2.7101162830988565\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7050 (2.7050)  acc1: 93.7500 (93.7500)  acc5: 98.4375 (98.4375)  time: 0.1008  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7148 (2.7105)  acc1: 89.8438 (90.3079)  acc5: 98.4375 (98.3911)  time: 0.0998  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.31395775539907, Acc@5 = 98.50621101734892, loss = 2.7104432853062947\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7063 (2.7063)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.1007  data: 0.0018  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7034 (2.7077)  acc1: 90.6250 (90.5631)  acc5: 99.2188 (98.6077)  time: 0.0997  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.3139577545993, Acc@5 = 98.50621101734892, loss = 2.709778938293457\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7121 (2.7121)  acc1: 90.6250 (90.6250)  acc5: 96.0938 (96.0938)  time: 0.1010  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7073 (2.7095)  acc1: 90.6250 (90.5090)  acc5: 98.4375 (98.5303)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.31395775439935, Acc@5 = 98.50621101734892, loss = 2.710322518348694\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.6959 (2.6959)  acc1: 91.4062 (91.4062)  acc5: 100.0000 (100.0000)  time: 0.1008  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7040 (2.7100)  acc1: 91.4062 (90.0526)  acc5: 98.4375 (98.4530)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.3139577545993, Acc@5 = 98.50621101734892, loss = 2.709211645126343\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7329 (2.7329)  acc1: 88.2812 (88.2812)  acc5: 95.3125 (95.3125)  time: 0.1007  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7093 (2.7096)  acc1: 89.8438 (90.3233)  acc5: 98.4375 (98.5226)  time: 0.0997  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.3139577545993, Acc@5 = 98.50621101734892, loss = 2.7089156977335613\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7053 (2.7053)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.1009  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7062 (2.7049)  acc1: 91.4062 (90.3775)  acc5: 98.4375 (98.5458)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.37161276836466, Acc@5 = 98.53241784160595, loss = 2.705260009765625\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7181 (2.7181)  acc1: 89.8438 (89.8438)  acc5: 97.6562 (97.6562)  time: 0.1026  data: 0.0034  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7014 (2.7042)  acc1: 91.4062 (90.5012)  acc5: 98.4375 (98.6618)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.37161276796478, Acc@5 = 98.53241784160595, loss = 2.7047218052546182\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.6853 (2.6853)  acc1: 94.5312 (94.5312)  acc5: 100.0000 (100.0000)  time: 0.1015  data: 0.0026  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7045 (2.7055)  acc1: 89.0625 (90.0681)  acc5: 98.4375 (98.4762)  time: 0.0997  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.37161276876455, Acc@5 = 98.53241784240572, loss = 2.7059260416030884\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7131 (2.7131)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.1011  data: 0.0021  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.6985 (2.7052)  acc1: 90.6250 (90.5941)  acc5: 98.4375 (98.5381)  time: 0.0998  data: 0.0009  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.37161276796478, Acc@5 = 98.53241784160595, loss = 2.70506635983785\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7035 (2.7035)  acc1: 91.4062 (91.4062)  acc5: 98.4375 (98.4375)  time: 0.1010  data: 0.0020  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7081 (2.7043)  acc1: 89.8438 (90.5090)  acc5: 97.6562 (98.4607)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.37161276876455, Acc@5 = 98.53241784160595, loss = 2.7054354508717853\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.6943 (2.6943)  acc1: 88.2812 (88.2812)  acc5: 100.0000 (100.0000)  time: 0.1010  data: 0.0022  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.6995 (2.7042)  acc1: 90.6250 (90.3775)  acc5: 98.4375 (98.7005)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.37161276796478, Acc@5 = 98.53241784160595, loss = 2.704679708480835\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7167 (2.7167)  acc1: 86.7188 (86.7188)  acc5: 96.8750 (96.8750)  time: 0.1024  data: 0.0033  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7009 (2.7048)  acc1: 91.4062 (90.5476)  acc5: 99.2188 (98.4375)  time: 0.0999  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.37161276836466, Acc@5 = 98.53241784160595, loss = 2.7049842357635496\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7341 (2.7341)  acc1: 90.6250 (90.6250)  acc5: 96.0938 (96.0938)  time: 0.1011  data: 0.0021  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7019 (2.7045)  acc1: 89.0625 (90.4548)  acc5: 99.2188 (98.5071)  time: 0.0998  data: 0.0010  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.37161276876455, Acc@5 = 98.53241784160595, loss = 2.7049290784200033\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7196 (2.7196)  acc1: 86.7188 (86.7188)  acc5: 96.0938 (96.0938)  time: 0.1007  data: 0.0019  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7065 (2.7052)  acc1: 89.0625 (90.1300)  acc5: 97.6562 (98.5149)  time: 0.0998  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.37161276796478, Acc@5 = 98.53241784160595, loss = 2.7046785799662274\n",
      "Test:  [  0/150]  eta: 0:00:15  loss: 2.7005 (2.7005)  acc1: 92.9688 (92.9688)  acc5: 98.4375 (98.4375)  time: 0.1019  data: 0.0028  max mem: 21741\n",
      "Test:  [100/150]  eta: 0:00:04  loss: 2.7039 (2.7054)  acc1: 90.6250 (90.2537)  acc5: 99.2188 (98.5303)  time: 0.0999  data: 0.0011  max mem: 21741\n",
      "Test: Total time: 0:00:14\n",
      " * Acc@1 = 90.37161276796478, Acc@5 = 98.53241784160595, loss = 2.7046916246414185\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "for time in range(1,11):\n",
    "  acc = 0\n",
    "  for i in range(10):\n",
    "    net = NetworkB().to(device)\n",
    "    functional.set_step_mode(net,step_mode='m')\n",
    "    functional.set_backend(net, backend='cupy')\n",
    "    weights = torch.load(model_dir)\n",
    "    net.load_state_dict(weights)\n",
    "    net.set_T(time)\n",
    "    test_loss, test_acc1, test_acc5 = evaluate(net, criterion, data_loader_test, device=device, header='Test:')\n",
    "    acc += test_acc1\n",
    "  acc_list.append(acc/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_snn.txt', 'a') as file:\n",
    "    line = ' '.join(map(str, acc_list))\n",
    "    file.write(line + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_com",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
