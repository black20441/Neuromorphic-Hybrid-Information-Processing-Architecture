{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import utils\n",
    "import math\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.distributed.optim\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from spikingjelly.activation_based import layer,functional,neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "np.int = int\n",
    "\n",
    "train_tb_writer = None\n",
    "te_tb_writer = None\n",
    "device = torch.device('cuda:2')\n",
    "deviceIds = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Classification Training')\n",
    "\n",
    "    parser.add_argument('--device', default='cuda:0', help='device')\n",
    "    parser.add_argument('-b', '--batch-size', default=32, type=int)\n",
    "    parser.add_argument('--data-path', default='./data/', help='dataset')\n",
    "    parser.add_argument('--epochs', default=320, type=int, metavar='N',\n",
    "                        help='number of total epochs to pre-train')\n",
    "    parser.add_argument('-j', '--workers', default=16, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 16)')\n",
    "    parser.add_argument('--lr', default=0.0025, type=float, help='initial learning rate')\n",
    "\n",
    "    parser.add_argument('--print-freq', default=10, type=int, help='print frequency')\n",
    "    parser.add_argument('--output-dir', default='./logs', help='path where to save')\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--tb', action='store_true',\n",
    "                        help='Use TensorBoard to record logs')\n",
    "    parser.add_argument(\n",
    "        \"--cache-dataset\",\n",
    "        dest=\"cache_dataset\",\n",
    "        help=\"Cache the datasets for quicker initialization. It also serializes the transforms\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(args=['--data-path','../data','--lr','0.01','-b','128','--epochs','100','--print-freq','100','--tb','--cache-dataset'])\n",
    "    return args\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkA(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkA, self).__init__()\n",
    "        self.T = 10\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=3, padding=1) \n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "        self.sn1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 96, kernel_size=3, padding=1) \n",
    "        self.bn2 = nn.BatchNorm2d(96)\n",
    "        self.sn2 = nn.ReLU()\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.conv3 = nn.Conv2d(96, 128, kernel_size=3, padding=1) \n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.sn3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.sn4 = nn.ReLU()\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1) \n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.sn5 = nn.ReLU()\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(512)\n",
    "        self.sn8 = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.conv10 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(512)\n",
    "        self.sn10 = nn.ReLU()\n",
    "\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        \n",
    "        self.conv11 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(256)\n",
    "        self.sn11 = neuron.IFNode(detach_reset=True)\n",
    "\n",
    "        self.conv13 = layer.SeqToANNContainer(nn.Conv2d(256, 128, kernel_size=3, padding=1))\n",
    "        self.bn13 = layer.SeqToANNContainer(nn.BatchNorm2d(128))\n",
    "        self.sn13 = neuron.IFNode(detach_reset=True)\n",
    "\n",
    "        self.pool5 = layer.SeqToANNContainer(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.linear1 = layer.SeqToANNContainer(nn.Linear(128, 256))\n",
    "        self.bn14 = layer.SeqToANNContainer(nn.BatchNorm1d(256))\n",
    "        self.sn14 = neuron.IFNode(detach_reset=True)\n",
    "\n",
    "        self.linear3 = layer.SeqToANNContainer(nn.Linear(256, 10))\n",
    "        self.bn15 = layer.SeqToANNContainer(nn.BatchNorm1d(10))\n",
    "        self.sn15 = neuron.IFNode(detach_reset=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        T = self.T\n",
    "        \n",
    "        x = self.sn1(self.bn1(self.conv1(x)))\n",
    "        x = self.sn2(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.sn3(self.bn3(self.conv3(x)))\n",
    "        x = self.sn4(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.sn5(self.bn5(self.conv5(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.sn8(self.bn8(self.conv8(x)))\n",
    "        x = self.sn10(self.bn10(self.conv10(x)))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.bn11(self.conv11(x))\n",
    "\n",
    "        x = x.unsqueeze(0)\n",
    "        x = x.repeat(T, 1, 1, 1, 1)\n",
    "        x = self.sn11(x)\n",
    "        x = self.sn13(self.bn13(self.conv13(x)))\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        x = torch.flatten(x,2)\n",
    "\n",
    "        x = self.sn14(self.bn14(self.linear1(x)))\n",
    "\n",
    "        x = self.sn15(self.bn15(self.linear3(x)))\n",
    "\n",
    "        return x.mean(0)\n",
    "    \n",
    "    def set_T(self, T):\n",
    "        self.T = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Creating model\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "max_test_acc1 = 0.\n",
    "test_acc5_at_max_test_acc1 = 0.\n",
    "\n",
    "utils.init_distributed_mode(args)\n",
    "print(args)\n",
    "output_dir = os.path.join(args.output_dir, f'b_{args.batch_size}_lr{args.lr}')\n",
    "\n",
    "time_now = datetime.datetime.now()\n",
    "output_dir += f'_{time_now.year}_{time_now.month}_{time_now.day}_{time_now.hour}__{time_now.minute}'\n",
    "\n",
    "if output_dir:\n",
    "    utils.mkdir(output_dir)\n",
    "\n",
    "# Load data\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=torchvision.datasets.CIFAR10(root= args.data_path,\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.557, 0.549, 0.5534])\n",
    "    ]),\n",
    "    download=True),\n",
    "    batch_size=args.batch_size, shuffle=True, pin_memory=True, drop_last=True, num_workers=args.workers)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset=torchvision.datasets.CIFAR10(root=args.data_path,\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.557, 0.549, 0.5534])\n",
    "    ]),\n",
    "    download=True),\n",
    "    batch_size=args.batch_size, shuffle=False, pin_memory=True, drop_last=False, num_workers=args.workers)\n",
    "\n",
    "\n",
    "print(\"Creating model\")\n",
    "\n",
    "net = NetworkA().to(device)\n",
    "functional.set_step_mode(net,step_mode='m')\n",
    "functional.set_backend(net, backend='cupy')\n",
    "net.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.4, last_epoch=-1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if args.resume:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    args.start_epoch = checkpoint['epoch'] + 1\n",
    "    max_test_acc1 = checkpoint['max_test_acc1']\n",
    "    test_acc5_at_max_test_acc1 = checkpoint['test_acc5_at_max_test_acc1']\n",
    "\n",
    "if args.tb and utils.is_main_process():\n",
    "    purge_step_train = args.start_epoch\n",
    "    purge_step_te = args.start_epoch\n",
    "    train_tb_writer = SummaryWriter(output_dir + '_logs/train', purge_step=purge_step_train)\n",
    "    te_tb_writer = SummaryWriter(output_dir + '_logs/te', purge_step=purge_step_te)\n",
    "    with open(output_dir + '_logs/args.txt', 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write(str(args))\n",
    "\n",
    "    with open(output_dir + '_logs/resluts.txt', 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write('Results\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(net, criterion, data_loader, device, epoch, print_freq, scaler=None,lr = 1e-2):\n",
    "    net.train()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value}'))\n",
    "    metric_logger.add_meter('img/s', utils.SmoothedValue(window_size=10, fmt='{value}'))\n",
    "\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    for image, target in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        start_time = time.time()\n",
    "        image, target = image.to(device), target.to(device)\n",
    "\n",
    "        output = net(image)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        functional.reset_net(net)\n",
    "\n",
    "        acc1, acc5 = utils.accuracy(output, target, topk=(1, 5))\n",
    "        batch_size = image.shape[0]\n",
    "        loss_s = loss.item()\n",
    "        if math.isnan(loss_s):\n",
    "            raise ValueError('loss is Nan')\n",
    "        acc1_s = acc1.item()\n",
    "        acc5_s = acc5.item()\n",
    "\n",
    "        metric_logger.update(loss=loss_s, lr=lr)\n",
    "\n",
    "        metric_logger.meters['acc1'].update(acc1_s, n=batch_size)\n",
    "        metric_logger.meters['acc5'].update(acc5_s, n=batch_size)\n",
    "        metric_logger.meters['img/s'].update(batch_size / (time.time() - start_time))\n",
    "        \n",
    "    scheduler.step()\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    return metric_logger.loss.global_avg, metric_logger.acc1.global_avg, metric_logger.acc5.global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, criterion, data_loader, device, print_freq=100, header='Test:'):\n",
    "    net.eval()\n",
    "    net.to(device)\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    with torch.no_grad():\n",
    "        for image, target in metric_logger.log_every(data_loader, print_freq, header):\n",
    "            image = image.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "            output = net(image)\n",
    "            loss = criterion(output, target)\n",
    "            functional.reset_net(net)\n",
    "\n",
    "            acc1, acc5 = utils.accuracy(output, target, topk=(1, 5))\n",
    "            batch_size = image.shape[0]\n",
    "            metric_logger.update(loss=loss.item())\n",
    "            metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "    metric_logger.synchronize_between_processes()\n",
    "\n",
    "    loss, acc1, acc5 = metric_logger.loss.global_avg, metric_logger.acc1.global_avg, metric_logger.acc5.global_avg\n",
    "    print(f' * Acc@1 = {acc1}, Acc@5 = {acc5}, loss = {loss}')\n",
    "    return loss, acc1, acc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch: [0]  [  0/390]  eta: 0:11:17  lr: 0.01  img/s: 97.7923153188878  loss: 2.3193 (2.3193)  acc1: 10.1562 (10.1562)  acc5: 44.5312 (44.5312)  time: 1.7366  data: 0.4277  max mem: 0\n",
      "Epoch: [0]  [100/390]  eta: 0:00:13  lr: 0.01  img/s: 6938.199149640083  loss: 1.9954 (2.0855)  acc1: 30.4688 (25.2553)  acc5: 84.3750 (80.1516)  time: 0.0280  data: 0.0004  max mem: 0\n",
      "Epoch: [0]  [200/390]  eta: 0:00:07  lr: 0.01  img/s: 5786.306888115279  loss: 1.9320 (2.0216)  acc1: 33.5938 (29.0267)  acc5: 88.2812 (83.3489)  time: 0.0306  data: 0.0004  max mem: 0\n",
      "Epoch: [0]  [300/390]  eta: 0:00:03  lr: 0.01  img/s: 4508.0350653276455  loss: 1.9018 (1.9859)  acc1: 32.8125 (31.1799)  acc5: 89.8438 (85.3276)  time: 0.0287  data: 0.0003  max mem: 0\n",
      "Epoch: [0] Total time: 0:00:13\n",
      "Test:  [ 0/79]  eta: 0:08:11  loss: 1.8606 (1.8606)  acc1: 42.9688 (42.9688)  acc5: 91.4062 (91.4062)  time: 6.2269  data: 0.5421  max mem: 0\n",
      "Test: Total time: 0:00:08\n",
      " * Acc@1 = 41.6, Acc@5 = 90.14, loss = 1.8718802038627336\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:00:22 max_test_acc1 41.6 test_acc5_at_max_test_acc1 90.14 train_acc1 33.26121794871795 train_acc5 33.26121794871795\n",
      "Epoch: [1]  [  0/390]  eta: 0:03:57  lr: 0.01  img/s: 2202.0052992084  loss: 1.8809 (1.8809)  acc1: 35.9375 (35.9375)  acc5: 91.4062 (91.4062)  time: 0.6079  data: 0.5498  max mem: 0\n",
      "Epoch: [1]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 3827.847419681435  loss: 1.8288 (1.8430)  acc1: 42.1875 (42.8991)  acc5: 90.6250 (91.1897)  time: 0.0324  data: 0.0005  max mem: 0\n",
      "Epoch: [1]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 4764.266614605057  loss: 1.8222 (1.8342)  acc1: 44.5312 (43.8744)  acc5: 91.4062 (91.6589)  time: 0.0288  data: 0.0003  max mem: 0\n",
      "Epoch: [1]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4587.189623794185  loss: 1.7890 (1.8221)  acc1: 47.6562 (44.8868)  acc5: 93.7500 (92.2654)  time: 0.0274  data: 0.0003  max mem: 0\n",
      "Epoch: [1] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.8038 (1.8038)  acc1: 52.3438 (52.3438)  acc5: 90.6250 (90.6250)  time: 0.5944  data: 0.5682  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 45.91, Acc@5 = 91.85, loss = 1.8256157319757003\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:00:35 max_test_acc1 45.91 test_acc5_at_max_test_acc1 91.85 train_acc1 46.2119391025641 train_acc5 46.2119391025641\n",
      "Epoch: [2]  [  0/390]  eta: 0:03:56  lr: 0.01  img/s: 2981.721661279394  loss: 1.7592 (1.7592)  acc1: 57.0312 (57.0312)  acc5: 94.5312 (94.5312)  time: 0.6076  data: 0.5646  max mem: 0\n",
      "Epoch: [2]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4814.985757847534  loss: 1.7633 (1.7696)  acc1: 54.6875 (53.4576)  acc5: 95.3125 (93.9124)  time: 0.0290  data: 0.0004  max mem: 0\n",
      "Epoch: [2]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 3560.3880363419325  loss: 1.7499 (1.7611)  acc1: 59.3750 (55.7875)  acc5: 94.5312 (94.2164)  time: 0.0307  data: 0.0003  max mem: 0\n",
      "Epoch: [2]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4782.729145137726  loss: 1.7148 (1.7506)  acc1: 64.8438 (57.6672)  acc5: 95.3125 (94.3522)  time: 0.0293  data: 0.0003  max mem: 0\n",
      "Epoch: [2] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.7068 (1.7068)  acc1: 64.8438 (64.8438)  acc5: 93.7500 (93.7500)  time: 0.6106  data: 0.5812  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 61.7, Acc@5 = 94.46, loss = 1.7324054558065873\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:00:48 max_test_acc1 61.7 test_acc5_at_max_test_acc1 94.46 train_acc1 58.97435897435897 train_acc5 58.97435897435897\n",
      "Epoch: [3]  [  0/390]  eta: 0:04:40  lr: 0.01  img/s: 2723.5325761073036  loss: 1.6917 (1.6917)  acc1: 67.9688 (67.9688)  acc5: 96.0938 (96.0938)  time: 0.7188  data: 0.6718  max mem: 0\n",
      "Epoch: [3]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4839.247095302909  loss: 1.7002 (1.7071)  acc1: 64.0625 (64.8360)  acc5: 94.5312 (95.1655)  time: 0.0286  data: 0.0003  max mem: 0\n",
      "Epoch: [3]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 3907.3290005167355  loss: 1.6850 (1.7052)  acc1: 64.8438 (65.0770)  acc5: 96.0938 (95.1493)  time: 0.0282  data: 0.0003  max mem: 0\n",
      "Epoch: [3]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4845.930172942918  loss: 1.6767 (1.6978)  acc1: 67.1875 (65.9417)  acc5: 96.0938 (95.4163)  time: 0.0272  data: 0.0003  max mem: 0\n",
      "Epoch: [3] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:37  loss: 1.6764 (1.6764)  acc1: 70.3125 (70.3125)  acc5: 95.3125 (95.3125)  time: 0.4761  data: 0.4492  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 69.42, Acc@5 = 95.97, loss = 1.6701554769202123\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:01:01 max_test_acc1 69.42 test_acc5_at_max_test_acc1 95.97 train_acc1 66.3261217948718 train_acc5 66.3261217948718\n",
      "Epoch: [4]  [  0/390]  eta: 0:04:13  lr: 0.01  img/s: 2368.5864565456204  loss: 1.6791 (1.6791)  acc1: 68.7500 (68.7500)  acc5: 96.0938 (96.0938)  time: 0.6502  data: 0.5961  max mem: 0\n",
      "Epoch: [4]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4051.275002075174  loss: 1.6711 (1.6722)  acc1: 67.1875 (69.3533)  acc5: 96.0938 (95.9236)  time: 0.0299  data: 0.0003  max mem: 0\n",
      "Epoch: [4]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4150.849791247874  loss: 1.6584 (1.6684)  acc1: 71.0938 (69.5274)  acc5: 96.8750 (96.0588)  time: 0.0299  data: 0.0003  max mem: 0\n",
      "Epoch: [4]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4091.286679926537  loss: 1.6598 (1.6658)  acc1: 69.5312 (69.6844)  acc5: 96.8750 (96.1742)  time: 0.0306  data: 0.0004  max mem: 0\n",
      "Epoch: [4] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.6671 (1.6671)  acc1: 60.9375 (60.9375)  acc5: 97.6562 (97.6562)  time: 0.6654  data: 0.6385  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 67.64, Acc@5 = 96.36, loss = 1.668116094190863\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:01:15 max_test_acc1 69.42 test_acc5_at_max_test_acc1 95.97 train_acc1 69.6454326923077 train_acc5 69.6454326923077\n",
      "Epoch: [5]  [  0/390]  eta: 0:04:22  lr: 0.01  img/s: 2247.1303983860303  loss: 1.6361 (1.6361)  acc1: 74.2188 (74.2188)  acc5: 96.0938 (96.0938)  time: 0.6733  data: 0.6163  max mem: 0\n",
      "Epoch: [5]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 3696.6688379202787  loss: 1.6243 (1.6330)  acc1: 75.0000 (73.3292)  acc5: 97.6562 (97.1844)  time: 0.0276  data: 0.0003  max mem: 0\n",
      "Epoch: [5]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4273.021059836678  loss: 1.6171 (1.6297)  acc1: 75.0000 (73.7834)  acc5: 96.8750 (97.1354)  time: 0.0291  data: 0.0003  max mem: 0\n",
      "Epoch: [5]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 5252.574693526137  loss: 1.6165 (1.6266)  acc1: 75.0000 (74.2681)  acc5: 97.6562 (97.1527)  time: 0.0291  data: 0.0004  max mem: 0\n",
      "Epoch: [5] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5963 (1.5963)  acc1: 73.4375 (73.4375)  acc5: 97.6562 (97.6562)  time: 0.6270  data: 0.5996  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 73.38, Acc@5 = 96.59, loss = 1.636530966698369\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:01:28 max_test_acc1 73.38 test_acc5_at_max_test_acc1 96.59 train_acc1 74.39903846153847 train_acc5 74.39903846153847\n",
      "Epoch: [6]  [  0/390]  eta: 0:04:18  lr: 0.01  img/s: 2374.809956252295  loss: 1.6217 (1.6217)  acc1: 75.7812 (75.7812)  acc5: 98.4375 (98.4375)  time: 0.6631  data: 0.6092  max mem: 0\n",
      "Epoch: [6]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4972.408187459479  loss: 1.6115 (1.6154)  acc1: 75.0000 (75.9127)  acc5: 96.8750 (97.2927)  time: 0.0289  data: 0.0004  max mem: 0\n",
      "Epoch: [6]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4323.502411918663  loss: 1.6061 (1.6144)  acc1: 76.5625 (75.8784)  acc5: 97.6562 (97.3609)  time: 0.0293  data: 0.0003  max mem: 0\n",
      "Epoch: [6]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4369.137778935204  loss: 1.5977 (1.6148)  acc1: 75.7812 (75.8955)  acc5: 96.8750 (97.3318)  time: 0.0287  data: 0.0004  max mem: 0\n",
      "Epoch: [6] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.6444 (1.6444)  acc1: 70.3125 (70.3125)  acc5: 96.8750 (96.8750)  time: 0.5904  data: 0.5636  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 74.42, Acc@5 = 96.58, loss = 1.6366566811935812\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:01:42 max_test_acc1 74.42 test_acc5_at_max_test_acc1 96.58 train_acc1 76.02764423076923 train_acc5 76.02764423076923\n",
      "Epoch: [7]  [  0/390]  eta: 0:04:43  lr: 0.01  img/s: 2466.217612200836  loss: 1.5647 (1.5647)  acc1: 78.9062 (78.9062)  acc5: 100.0000 (100.0000)  time: 0.7272  data: 0.6753  max mem: 0\n",
      "Epoch: [7]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4513.568442822793  loss: 1.6011 (1.6030)  acc1: 77.3438 (77.6532)  acc5: 97.6562 (97.7027)  time: 0.0259  data: 0.0003  max mem: 0\n",
      "Epoch: [7]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4104.172523717424  loss: 1.5991 (1.6056)  acc1: 77.3438 (77.6508)  acc5: 97.6562 (97.5746)  time: 0.0311  data: 0.0003  max mem: 0\n",
      "Epoch: [7]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 3993.713499319343  loss: 1.6060 (1.6045)  acc1: 78.9062 (77.8213)  acc5: 96.8750 (97.6121)  time: 0.0307  data: 0.0003  max mem: 0\n",
      "Epoch: [7] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5933 (1.5933)  acc1: 79.6875 (79.6875)  acc5: 98.4375 (98.4375)  time: 0.5891  data: 0.5618  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 76.71, Acc@5 = 97.18, loss = 1.616260770000989\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:01:55 max_test_acc1 76.71 test_acc5_at_max_test_acc1 97.18 train_acc1 77.73036858974359 train_acc5 77.73036858974359\n",
      "Epoch: [8]  [  0/390]  eta: 0:04:25  lr: 0.01  img/s: 2604.5258429146656  loss: 1.6428 (1.6428)  acc1: 77.3438 (77.3438)  acc5: 92.9688 (92.9688)  time: 0.6803  data: 0.6311  max mem: 0\n",
      "Epoch: [8]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4198.0757086444855  loss: 1.5921 (1.5938)  acc1: 80.4688 (79.5869)  acc5: 97.6562 (97.9734)  time: 0.0271  data: 0.0003  max mem: 0\n",
      "Epoch: [8]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 4645.457795775684  loss: 1.5943 (1.5971)  acc1: 79.6875 (79.0734)  acc5: 97.6562 (97.7495)  time: 0.0299  data: 0.0003  max mem: 0\n",
      "Epoch: [8]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 5655.380349938377  loss: 1.6036 (1.5969)  acc1: 78.9062 (79.1087)  acc5: 98.4375 (97.7886)  time: 0.0285  data: 0.0003  max mem: 0\n",
      "Epoch: [8] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:36  loss: 1.6034 (1.6034)  acc1: 78.1250 (78.1250)  acc5: 96.8750 (96.8750)  time: 0.4557  data: 0.4394  max mem: 0\n",
      "Test: Total time: 0:00:00\n",
      " * Acc@1 = 76.77, Acc@5 = 97.28, loss = 1.6148869146274616\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:02:08 max_test_acc1 76.77 test_acc5_at_max_test_acc1 97.28 train_acc1 79.01241987179488 train_acc5 79.01241987179488\n",
      "Epoch: [9]  [  0/390]  eta: 0:04:06  lr: 0.01  img/s: 2855.6811506321774  loss: 1.5877 (1.5877)  acc1: 82.0312 (82.0312)  acc5: 98.4375 (98.4375)  time: 0.6309  data: 0.5861  max mem: 0\n",
      "Epoch: [9]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 3811.5147634091795  loss: 1.5791 (1.5888)  acc1: 80.4688 (80.0588)  acc5: 98.4375 (97.9425)  time: 0.0302  data: 0.0004  max mem: 0\n",
      "Epoch: [9]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 4766.593081895021  loss: 1.5976 (1.5891)  acc1: 79.6875 (80.4765)  acc5: 97.6562 (97.8195)  time: 0.0295  data: 0.0003  max mem: 0\n",
      "Epoch: [9]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4603.236862186935  loss: 1.5903 (1.5895)  acc1: 80.4688 (80.6141)  acc5: 97.6562 (97.7782)  time: 0.0277  data: 0.0004  max mem: 0\n",
      "Epoch: [9] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:54  loss: 1.5738 (1.5738)  acc1: 82.0312 (82.0312)  acc5: 99.2188 (99.2188)  time: 0.6877  data: 0.6623  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 77.58, Acc@5 = 96.58, loss = 1.6214095897312406\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:02:22 max_test_acc1 77.58 test_acc5_at_max_test_acc1 96.58 train_acc1 80.50080128205128 train_acc5 80.50080128205128\n",
      "Epoch: [10]  [  0/390]  eta: 0:04:08  lr: 0.01  img/s: 2699.1594495809513  loss: 1.5747 (1.5747)  acc1: 83.5938 (83.5938)  acc5: 99.2188 (99.2188)  time: 0.6364  data: 0.5890  max mem: 0\n",
      "Epoch: [10]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4407.698595272695  loss: 1.5670 (1.5762)  acc1: 84.3750 (82.6191)  acc5: 98.4375 (98.1126)  time: 0.0324  data: 0.0004  max mem: 0\n",
      "Epoch: [10]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 3928.256678544513  loss: 1.5761 (1.5747)  acc1: 82.8125 (83.0613)  acc5: 97.6562 (98.1188)  time: 0.0297  data: 0.0004  max mem: 0\n",
      "Epoch: [10]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4222.475830934516  loss: 1.5729 (1.5747)  acc1: 83.5938 (83.0617)  acc5: 98.4375 (98.1494)  time: 0.0301  data: 0.0003  max mem: 0\n",
      "Epoch: [10] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5374 (1.5374)  acc1: 85.1562 (85.1562)  acc5: 99.2188 (99.2188)  time: 0.6271  data: 0.6007  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 81.94, Acc@5 = 97.64, loss = 1.5863352998902527\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:02:36 max_test_acc1 81.94 test_acc5_at_max_test_acc1 97.64 train_acc1 83.09294871794872 train_acc5 83.09294871794872\n",
      "Epoch: [11]  [  0/390]  eta: 0:04:22  lr: 0.01  img/s: 2234.9971982964976  loss: 1.5485 (1.5485)  acc1: 86.7188 (86.7188)  acc5: 99.2188 (99.2188)  time: 0.6741  data: 0.6168  max mem: 0\n",
      "Epoch: [11]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4188.7081477089205  loss: 1.5687 (1.5708)  acc1: 83.5938 (83.7330)  acc5: 97.6562 (98.1668)  time: 0.0275  data: 0.0004  max mem: 0\n",
      "Epoch: [11]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4207.3203974797025  loss: 1.5601 (1.5696)  acc1: 84.3750 (83.9708)  acc5: 98.4375 (98.1887)  time: 0.0301  data: 0.0004  max mem: 0\n",
      "Epoch: [11]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4218.22926913588  loss: 1.5708 (1.5686)  acc1: 82.8125 (83.9467)  acc5: 98.4375 (98.2506)  time: 0.0235  data: 0.0003  max mem: 0\n",
      "Epoch: [11] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5744 (1.5744)  acc1: 82.8125 (82.8125)  acc5: 99.2188 (99.2188)  time: 0.6172  data: 0.5985  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 82.97, Acc@5 = 97.71, loss = 1.5835941305643395\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:02:49 max_test_acc1 82.97 test_acc5_at_max_test_acc1 97.71 train_acc1 84.08453525641026 train_acc5 84.08453525641026\n",
      "Epoch: [12]  [  0/390]  eta: 0:04:32  lr: 0.01  img/s: 2414.6827864133566  loss: 1.5374 (1.5374)  acc1: 85.9375 (85.9375)  acc5: 99.2188 (99.2188)  time: 0.6983  data: 0.6453  max mem: 0\n",
      "Epoch: [12]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4314.052664989916  loss: 1.5676 (1.5654)  acc1: 84.3750 (84.3905)  acc5: 98.4375 (98.5535)  time: 0.0302  data: 0.0004  max mem: 0\n",
      "Epoch: [12]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 4561.660197804439  loss: 1.5638 (1.5639)  acc1: 84.3750 (84.5110)  acc5: 98.4375 (98.5269)  time: 0.0287  data: 0.0003  max mem: 0\n",
      "Epoch: [12]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4718.4583717843925  loss: 1.5597 (1.5650)  acc1: 85.1562 (84.5022)  acc5: 97.6562 (98.4349)  time: 0.0282  data: 0.0003  max mem: 0\n",
      "Epoch: [12] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5638 (1.5638)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6696  data: 0.6433  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 82.77, Acc@5 = 97.71, loss = 1.584044509296176\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:03:01 max_test_acc1 82.97 test_acc5_at_max_test_acc1 97.71 train_acc1 84.57932692307692 train_acc5 84.57932692307692\n",
      "Epoch: [13]  [  0/390]  eta: 0:03:59  lr: 0.01  img/s: 2688.966136924826  loss: 1.5921 (1.5921)  acc1: 85.9375 (85.9375)  acc5: 97.6562 (97.6562)  time: 0.6143  data: 0.5666  max mem: 0\n",
      "Epoch: [13]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 5154.835015218581  loss: 1.5561 (1.5579)  acc1: 85.9375 (85.5121)  acc5: 98.4375 (98.4066)  time: 0.0293  data: 0.0003  max mem: 0\n",
      "Epoch: [13]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4216.970214905115  loss: 1.5577 (1.5600)  acc1: 85.9375 (85.4283)  acc5: 98.4375 (98.3364)  time: 0.0295  data: 0.0003  max mem: 0\n",
      "Epoch: [13]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4022.167787950074  loss: 1.5571 (1.5604)  acc1: 85.9375 (85.3639)  acc5: 98.4375 (98.3389)  time: 0.0280  data: 0.0003  max mem: 0\n",
      "Epoch: [13] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5472 (1.5472)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.5922  data: 0.5645  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 83.07, Acc@5 = 97.66, loss = 1.579209704942341\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:03:15 max_test_acc1 83.07 test_acc5_at_max_test_acc1 97.66 train_acc1 85.29246794871794 train_acc5 85.29246794871794\n",
      "Epoch: [14]  [  0/390]  eta: 0:04:30  lr: 0.01  img/s: 2168.6584288997774  loss: 1.5636 (1.5636)  acc1: 83.5938 (83.5938)  acc5: 99.2188 (99.2188)  time: 0.6947  data: 0.6357  max mem: 0\n",
      "Epoch: [14]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4036.5626983052884  loss: 1.5527 (1.5523)  acc1: 85.9375 (86.5486)  acc5: 97.6562 (98.6154)  time: 0.0288  data: 0.0003  max mem: 0\n",
      "Epoch: [14]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4601.422001285622  loss: 1.5745 (1.5562)  acc1: 83.5938 (86.0774)  acc5: 98.4375 (98.3870)  time: 0.0294  data: 0.0003  max mem: 0\n",
      "Epoch: [14]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4070.904701243555  loss: 1.5595 (1.5566)  acc1: 85.9375 (85.9764)  acc5: 98.4375 (98.4012)  time: 0.0269  data: 0.0003  max mem: 0\n",
      "Epoch: [14] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5774 (1.5774)  acc1: 82.8125 (82.8125)  acc5: 99.2188 (99.2188)  time: 0.6179  data: 0.5925  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 82.81, Acc@5 = 97.68, loss = 1.5810228193862528\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:03:28 max_test_acc1 83.07 test_acc5_at_max_test_acc1 97.66 train_acc1 85.84134615384616 train_acc5 85.84134615384616\n",
      "Epoch: [15]  [  0/390]  eta: 0:04:28  lr: 0.01  img/s: 3270.253106573754  loss: 1.5564 (1.5564)  acc1: 85.1562 (85.1562)  acc5: 100.0000 (100.0000)  time: 0.6883  data: 0.6491  max mem: 0\n",
      "Epoch: [15]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4343.51036787133  loss: 1.5527 (1.5539)  acc1: 85.9375 (86.3011)  acc5: 97.6562 (98.3292)  time: 0.0313  data: 0.0004  max mem: 0\n",
      "Epoch: [15]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4220.915553528889  loss: 1.5520 (1.5507)  acc1: 86.7188 (86.7032)  acc5: 98.4375 (98.5424)  time: 0.0275  data: 0.0003  max mem: 0\n",
      "Epoch: [15]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4919.77926231386  loss: 1.5454 (1.5498)  acc1: 87.5000 (86.9134)  acc5: 98.4375 (98.5154)  time: 0.0288  data: 0.0003  max mem: 0\n",
      "Epoch: [15] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5523 (1.5523)  acc1: 84.3750 (84.3750)  acc5: 97.6562 (97.6562)  time: 0.6328  data: 0.6127  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 84.43, Acc@5 = 97.82, loss = 1.5726190171664274\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:03:42 max_test_acc1 84.43 test_acc5_at_max_test_acc1 97.82 train_acc1 86.93108974358974 train_acc5 86.93108974358974\n",
      "Epoch: [16]  [  0/390]  eta: 0:04:25  lr: 0.01  img/s: 3034.1975358878717  loss: 1.5430 (1.5430)  acc1: 89.0625 (89.0625)  acc5: 97.6562 (97.6562)  time: 0.6809  data: 0.6386  max mem: 0\n",
      "Epoch: [16]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4181.074817958803  loss: 1.5449 (1.5466)  acc1: 86.7188 (87.4691)  acc5: 98.4375 (98.7082)  time: 0.0301  data: 0.0003  max mem: 0\n",
      "Epoch: [16]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4099.753436373633  loss: 1.5391 (1.5458)  acc1: 88.2812 (87.3795)  acc5: 99.2188 (98.6552)  time: 0.0290  data: 0.0004  max mem: 0\n",
      "Epoch: [16]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 5413.151090452616  loss: 1.5448 (1.5458)  acc1: 86.7188 (87.3547)  acc5: 99.2188 (98.6088)  time: 0.0292  data: 0.0004  max mem: 0\n",
      "Epoch: [16] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5574 (1.5574)  acc1: 82.8125 (82.8125)  acc5: 99.2188 (99.2188)  time: 0.6118  data: 0.5856  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 84.57, Acc@5 = 98.02, loss = 1.5702090806598905\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:03:56 max_test_acc1 84.57 test_acc5_at_max_test_acc1 98.02 train_acc1 87.37580128205128 train_acc5 87.37580128205128\n",
      "Epoch: [17]  [  0/390]  eta: 0:04:20  lr: 0.01  img/s: 2454.985284837621  loss: 1.5390 (1.5390)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.6680  data: 0.6158  max mem: 0\n",
      "Epoch: [17]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4565.539424450643  loss: 1.5485 (1.5434)  acc1: 86.7188 (87.7475)  acc5: 98.4375 (98.7392)  time: 0.0282  data: 0.0003  max mem: 0\n",
      "Epoch: [17]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4451.112316046926  loss: 1.5485 (1.5451)  acc1: 85.9375 (87.6788)  acc5: 98.4375 (98.6707)  time: 0.0277  data: 0.0003  max mem: 0\n",
      "Epoch: [17]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4688.624182350116  loss: 1.5378 (1.5447)  acc1: 88.2812 (87.7699)  acc5: 99.2188 (98.7022)  time: 0.0282  data: 0.0003  max mem: 0\n",
      "Epoch: [17] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5556 (1.5556)  acc1: 84.3750 (84.3750)  acc5: 97.6562 (97.6562)  time: 0.5974  data: 0.5752  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.09, Acc@5 = 98.04, loss = 1.5679650457599494\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:04:09 max_test_acc1 85.09 test_acc5_at_max_test_acc1 98.04 train_acc1 87.6983173076923 train_acc5 87.6983173076923\n",
      "Epoch: [18]  [  0/390]  eta: 0:04:16  lr: 0.01  img/s: 2620.058328615769  loss: 1.5366 (1.5366)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.6566  data: 0.6077  max mem: 0\n",
      "Epoch: [18]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4273.361181864493  loss: 1.5323 (1.5399)  acc1: 87.5000 (88.1575)  acc5: 98.4375 (98.7624)  time: 0.0300  data: 0.0003  max mem: 0\n",
      "Epoch: [18]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4510.0421878544  loss: 1.5405 (1.5412)  acc1: 86.7188 (88.1880)  acc5: 98.4375 (98.7212)  time: 0.0300  data: 0.0003  max mem: 0\n",
      "Epoch: [18]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4723.2737606123255  loss: 1.5412 (1.5418)  acc1: 88.2812 (88.1515)  acc5: 98.4375 (98.6971)  time: 0.0287  data: 0.0003  max mem: 0\n",
      "Epoch: [18] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:53  loss: 1.5615 (1.5615)  acc1: 82.8125 (82.8125)  acc5: 99.2188 (99.2188)  time: 0.6713  data: 0.6443  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 84.8, Acc@5 = 97.9, loss = 1.5699069831944719\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:04:23 max_test_acc1 85.09 test_acc5_at_max_test_acc1 98.04 train_acc1 88.15705128205128 train_acc5 88.15705128205128\n",
      "Epoch: [19]  [  0/390]  eta: 0:04:16  lr: 0.01  img/s: 3294.8792016742254  loss: 1.5272 (1.5272)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.6569  data: 0.6180  max mem: 0\n",
      "Epoch: [19]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 3921.513703032782  loss: 1.5376 (1.5369)  acc1: 88.2812 (88.9542)  acc5: 99.2188 (98.8088)  time: 0.0289  data: 0.0004  max mem: 0\n",
      "Epoch: [19]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4069.1459711832163  loss: 1.5444 (1.5401)  acc1: 87.5000 (88.5689)  acc5: 99.2188 (98.6863)  time: 0.0299  data: 0.0004  max mem: 0\n",
      "Epoch: [19]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4393.702580386444  loss: 1.5415 (1.5404)  acc1: 88.2812 (88.6057)  acc5: 98.4375 (98.6945)  time: 0.0304  data: 0.0004  max mem: 0\n",
      "Epoch: [19] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:55  loss: 1.5411 (1.5411)  acc1: 88.2812 (88.2812)  acc5: 100.0000 (100.0000)  time: 0.6978  data: 0.6737  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 84.1, Acc@5 = 97.88, loss = 1.5697363509407527\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:04:36 max_test_acc1 85.09 test_acc5_at_max_test_acc1 98.04 train_acc1 88.57171474358974 train_acc5 88.57171474358974\n",
      "Epoch: [20]  [  0/390]  eta: 0:04:17  lr: 0.01  img/s: 1926.75463680735  loss: 1.5123 (1.5123)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.6596  data: 0.5931  max mem: 0\n",
      "Epoch: [20]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4026.149355811198  loss: 1.5398 (1.5363)  acc1: 87.5000 (88.7918)  acc5: 99.2188 (98.8397)  time: 0.0289  data: 0.0004  max mem: 0\n",
      "Epoch: [20]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4320.405845619005  loss: 1.5333 (1.5362)  acc1: 88.2812 (88.7904)  acc5: 99.2188 (98.8767)  time: 0.0274  data: 0.0003  max mem: 0\n",
      "Epoch: [20]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4188.41404275238  loss: 1.5320 (1.5367)  acc1: 89.0625 (88.8808)  acc5: 98.4375 (98.8424)  time: 0.0297  data: 0.0003  max mem: 0\n",
      "Epoch: [20] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5347 (1.5347)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.5945  data: 0.5663  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.84, Acc@5 = 97.87, loss = 1.565645564960528\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:04:50 max_test_acc1 85.84 test_acc5_at_max_test_acc1 97.87 train_acc1 88.88020833333333 train_acc5 88.88020833333333\n",
      "Epoch: [21]  [  0/390]  eta: 0:04:21  lr: 0.01  img/s: 2340.2857491586897  loss: 1.5686 (1.5686)  acc1: 84.3750 (84.3750)  acc5: 96.0938 (96.0938)  time: 0.6693  data: 0.6146  max mem: 0\n",
      "Epoch: [21]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4110.83478433985  loss: 1.5337 (1.5363)  acc1: 89.8438 (89.1012)  acc5: 99.2188 (98.8088)  time: 0.0279  data: 0.0003  max mem: 0\n",
      "Epoch: [21]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4318.633407070748  loss: 1.5351 (1.5367)  acc1: 89.0625 (88.9848)  acc5: 98.4375 (98.7795)  time: 0.0294  data: 0.0003  max mem: 0\n",
      "Epoch: [21]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4189.067665418227  loss: 1.5304 (1.5357)  acc1: 89.0625 (89.0469)  acc5: 99.2188 (98.8424)  time: 0.0302  data: 0.0003  max mem: 0\n",
      "Epoch: [21] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5416 (1.5416)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.6333  data: 0.6060  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.2, Acc@5 = 97.91, loss = 1.5655754487725753\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:05:03 max_test_acc1 85.84 test_acc5_at_max_test_acc1 97.87 train_acc1 89.14463141025641 train_acc5 89.14463141025641\n",
      "Epoch: [22]  [  0/390]  eta: 0:04:16  lr: 0.01  img/s: 2746.1146791337173  loss: 1.5523 (1.5523)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 0.6565  data: 0.6098  max mem: 0\n",
      "Epoch: [22]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4303.712439677424  loss: 1.5291 (1.5353)  acc1: 89.0625 (89.0702)  acc5: 99.2188 (98.8707)  time: 0.0274  data: 0.0003  max mem: 0\n",
      "Epoch: [22]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4025.605950631355  loss: 1.5331 (1.5344)  acc1: 89.0625 (89.2957)  acc5: 98.4375 (98.8417)  time: 0.0304  data: 0.0003  max mem: 0\n",
      "Epoch: [22]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 7790.560736000464  loss: 1.5257 (1.5333)  acc1: 90.6250 (89.4259)  acc5: 99.2188 (98.9255)  time: 0.0196  data: 0.0002  max mem: 0\n",
      "Epoch: [22] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5521 (1.5521)  acc1: 87.5000 (87.5000)  acc5: 96.8750 (96.8750)  time: 0.6647  data: 0.6409  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.71, Acc@5 = 97.94, loss = 1.5647112282016609\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:05:17 max_test_acc1 85.84 test_acc5_at_max_test_acc1 97.87 train_acc1 89.34695512820512 train_acc5 89.34695512820512\n",
      "Epoch: [23]  [  0/390]  eta: 0:04:21  lr: 0.01  img/s: 2235.136771636497  loss: 1.5423 (1.5423)  acc1: 85.9375 (85.9375)  acc5: 97.6562 (97.6562)  time: 0.6703  data: 0.6130  max mem: 0\n",
      "Epoch: [23]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 3919.252111575889  loss: 1.5320 (1.5328)  acc1: 89.8438 (89.1863)  acc5: 99.2188 (99.0331)  time: 0.0287  data: 0.0004  max mem: 0\n",
      "Epoch: [23]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4504.744225073209  loss: 1.5315 (1.5329)  acc1: 90.6250 (89.2374)  acc5: 98.4375 (98.9661)  time: 0.0308  data: 0.0004  max mem: 0\n",
      "Epoch: [23]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4250.927685181519  loss: 1.5365 (1.5335)  acc1: 89.0625 (89.3117)  acc5: 98.4375 (98.9696)  time: 0.0293  data: 0.0004  max mem: 0\n",
      "Epoch: [23] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5552 (1.5552)  acc1: 85.1562 (85.1562)  acc5: 99.2188 (99.2188)  time: 0.6052  data: 0.5821  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.61, Acc@5 = 97.92, loss = 1.5656606471991237\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:05:30 max_test_acc1 85.84 test_acc5_at_max_test_acc1 97.87 train_acc1 89.29086538461539 train_acc5 89.29086538461539\n",
      "Epoch: [24]  [  0/390]  eta: 0:04:15  lr: 0.01  img/s: 2204.582330356226  loss: 1.5482 (1.5482)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6561  data: 0.5980  max mem: 0\n",
      "Epoch: [24]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4246.993260133531  loss: 1.5359 (1.5314)  acc1: 89.8438 (89.7587)  acc5: 99.2188 (98.9248)  time: 0.0297  data: 0.0003  max mem: 0\n",
      "Epoch: [24]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4612.411935015507  loss: 1.5318 (1.5309)  acc1: 89.0625 (89.8010)  acc5: 99.2188 (98.9544)  time: 0.0288  data: 0.0003  max mem: 0\n",
      "Epoch: [24]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4569.425254485412  loss: 1.5301 (1.5315)  acc1: 89.8438 (89.8048)  acc5: 99.2188 (98.9514)  time: 0.0305  data: 0.0003  max mem: 0\n",
      "Epoch: [24] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5429 (1.5429)  acc1: 88.2812 (88.2812)  acc5: 100.0000 (100.0000)  time: 0.6072  data: 0.5786  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.61, Acc@5 = 97.91, loss = 1.5663590551931648\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:05:44 max_test_acc1 85.84 test_acc5_at_max_test_acc1 97.87 train_acc1 89.68349358974359 train_acc5 89.68349358974359\n",
      "Epoch: [25]  [  0/390]  eta: 0:04:16  lr: 0.01  img/s: 1960.72104684584  loss: 1.5689 (1.5689)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6588  data: 0.5934  max mem: 0\n",
      "Epoch: [25]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4264.095246415948  loss: 1.5282 (1.5318)  acc1: 89.8438 (89.6426)  acc5: 99.2188 (98.9635)  time: 0.0304  data: 0.0003  max mem: 0\n",
      "Epoch: [25]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4368.497851842208  loss: 1.5306 (1.5316)  acc1: 89.0625 (89.7505)  acc5: 99.2188 (98.9272)  time: 0.0287  data: 0.0004  max mem: 0\n",
      "Epoch: [25]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4116.697813868249  loss: 1.5336 (1.5318)  acc1: 89.8438 (89.8775)  acc5: 98.4375 (98.9099)  time: 0.0306  data: 0.0003  max mem: 0\n",
      "Epoch: [25] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:45  loss: 1.5405 (1.5405)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.5793  data: 0.5637  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.92, Acc@5 = 97.99, loss = 1.5637807091580163\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:05:57 max_test_acc1 85.92 test_acc5_at_max_test_acc1 97.99 train_acc1 89.83373397435898 train_acc5 89.83373397435898\n",
      "Epoch: [26]  [  0/390]  eta: 0:04:20  lr: 0.01  img/s: 2010.5416360831073  loss: 1.5395 (1.5395)  acc1: 86.7188 (86.7188)  acc5: 98.4375 (98.4375)  time: 0.6671  data: 0.6034  max mem: 0\n",
      "Epoch: [26]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4904.812000950136  loss: 1.5381 (1.5321)  acc1: 89.0625 (89.8979)  acc5: 99.2188 (98.9790)  time: 0.0297  data: 0.0004  max mem: 0\n",
      "Epoch: [26]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 3941.0311687930352  loss: 1.5247 (1.5327)  acc1: 90.6250 (89.8671)  acc5: 99.2188 (98.8573)  time: 0.0310  data: 0.0004  max mem: 0\n",
      "Epoch: [26]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4955.883984122588  loss: 1.5303 (1.5319)  acc1: 89.0625 (89.8100)  acc5: 99.2188 (98.8580)  time: 0.0305  data: 0.0003  max mem: 0\n",
      "Epoch: [26] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:53  loss: 1.5463 (1.5463)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6712  data: 0.6429  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.54, Acc@5 = 97.91, loss = 1.5655263164375401\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:06:11 max_test_acc1 85.92 test_acc5_at_max_test_acc1 97.99 train_acc1 89.93790064102564 train_acc5 89.93790064102564\n",
      "Epoch: [27]  [  0/390]  eta: 0:04:15  lr: 0.01  img/s: 3185.685959436526  loss: 1.5408 (1.5408)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.6539  data: 0.6136  max mem: 0\n",
      "Epoch: [27]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 6088.835720685471  loss: 1.5304 (1.5313)  acc1: 90.6250 (89.6194)  acc5: 99.2188 (99.0176)  time: 0.0284  data: 0.0003  max mem: 0\n",
      "Epoch: [27]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 5346.4678138942  loss: 1.5355 (1.5315)  acc1: 89.8438 (89.8982)  acc5: 98.4375 (98.9039)  time: 0.0266  data: 0.0003  max mem: 0\n",
      "Epoch: [27]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 3803.3871177987476  loss: 1.5317 (1.5312)  acc1: 89.0625 (89.9683)  acc5: 98.4375 (98.8787)  time: 0.0272  data: 0.0004  max mem: 0\n",
      "Epoch: [27] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5406 (1.5406)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6186  data: 0.5933  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.48, Acc@5 = 97.97, loss = 1.5652827673320528\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:06:24 max_test_acc1 85.92 test_acc5_at_max_test_acc1 97.99 train_acc1 90.01201923076923 train_acc5 90.01201923076923\n",
      "Epoch: [28]  [  0/390]  eta: 0:04:13  lr: 0.01  img/s: 2431.414508663714  loss: 1.5452 (1.5452)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.6511  data: 0.5984  max mem: 0\n",
      "Epoch: [28]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4268.774098133851  loss: 1.5218 (1.5297)  acc1: 90.6250 (90.0758)  acc5: 99.2188 (99.1182)  time: 0.0281  data: 0.0003  max mem: 0\n",
      "Epoch: [28]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 5755.477186963979  loss: 1.5263 (1.5307)  acc1: 89.8438 (89.8204)  acc5: 99.2188 (98.9661)  time: 0.0284  data: 0.0003  max mem: 0\n",
      "Epoch: [28]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4248.707370153766  loss: 1.5285 (1.5314)  acc1: 89.8438 (89.7451)  acc5: 98.4375 (98.9514)  time: 0.0296  data: 0.0003  max mem: 0\n",
      "Epoch: [28] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5540 (1.5540)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 0.6258  data: 0.5993  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.21, Acc@5 = 97.91, loss = 1.5630927448031269\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:06:38 max_test_acc1 86.21 test_acc5_at_max_test_acc1 97.91 train_acc1 89.76161858974359 train_acc5 89.76161858974359\n",
      "Epoch: [29]  [  0/390]  eta: 0:04:10  lr: 0.01  img/s: 2987.29627527571  loss: 1.5475 (1.5475)  acc1: 85.9375 (85.9375)  acc5: 97.6562 (97.6562)  time: 0.6421  data: 0.5992  max mem: 0\n",
      "Epoch: [29]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4847.724200204068  loss: 1.5275 (1.5290)  acc1: 90.6250 (90.1841)  acc5: 99.2188 (99.0176)  time: 0.0274  data: 0.0004  max mem: 0\n",
      "Epoch: [29]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4298.371606312199  loss: 1.5246 (1.5288)  acc1: 89.8438 (90.2130)  acc5: 99.2188 (98.9855)  time: 0.0301  data: 0.0004  max mem: 0\n",
      "Epoch: [29]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4134.61056003943  loss: 1.5310 (1.5296)  acc1: 88.2812 (90.0773)  acc5: 99.2188 (98.9566)  time: 0.0286  data: 0.0004  max mem: 0\n",
      "Epoch: [29] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5495 (1.5495)  acc1: 86.7188 (86.7188)  acc5: 99.2188 (99.2188)  time: 0.6049  data: 0.5800  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.81, Acc@5 = 97.98, loss = 1.5633206367492676\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:06:51 max_test_acc1 86.21 test_acc5_at_max_test_acc1 97.91 train_acc1 90.11017628205128 train_acc5 90.11017628205128\n",
      "Epoch: [30]  [  0/390]  eta: 0:03:54  lr: 0.01  img/s: 2421.882990865005  loss: 1.5547 (1.5547)  acc1: 89.8438 (89.8438)  acc5: 97.6562 (97.6562)  time: 0.6024  data: 0.5495  max mem: 0\n",
      "Epoch: [30]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4953.5519325343  loss: 1.5197 (1.5277)  acc1: 89.8438 (89.9752)  acc5: 99.2188 (99.0331)  time: 0.0246  data: 0.0003  max mem: 0\n",
      "Epoch: [30]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 4524.561652494164  loss: 1.5202 (1.5284)  acc1: 89.8438 (90.1314)  acc5: 99.2188 (98.9817)  time: 0.0249  data: 0.0002  max mem: 0\n",
      "Epoch: [30]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4900.916627869825  loss: 1.5244 (1.5291)  acc1: 90.6250 (90.1760)  acc5: 99.2188 (98.9696)  time: 0.0277  data: 0.0003  max mem: 0\n",
      "Epoch: [30] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5423 (1.5423)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.6475  data: 0.6296  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.0, Acc@5 = 97.89, loss = 1.56301864793029\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:07:03 max_test_acc1 86.21 test_acc5_at_max_test_acc1 97.91 train_acc1 90.22435897435898 train_acc5 90.22435897435898\n",
      "Epoch: [31]  [  0/390]  eta: 0:04:17  lr: 0.01  img/s: 2278.5649313719664  loss: 1.5447 (1.5447)  acc1: 87.5000 (87.5000)  acc5: 97.6562 (97.6562)  time: 0.6595  data: 0.6033  max mem: 0\n",
      "Epoch: [31]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 5438.209436599745  loss: 1.5222 (1.5269)  acc1: 89.8438 (90.1377)  acc5: 99.2188 (99.1259)  time: 0.0304  data: 0.0003  max mem: 0\n",
      "Epoch: [31]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 6308.261603177214  loss: 1.5328 (1.5287)  acc1: 89.0625 (90.2169)  acc5: 99.2188 (99.0166)  time: 0.0283  data: 0.0004  max mem: 0\n",
      "Epoch: [31]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 5483.9824306931705  loss: 1.5267 (1.5284)  acc1: 90.6250 (90.2357)  acc5: 99.2188 (99.0059)  time: 0.0274  data: 0.0003  max mem: 0\n",
      "Epoch: [31] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5348 (1.5348)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.6553  data: 0.6289  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.07, Acc@5 = 97.94, loss = 1.562512551681905\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:07:17 max_test_acc1 86.21 test_acc5_at_max_test_acc1 97.91 train_acc1 90.25841346153847 train_acc5 90.25841346153847\n",
      "Epoch: [32]  [  0/390]  eta: 0:04:21  lr: 0.01  img/s: 2806.8893129571443  loss: 1.5350 (1.5350)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.6698  data: 0.6241  max mem: 0\n",
      "Epoch: [32]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4170.357028003263  loss: 1.5264 (1.5289)  acc1: 89.8438 (90.4084)  acc5: 99.2188 (98.9171)  time: 0.0294  data: 0.0004  max mem: 0\n",
      "Epoch: [32]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 3986.1225229238594  loss: 1.5271 (1.5282)  acc1: 90.6250 (90.3451)  acc5: 99.2188 (98.9467)  time: 0.0285  data: 0.0003  max mem: 0\n",
      "Epoch: [32]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4476.72221805295  loss: 1.5282 (1.5286)  acc1: 89.8438 (90.3577)  acc5: 99.2188 (98.9618)  time: 0.0288  data: 0.0003  max mem: 0\n",
      "Epoch: [32] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5420 (1.5420)  acc1: 88.2812 (88.2812)  acc5: 97.6562 (97.6562)  time: 0.6239  data: 0.5977  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.79, Acc@5 = 97.82, loss = 1.563958608651463\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:07:30 max_test_acc1 86.21 test_acc5_at_max_test_acc1 97.91 train_acc1 90.28846153846153 train_acc5 90.28846153846153\n",
      "Epoch: [33]  [  0/390]  eta: 0:04:17  lr: 0.01  img/s: 2714.9383404046584  loss: 1.5282 (1.5282)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.6607  data: 0.6135  max mem: 0\n",
      "Epoch: [33]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 3839.179862700229  loss: 1.5242 (1.5291)  acc1: 90.6250 (90.4858)  acc5: 99.2188 (98.8939)  time: 0.0317  data: 0.0004  max mem: 0\n",
      "Epoch: [33]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4634.710083997341  loss: 1.5218 (1.5275)  acc1: 90.6250 (90.6444)  acc5: 99.2188 (98.9428)  time: 0.0293  data: 0.0004  max mem: 0\n",
      "Epoch: [33]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4458.98666135114  loss: 1.5330 (1.5277)  acc1: 90.6250 (90.5342)  acc5: 98.4375 (98.9618)  time: 0.0290  data: 0.0004  max mem: 0\n",
      "Epoch: [33] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5538 (1.5538)  acc1: 89.8438 (89.8438)  acc5: 96.0938 (96.0938)  time: 0.6081  data: 0.5814  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.6, Acc@5 = 97.81, loss = 1.5605800151824951\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:07:44 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.48076923076923 train_acc5 90.48076923076923\n",
      "Epoch: [34]  [  0/390]  eta: 0:04:15  lr: 0.01  img/s: 2128.8350529362783  loss: 1.5343 (1.5343)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.6562  data: 0.5960  max mem: 0\n",
      "Epoch: [34]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4396.1850608407985  loss: 1.5233 (1.5268)  acc1: 90.6250 (90.4858)  acc5: 99.2188 (99.0563)  time: 0.0297  data: 0.0004  max mem: 0\n",
      "Epoch: [34]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4102.792495510298  loss: 1.5299 (1.5270)  acc1: 89.8438 (90.3957)  acc5: 99.2188 (99.0866)  time: 0.0288  data: 0.0004  max mem: 0\n",
      "Epoch: [34]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4339.087133978292  loss: 1.5274 (1.5276)  acc1: 89.0625 (90.3421)  acc5: 98.4375 (99.0111)  time: 0.0299  data: 0.0004  max mem: 0\n",
      "Epoch: [34] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5483 (1.5483)  acc1: 87.5000 (87.5000)  acc5: 97.6562 (97.6562)  time: 0.6198  data: 0.5926  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.78, Acc@5 = 98.0, loss = 1.564995193783241\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:07:57 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.40064102564102 train_acc5 90.40064102564102\n",
      "Epoch: [35]  [  0/390]  eta: 0:04:24  lr: 0.01  img/s: 2405.561956994162  loss: 1.5427 (1.5427)  acc1: 89.8438 (89.8438)  acc5: 97.6562 (97.6562)  time: 0.6782  data: 0.6250  max mem: 0\n",
      "Epoch: [35]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4362.640576624601  loss: 1.5227 (1.5248)  acc1: 89.8438 (90.6250)  acc5: 99.2188 (98.9248)  time: 0.0308  data: 0.0004  max mem: 0\n",
      "Epoch: [35]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4333.83311134252  loss: 1.5216 (1.5267)  acc1: 91.4062 (90.4112)  acc5: 99.2188 (98.9389)  time: 0.0296  data: 0.0003  max mem: 0\n",
      "Epoch: [35]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4588.640273504274  loss: 1.5272 (1.5270)  acc1: 91.4062 (90.4563)  acc5: 98.4375 (98.9410)  time: 0.0267  data: 0.0003  max mem: 0\n",
      "Epoch: [35] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5376 (1.5376)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.6374  data: 0.6142  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.7, Acc@5 = 98.0, loss = 1.5641787037064758\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:08:11 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.35657051282051 train_acc5 90.35657051282051\n",
      "Epoch: [36]  [  0/390]  eta: 0:04:20  lr: 0.01  img/s: 2503.4549083244738  loss: 1.5212 (1.5212)  acc1: 89.8438 (89.8438)  acc5: 97.6562 (97.6562)  time: 0.6681  data: 0.6170  max mem: 0\n",
      "Epoch: [36]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4559.877966332026  loss: 1.5282 (1.5301)  acc1: 89.0625 (90.0835)  acc5: 99.2188 (98.9790)  time: 0.0292  data: 0.0003  max mem: 0\n",
      "Epoch: [36]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 5501.064737586327  loss: 1.5283 (1.5291)  acc1: 90.6250 (90.2402)  acc5: 99.2188 (98.9506)  time: 0.0267  data: 0.0003  max mem: 0\n",
      "Epoch: [36]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4344.424221335686  loss: 1.5213 (1.5282)  acc1: 91.4062 (90.3317)  acc5: 99.2188 (98.9722)  time: 0.0295  data: 0.0003  max mem: 0\n",
      "Epoch: [36] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5448 (1.5448)  acc1: 89.8438 (89.8438)  acc5: 97.6562 (97.6562)  time: 0.6200  data: 0.5903  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.13, Acc@5 = 97.94, loss = 1.5636629379248317\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:08:24 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.3545673076923 train_acc5 90.3545673076923\n",
      "Epoch: [37]  [  0/390]  eta: 0:04:01  lr: 0.01  img/s: 2655.594471869652  loss: 1.5266 (1.5266)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.6186  data: 0.5703  max mem: 0\n",
      "Epoch: [37]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 5720.5821266076355  loss: 1.5275 (1.5280)  acc1: 89.8438 (90.3001)  acc5: 99.2188 (98.9712)  time: 0.0275  data: 0.0003  max mem: 0\n",
      "Epoch: [37]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 3849.5856362306577  loss: 1.5227 (1.5287)  acc1: 91.4062 (90.2674)  acc5: 99.2188 (98.9700)  time: 0.0301  data: 0.0003  max mem: 0\n",
      "Epoch: [37]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4612.530817739746  loss: 1.5248 (1.5278)  acc1: 90.6250 (90.5342)  acc5: 99.2188 (98.9722)  time: 0.0293  data: 0.0003  max mem: 0\n",
      "Epoch: [37] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5439 (1.5439)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.6018  data: 0.5783  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.29, Acc@5 = 97.85, loss = 1.5633011000065864\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:08:38 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.4707532051282 train_acc5 90.4707532051282\n",
      "Epoch: [38]  [  0/390]  eta: 0:04:09  lr: 0.01  img/s: 2295.065542654879  loss: 1.5286 (1.5286)  acc1: 91.4062 (91.4062)  acc5: 97.6562 (97.6562)  time: 0.6388  data: 0.5830  max mem: 0\n",
      "Epoch: [38]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4900.916627869825  loss: 1.5233 (1.5271)  acc1: 90.6250 (90.5476)  acc5: 99.2188 (98.9325)  time: 0.0290  data: 0.0003  max mem: 0\n",
      "Epoch: [38]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4223.040470702987  loss: 1.5232 (1.5270)  acc1: 90.6250 (90.6211)  acc5: 99.2188 (98.9817)  time: 0.0283  data: 0.0003  max mem: 0\n",
      "Epoch: [38]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4703.617592430349  loss: 1.5230 (1.5277)  acc1: 89.8438 (90.4822)  acc5: 99.2188 (98.9748)  time: 0.0297  data: 0.0003  max mem: 0\n",
      "Epoch: [38] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5463 (1.5463)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6565  data: 0.6319  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.24, Acc@5 = 97.92, loss = 1.563197071039224\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:08:51 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.51282051282051 train_acc5 90.51282051282051\n",
      "Epoch: [39]  [  0/390]  eta: 0:04:26  lr: 0.01  img/s: 3523.6536144182933  loss: 1.5053 (1.5053)  acc1: 91.4062 (91.4062)  acc5: 100.0000 (100.0000)  time: 0.6831  data: 0.6467  max mem: 0\n",
      "Epoch: [39]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4486.261485752486  loss: 1.5274 (1.5261)  acc1: 90.6250 (90.8184)  acc5: 99.2188 (99.0718)  time: 0.0252  data: 0.0004  max mem: 0\n",
      "Epoch: [39]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4212.238923541642  loss: 1.5241 (1.5274)  acc1: 89.8438 (90.4656)  acc5: 99.2188 (99.0322)  time: 0.0300  data: 0.0003  max mem: 0\n",
      "Epoch: [39]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 3884.373481510422  loss: 1.5268 (1.5272)  acc1: 89.8438 (90.4433)  acc5: 99.2188 (99.0241)  time: 0.0306  data: 0.0003  max mem: 0\n",
      "Epoch: [39] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:53  loss: 1.5518 (1.5518)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6740  data: 0.6468  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.19, Acc@5 = 97.78, loss = 1.564401172384431\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:09:05 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.3886217948718 train_acc5 90.3886217948718\n",
      "Epoch: [40]  [  0/390]  eta: 0:04:05  lr: 0.01  img/s: 2632.326624271278  loss: 1.4973 (1.4973)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.6301  data: 0.5815  max mem: 0\n",
      "Epoch: [40]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4158.276433091419  loss: 1.5146 (1.5271)  acc1: 91.4062 (90.7333)  acc5: 99.2188 (98.8243)  time: 0.0304  data: 0.0004  max mem: 0\n",
      "Epoch: [40]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 5225.122746914782  loss: 1.5203 (1.5279)  acc1: 91.4062 (90.5084)  acc5: 100.0000 (98.9817)  time: 0.0286  data: 0.0003  max mem: 0\n",
      "Epoch: [40]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4313.983334538646  loss: 1.5323 (1.5281)  acc1: 89.8438 (90.4277)  acc5: 99.2188 (98.9488)  time: 0.0294  data: 0.0003  max mem: 0\n",
      "Epoch: [40] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5505 (1.5505)  acc1: 91.4062 (91.4062)  acc5: 99.2188 (99.2188)  time: 0.5845  data: 0.5572  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.32, Acc@5 = 98.02, loss = 1.561687387997591\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:09:18 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.5088141025641 train_acc5 90.5088141025641\n",
      "Epoch: [41]  [  0/390]  eta: 0:04:16  lr: 0.01  img/s: 2265.306784474065  loss: 1.4957 (1.4957)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.6589  data: 0.6024  max mem: 0\n",
      "Epoch: [41]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4930.849669360764  loss: 1.5197 (1.5240)  acc1: 91.4062 (90.6405)  acc5: 99.2188 (99.1259)  time: 0.0293  data: 0.0004  max mem: 0\n",
      "Epoch: [41]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4605.724756790145  loss: 1.5187 (1.5264)  acc1: 91.4062 (90.5822)  acc5: 99.2188 (99.0516)  time: 0.0299  data: 0.0004  max mem: 0\n",
      "Epoch: [41]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4261.590519054763  loss: 1.5231 (1.5275)  acc1: 89.8438 (90.4303)  acc5: 99.2188 (99.0293)  time: 0.0298  data: 0.0003  max mem: 0\n",
      "Epoch: [41] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5353 (1.5353)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.6376  data: 0.6173  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.14, Acc@5 = 98.02, loss = 1.562808927101425\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:09:32 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.4667467948718 train_acc5 90.4667467948718\n",
      "Epoch: [42]  [  0/390]  eta: 0:04:05  lr: 0.01  img/s: 2450.5589804684114  loss: 1.5301 (1.5301)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.6294  data: 0.5771  max mem: 0\n",
      "Epoch: [42]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4411.175298051879  loss: 1.5286 (1.5274)  acc1: 89.0625 (90.3233)  acc5: 99.2188 (99.0408)  time: 0.0294  data: 0.0004  max mem: 0\n",
      "Epoch: [42]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4245.884866938194  loss: 1.5261 (1.5272)  acc1: 90.6250 (90.4812)  acc5: 99.2188 (99.0089)  time: 0.0307  data: 0.0003  max mem: 0\n",
      "Epoch: [42]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4289.236875534286  loss: 1.5238 (1.5271)  acc1: 89.8438 (90.4667)  acc5: 98.4375 (99.0423)  time: 0.0291  data: 0.0003  max mem: 0\n",
      "Epoch: [42] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5369 (1.5369)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.6083  data: 0.5804  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.16, Acc@5 = 97.79, loss = 1.5621655545657194\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:09:45 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.42067307692308 train_acc5 90.42067307692308\n",
      "Epoch: [43]  [  0/390]  eta: 0:04:07  lr: 0.01  img/s: 2536.957338625839  loss: 1.5397 (1.5397)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.6356  data: 0.5851  max mem: 0\n",
      "Epoch: [43]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4753.1731916777335  loss: 1.5269 (1.5277)  acc1: 89.8438 (90.3079)  acc5: 99.2188 (99.0176)  time: 0.0277  data: 0.0003  max mem: 0\n",
      "Epoch: [43]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4100.598907771625  loss: 1.5315 (1.5278)  acc1: 90.6250 (90.2985)  acc5: 99.2188 (98.9817)  time: 0.0294  data: 0.0003  max mem: 0\n",
      "Epoch: [43]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4058.0732140562523  loss: 1.5262 (1.5277)  acc1: 89.8438 (90.3732)  acc5: 99.2188 (99.0007)  time: 0.0294  data: 0.0004  max mem: 0\n",
      "Epoch: [43] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5557 (1.5557)  acc1: 85.1562 (85.1562)  acc5: 99.2188 (99.2188)  time: 0.6215  data: 0.5942  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.02, Acc@5 = 97.84, loss = 1.563709916947763\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:09:59 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.47275641025641 train_acc5 90.47275641025641\n",
      "Epoch: [44]  [  0/390]  eta: 0:04:15  lr: 0.01  img/s: 2178.3199451434507  loss: 1.5184 (1.5184)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.6553  data: 0.5965  max mem: 0\n",
      "Epoch: [44]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4154.190101829212  loss: 1.5206 (1.5277)  acc1: 89.8438 (90.3775)  acc5: 99.2188 (98.8861)  time: 0.0316  data: 0.0003  max mem: 0\n",
      "Epoch: [44]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4265.72150933202  loss: 1.5292 (1.5281)  acc1: 89.0625 (90.4112)  acc5: 98.4375 (98.9467)  time: 0.0274  data: 0.0003  max mem: 0\n",
      "Epoch: [44]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4362.286095018323  loss: 1.5245 (1.5280)  acc1: 89.8438 (90.4459)  acc5: 99.2188 (98.9877)  time: 0.0303  data: 0.0004  max mem: 0\n",
      "Epoch: [44] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5282 (1.5282)  acc1: 92.1875 (92.1875)  acc5: 99.2188 (99.2188)  time: 0.6156  data: 0.5903  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.13, Acc@5 = 97.75, loss = 1.5623437815074679\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:10:12 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.42067307692308 train_acc5 90.42067307692308\n",
      "Epoch: [45]  [  0/390]  eta: 0:04:06  lr: 0.01  img/s: 2551.12220295089  loss: 1.5354 (1.5354)  acc1: 86.7188 (86.7188)  acc5: 99.2188 (99.2188)  time: 0.6323  data: 0.5821  max mem: 0\n",
      "Epoch: [45]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 3527.6822876967963  loss: 1.5218 (1.5255)  acc1: 91.4062 (91.0504)  acc5: 99.2188 (99.0408)  time: 0.0299  data: 0.0003  max mem: 0\n",
      "Epoch: [45]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4481.9918520002675  loss: 1.5258 (1.5270)  acc1: 89.8438 (90.5978)  acc5: 98.4375 (98.9855)  time: 0.0302  data: 0.0004  max mem: 0\n",
      "Epoch: [45]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4415.746802543161  loss: 1.5273 (1.5267)  acc1: 90.6250 (90.5783)  acc5: 99.2188 (99.0189)  time: 0.0272  data: 0.0003  max mem: 0\n",
      "Epoch: [45] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5498 (1.5498)  acc1: 86.7188 (86.7188)  acc5: 99.2188 (99.2188)  time: 0.5862  data: 0.5593  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.96, Acc@5 = 97.98, loss = 1.5635220521613011\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:10:26 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.625 train_acc5 90.625\n",
      "Epoch: [46]  [  0/390]  eta: 0:04:13  lr: 0.01  img/s: 2198.200530642995  loss: 1.5160 (1.5160)  acc1: 92.1875 (92.1875)  acc5: 99.2188 (99.2188)  time: 0.6489  data: 0.5906  max mem: 0\n",
      "Epoch: [46]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4670.473353632014  loss: 1.5299 (1.5268)  acc1: 89.8438 (90.5863)  acc5: 99.2188 (98.9944)  time: 0.0283  data: 0.0003  max mem: 0\n",
      "Epoch: [46]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4601.3036904986375  loss: 1.5218 (1.5259)  acc1: 90.6250 (90.5239)  acc5: 99.2188 (99.0361)  time: 0.0303  data: 0.0004  max mem: 0\n",
      "Epoch: [46]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4691.779214877478  loss: 1.5296 (1.5268)  acc1: 89.8438 (90.5160)  acc5: 99.2188 (98.9903)  time: 0.0293  data: 0.0003  max mem: 0\n",
      "Epoch: [46] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5323 (1.5323)  acc1: 91.4062 (91.4062)  acc5: 99.2188 (99.2188)  time: 0.6349  data: 0.6066  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.06, Acc@5 = 97.93, loss = 1.5618731085258195\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:10:39 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.43469551282051 train_acc5 90.43469551282051\n",
      "Epoch: [47]  [  0/390]  eta: 0:04:08  lr: 0.01  img/s: 2540.8835692616412  loss: 1.5179 (1.5179)  acc1: 91.4062 (91.4062)  acc5: 99.2188 (99.2188)  time: 0.6378  data: 0.5874  max mem: 0\n",
      "Epoch: [47]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4107.406677479573  loss: 1.5258 (1.5267)  acc1: 90.6250 (90.1918)  acc5: 99.2188 (98.9712)  time: 0.0285  data: 0.0003  max mem: 0\n",
      "Epoch: [47]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4777.451697871432  loss: 1.5286 (1.5277)  acc1: 89.8438 (90.3141)  acc5: 99.2188 (98.9778)  time: 0.0273  data: 0.0003  max mem: 0\n",
      "Epoch: [47]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4532.46865344027  loss: 1.5254 (1.5275)  acc1: 90.6250 (90.3291)  acc5: 99.2188 (98.9774)  time: 0.0290  data: 0.0004  max mem: 0\n",
      "Epoch: [47] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5479 (1.5479)  acc1: 89.8438 (89.8438)  acc5: 100.0000 (100.0000)  time: 0.6116  data: 0.5828  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.95, Acc@5 = 97.91, loss = 1.5660338311255733\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:10:52 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.36658653846153 train_acc5 90.36658653846153\n",
      "Epoch: [48]  [  0/390]  eta: 0:04:35  lr: 0.01  img/s: 3376.5890891709328  loss: 1.5233 (1.5233)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.7067  data: 0.6687  max mem: 0\n",
      "Epoch: [48]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 3918.9374133173715  loss: 1.5237 (1.5261)  acc1: 90.6250 (90.5399)  acc5: 99.2188 (99.0408)  time: 0.0310  data: 0.0005  max mem: 0\n",
      "Epoch: [48]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4705.8026944349485  loss: 1.5287 (1.5276)  acc1: 90.6250 (90.4618)  acc5: 99.2188 (98.9544)  time: 0.0293  data: 0.0003  max mem: 0\n",
      "Epoch: [48]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4512.126941437504  loss: 1.5254 (1.5270)  acc1: 89.8438 (90.4797)  acc5: 99.2188 (99.0189)  time: 0.0312  data: 0.0004  max mem: 0\n",
      "Epoch: [48] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:42  loss: 1.5469 (1.5469)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.5426  data: 0.5192  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.16, Acc@5 = 97.97, loss = 1.5632119224041323\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:11:06 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.5068108974359 train_acc5 90.5068108974359\n",
      "Epoch: [49]  [  0/390]  eta: 0:04:31  lr: 0.01  img/s: 2517.908236055548  loss: 1.5384 (1.5384)  acc1: 86.7188 (86.7188)  acc5: 99.2188 (99.2188)  time: 0.6964  data: 0.6455  max mem: 0\n",
      "Epoch: [49]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4476.236989111041  loss: 1.5266 (1.5268)  acc1: 89.8438 (90.4858)  acc5: 99.2188 (98.9790)  time: 0.0296  data: 0.0003  max mem: 0\n",
      "Epoch: [49]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 7256.679399321466  loss: 1.5279 (1.5268)  acc1: 89.0625 (90.5822)  acc5: 98.4375 (98.9817)  time: 0.0183  data: 0.0003  max mem: 0\n",
      "Epoch: [49]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 5188.110976894309  loss: 1.5286 (1.5271)  acc1: 89.8438 (90.4667)  acc5: 99.2188 (98.9800)  time: 0.0255  data: 0.0003  max mem: 0\n",
      "Epoch: [49] Total time: 0:00:10\n",
      "Test:  [ 0/79]  eta: 0:00:55  loss: 1.5277 (1.5277)  acc1: 91.4062 (91.4062)  acc5: 99.2188 (99.2188)  time: 0.7075  data: 0.6925  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.02, Acc@5 = 97.93, loss = 1.5620353372791145\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:11:18 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.40665064102564 train_acc5 90.40665064102564\n",
      "Epoch: [50]  [  0/390]  eta: 0:03:32  lr: 0.01  img/s: 2643.7466120390795  loss: 1.5362 (1.5362)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.5445  data: 0.4961  max mem: 0\n",
      "Epoch: [50]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4304.678651036739  loss: 1.5207 (1.5261)  acc1: 90.6250 (90.6173)  acc5: 99.2188 (99.0873)  time: 0.0284  data: 0.0003  max mem: 0\n",
      "Epoch: [50]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 4574.486733354919  loss: 1.5264 (1.5255)  acc1: 90.6250 (90.5239)  acc5: 98.4375 (99.0594)  time: 0.0283  data: 0.0003  max mem: 0\n",
      "Epoch: [50]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4905.080875634982  loss: 1.5297 (1.5269)  acc1: 90.6250 (90.4563)  acc5: 99.2188 (99.0293)  time: 0.0296  data: 0.0004  max mem: 0\n",
      "Epoch: [50] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:54  loss: 1.5312 (1.5312)  acc1: 92.1875 (92.1875)  acc5: 97.6562 (97.6562)  time: 0.6894  data: 0.6591  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.12, Acc@5 = 97.78, loss = 1.5625836290890658\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:11:32 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.48677884615384 train_acc5 90.48677884615384\n",
      "Epoch: [51]  [  0/390]  eta: 0:04:23  lr: 0.01  img/s: 2726.2294442638936  loss: 1.5235 (1.5235)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.6755  data: 0.6285  max mem: 0\n",
      "Epoch: [51]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4207.254455119665  loss: 1.5291 (1.5273)  acc1: 90.6250 (90.3156)  acc5: 98.4375 (99.0950)  time: 0.0306  data: 0.0003  max mem: 0\n",
      "Epoch: [51]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 3732.7547122584774  loss: 1.5207 (1.5268)  acc1: 90.6250 (90.4268)  acc5: 99.2188 (99.0438)  time: 0.0302  data: 0.0004  max mem: 0\n",
      "Epoch: [51]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4690.221655338703  loss: 1.5279 (1.5274)  acc1: 89.8438 (90.3058)  acc5: 99.2188 (99.0371)  time: 0.0297  data: 0.0003  max mem: 0\n",
      "Epoch: [51] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5435 (1.5435)  acc1: 91.4062 (91.4062)  acc5: 98.4375 (98.4375)  time: 0.5992  data: 0.5816  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.33, Acc@5 = 97.81, loss = 1.562877811963045\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:11:45 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.27844551282051 train_acc5 90.27844551282051\n",
      "Epoch: [52]  [  0/390]  eta: 0:04:07  lr: 0.01  img/s: 3114.732758970789  loss: 1.5122 (1.5122)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.6338  data: 0.5926  max mem: 0\n",
      "Epoch: [52]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4156.248351035828  loss: 1.5298 (1.5295)  acc1: 89.8438 (90.3388)  acc5: 99.2188 (98.8475)  time: 0.0291  data: 0.0004  max mem: 0\n",
      "Epoch: [52]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 5405.847290888403  loss: 1.5288 (1.5297)  acc1: 88.2812 (90.2208)  acc5: 99.2188 (98.9272)  time: 0.0287  data: 0.0004  max mem: 0\n",
      "Epoch: [52]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4774.732408395589  loss: 1.5252 (1.5284)  acc1: 90.6250 (90.3395)  acc5: 99.2188 (98.9826)  time: 0.0272  data: 0.0003  max mem: 0\n",
      "Epoch: [52] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5559 (1.5559)  acc1: 87.5000 (87.5000)  acc5: 97.6562 (97.6562)  time: 0.6368  data: 0.6128  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.01, Acc@5 = 97.75, loss = 1.5637951923322073\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:11:59 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.34655448717949 train_acc5 90.34655448717949\n",
      "Epoch: [53]  [  0/390]  eta: 0:04:21  lr: 0.01  img/s: 2249.61622459669  loss: 1.5201 (1.5201)  acc1: 91.4062 (91.4062)  acc5: 98.4375 (98.4375)  time: 0.6704  data: 0.6135  max mem: 0\n",
      "Epoch: [53]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4248.505638339123  loss: 1.5332 (1.5279)  acc1: 89.0625 (90.3929)  acc5: 99.2188 (98.9403)  time: 0.0304  data: 0.0004  max mem: 0\n",
      "Epoch: [53]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 6397.565623584928  loss: 1.5231 (1.5275)  acc1: 90.6250 (90.4035)  acc5: 99.2188 (99.0322)  time: 0.0298  data: 0.0003  max mem: 0\n",
      "Epoch: [53]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4110.771831762391  loss: 1.5231 (1.5269)  acc1: 90.6250 (90.4563)  acc5: 99.2188 (99.0319)  time: 0.0307  data: 0.0004  max mem: 0\n",
      "Epoch: [53] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5372 (1.5372)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.6241  data: 0.5972  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.38, Acc@5 = 97.91, loss = 1.5613673638693895\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:12:12 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.48277243589743 train_acc5 90.48277243589743\n",
      "Epoch: [54]  [  0/390]  eta: 0:04:18  lr: 0.01  img/s: 2268.1875147868996  loss: 1.5153 (1.5153)  acc1: 91.4062 (91.4062)  acc5: 99.2188 (99.2188)  time: 0.6630  data: 0.6065  max mem: 0\n",
      "Epoch: [54]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4364.378369589958  loss: 1.5208 (1.5270)  acc1: 90.6250 (90.4007)  acc5: 99.2188 (99.0331)  time: 0.0289  data: 0.0003  max mem: 0\n",
      "Epoch: [54]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4097.500549517646  loss: 1.5202 (1.5272)  acc1: 91.4062 (90.3840)  acc5: 99.2188 (98.9622)  time: 0.0299  data: 0.0003  max mem: 0\n",
      "Epoch: [54]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4673.726055541047  loss: 1.5247 (1.5267)  acc1: 90.6250 (90.5419)  acc5: 99.2188 (98.9903)  time: 0.0303  data: 0.0004  max mem: 0\n",
      "Epoch: [54] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5349 (1.5349)  acc1: 92.1875 (92.1875)  acc5: 99.2188 (99.2188)  time: 0.6645  data: 0.6405  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.28, Acc@5 = 98.07, loss = 1.5608939641638646\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:12:26 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.57091346153847 train_acc5 90.57091346153847\n",
      "Epoch: [55]  [  0/390]  eta: 0:04:20  lr: 0.01  img/s: 2529.367562601588  loss: 1.5184 (1.5184)  acc1: 92.9688 (92.9688)  acc5: 99.2188 (99.2188)  time: 0.6673  data: 0.6166  max mem: 0\n",
      "Epoch: [55]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4171.167057726672  loss: 1.5264 (1.5292)  acc1: 89.8438 (90.4548)  acc5: 99.2188 (98.9790)  time: 0.0294  data: 0.0003  max mem: 0\n",
      "Epoch: [55]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4260.440684691262  loss: 1.5269 (1.5286)  acc1: 90.6250 (90.3024)  acc5: 99.2188 (99.0205)  time: 0.0309  data: 0.0003  max mem: 0\n",
      "Epoch: [55]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4047.3046310187033  loss: 1.5170 (1.5273)  acc1: 91.4062 (90.4874)  acc5: 100.0000 (99.0526)  time: 0.0292  data: 0.0003  max mem: 0\n",
      "Epoch: [55] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5485 (1.5485)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.6193  data: 0.5923  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.05, Acc@5 = 97.69, loss = 1.5637760132173948\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:12:39 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.48677884615384 train_acc5 90.48677884615384\n",
      "Epoch: [56]  [  0/390]  eta: 0:04:26  lr: 0.01  img/s: 2808.0931862522034  loss: 1.5231 (1.5231)  acc1: 91.4062 (91.4062)  acc5: 99.2188 (99.2188)  time: 0.6825  data: 0.6368  max mem: 0\n",
      "Epoch: [56]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 5348.545104954323  loss: 1.5300 (1.5264)  acc1: 90.6250 (90.6405)  acc5: 99.2188 (99.0254)  time: 0.0296  data: 0.0003  max mem: 0\n",
      "Epoch: [56]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4269.656770663507  loss: 1.5267 (1.5270)  acc1: 89.0625 (90.5006)  acc5: 99.2188 (98.9506)  time: 0.0295  data: 0.0004  max mem: 0\n",
      "Epoch: [56]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4822.035010822997  loss: 1.5208 (1.5267)  acc1: 90.6250 (90.4667)  acc5: 99.2188 (98.9670)  time: 0.0292  data: 0.0003  max mem: 0\n",
      "Epoch: [56] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5504 (1.5504)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 0.6198  data: 0.5977  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.12, Acc@5 = 97.87, loss = 1.5642626979683019\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:12:53 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.5088141025641 train_acc5 90.5088141025641\n",
      "Epoch: [57]  [  0/390]  eta: 0:04:19  lr: 0.01  img/s: 2188.0322618771065  loss: 1.5193 (1.5193)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 0.6642  data: 0.6056  max mem: 0\n",
      "Epoch: [57]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4958.401403832833  loss: 1.5245 (1.5285)  acc1: 89.0625 (90.0990)  acc5: 99.2188 (99.0486)  time: 0.0304  data: 0.0004  max mem: 0\n",
      "Epoch: [57]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4479.897463284379  loss: 1.5280 (1.5285)  acc1: 90.6250 (90.2402)  acc5: 99.2188 (98.9700)  time: 0.0302  data: 0.0003  max mem: 0\n",
      "Epoch: [57]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4254.397362748828  loss: 1.5293 (1.5279)  acc1: 89.8438 (90.3732)  acc5: 99.2188 (99.0085)  time: 0.0292  data: 0.0004  max mem: 0\n",
      "Epoch: [57] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:57  loss: 1.5397 (1.5397)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.7273  data: 0.6988  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.91, Acc@5 = 98.02, loss = 1.5617473849767372\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:13:06 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.42267628205128 train_acc5 90.42267628205128\n",
      "Epoch: [58]  [  0/390]  eta: 0:04:09  lr: 0.01  img/s: 2434.5458140230908  loss: 1.5250 (1.5250)  acc1: 92.9688 (92.9688)  acc5: 98.4375 (98.4375)  time: 0.6406  data: 0.5879  max mem: 0\n",
      "Epoch: [58]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4628.157619330867  loss: 1.5302 (1.5282)  acc1: 89.8438 (90.3079)  acc5: 99.2188 (98.9944)  time: 0.0281  data: 0.0004  max mem: 0\n",
      "Epoch: [58]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 6781.070479462436  loss: 1.5305 (1.5282)  acc1: 90.6250 (90.4462)  acc5: 99.2188 (98.9389)  time: 0.0179  data: 0.0004  max mem: 0\n",
      "Epoch: [58]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4502.930644904258  loss: 1.5230 (1.5275)  acc1: 91.4062 (90.6016)  acc5: 99.2188 (98.9514)  time: 0.0297  data: 0.0004  max mem: 0\n",
      "Epoch: [58] Total time: 0:00:10\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5487 (1.5487)  acc1: 85.9375 (85.9375)  acc5: 99.2188 (99.2188)  time: 0.6144  data: 0.5887  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.18, Acc@5 = 98.0, loss = 1.5622864207134972\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:13:19 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.63701923076923 train_acc5 90.63701923076923\n",
      "Epoch: [59]  [  0/390]  eta: 0:04:25  lr: 0.01  img/s: 2746.92961666769  loss: 1.5488 (1.5488)  acc1: 88.2812 (88.2812)  acc5: 96.8750 (96.8750)  time: 0.6816  data: 0.6349  max mem: 0\n",
      "Epoch: [59]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4387.168019089178  loss: 1.5277 (1.5285)  acc1: 90.6250 (90.2537)  acc5: 99.2188 (98.9403)  time: 0.0282  data: 0.0003  max mem: 0\n",
      "Epoch: [59]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4105.647670612707  loss: 1.5181 (1.5281)  acc1: 90.6250 (90.3568)  acc5: 99.2188 (99.0089)  time: 0.0290  data: 0.0003  max mem: 0\n",
      "Epoch: [59]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4274.9604809491575  loss: 1.5200 (1.5275)  acc1: 90.6250 (90.4070)  acc5: 99.2188 (99.0033)  time: 0.0283  data: 0.0003  max mem: 0\n",
      "Epoch: [59] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:36  loss: 1.5333 (1.5333)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.4673  data: 0.4455  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.22, Acc@5 = 97.96, loss = 1.5627034223532374\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:13:32 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.51883012820512 train_acc5 90.51883012820512\n",
      "Epoch: [60]  [  0/390]  eta: 0:03:07  lr: 0.01  img/s: 2868.0380573852376  loss: 1.5560 (1.5560)  acc1: 87.5000 (87.5000)  acc5: 96.8750 (96.8750)  time: 0.4805  data: 0.4358  max mem: 0\n",
      "Epoch: [60]  [100/390]  eta: 0:00:08  lr: 0.01  img/s: 5581.300870143777  loss: 1.5175 (1.5272)  acc1: 90.6250 (90.2614)  acc5: 99.2188 (99.0795)  time: 0.0247  data: 0.0003  max mem: 0\n",
      "Epoch: [60]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 6575.835184890315  loss: 1.5244 (1.5263)  acc1: 90.6250 (90.6289)  acc5: 99.2188 (99.0555)  time: 0.0254  data: 0.0003  max mem: 0\n",
      "Epoch: [60]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4654.681047338304  loss: 1.5233 (1.5264)  acc1: 89.8438 (90.5497)  acc5: 99.2188 (99.0371)  time: 0.0263  data: 0.0004  max mem: 0\n",
      "Epoch: [60] Total time: 0:00:10\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5578 (1.5578)  acc1: 86.7188 (86.7188)  acc5: 96.8750 (96.8750)  time: 0.6424  data: 0.6181  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.84, Acc@5 = 97.77, loss = 1.5640455650377878\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:13:44 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.56089743589743 train_acc5 90.56089743589743\n",
      "Epoch: [61]  [  0/390]  eta: 0:04:34  lr: 0.01  img/s: 2348.0133829581587  loss: 1.5389 (1.5389)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.7043  data: 0.6498  max mem: 0\n",
      "Epoch: [61]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4201.492491058921  loss: 1.5209 (1.5257)  acc1: 90.6250 (90.5167)  acc5: 100.0000 (99.0640)  time: 0.0290  data: 0.0003  max mem: 0\n",
      "Epoch: [61]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 5163.610510522064  loss: 1.5219 (1.5258)  acc1: 90.6250 (90.5784)  acc5: 99.2188 (99.0711)  time: 0.0287  data: 0.0003  max mem: 0\n",
      "Epoch: [61]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4088.887372429551  loss: 1.5223 (1.5271)  acc1: 90.6250 (90.4926)  acc5: 99.2188 (98.9955)  time: 0.0296  data: 0.0003  max mem: 0\n",
      "Epoch: [61] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5585 (1.5585)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.6342  data: 0.6023  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.88, Acc@5 = 97.83, loss = 1.564243971546994\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:13:57 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.47275641025641 train_acc5 90.47275641025641\n",
      "Epoch: [62]  [  0/390]  eta: 0:04:16  lr: 0.01  img/s: 2305.2355254796216  loss: 1.5223 (1.5223)  acc1: 91.4062 (91.4062)  acc5: 99.2188 (99.2188)  time: 0.6570  data: 0.6014  max mem: 0\n",
      "Epoch: [62]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4817.145912965455  loss: 1.5249 (1.5258)  acc1: 90.6250 (90.3929)  acc5: 99.2188 (99.0486)  time: 0.0296  data: 0.0003  max mem: 0\n",
      "Epoch: [62]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4946.568927709289  loss: 1.5319 (1.5258)  acc1: 90.6250 (90.5589)  acc5: 99.2188 (99.0011)  time: 0.0278  data: 0.0003  max mem: 0\n",
      "Epoch: [62]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4541.13303559345  loss: 1.5315 (1.5270)  acc1: 89.0625 (90.4329)  acc5: 99.2188 (98.9722)  time: 0.0296  data: 0.0003  max mem: 0\n",
      "Epoch: [62] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:54  loss: 1.5401 (1.5401)  acc1: 84.3750 (84.3750)  acc5: 99.2188 (99.2188)  time: 0.6850  data: 0.6566  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.98, Acc@5 = 97.76, loss = 1.5618458503409276\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:14:11 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.5108173076923 train_acc5 90.5108173076923\n",
      "Epoch: [63]  [  0/390]  eta: 0:04:18  lr: 0.01  img/s: 2864.9311717557657  loss: 1.5229 (1.5229)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 0.6617  data: 0.6170  max mem: 0\n",
      "Epoch: [63]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4731.3907817044155  loss: 1.5296 (1.5255)  acc1: 90.6250 (90.3079)  acc5: 98.4375 (99.0408)  time: 0.0293  data: 0.0003  max mem: 0\n",
      "Epoch: [63]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4084.252539007524  loss: 1.5239 (1.5270)  acc1: 90.6250 (90.3179)  acc5: 99.2188 (98.9855)  time: 0.0305  data: 0.0004  max mem: 0\n",
      "Epoch: [63]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4576.280405059839  loss: 1.5254 (1.5269)  acc1: 90.6250 (90.3862)  acc5: 99.2188 (98.9826)  time: 0.0295  data: 0.0003  max mem: 0\n",
      "Epoch: [63] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5406 (1.5406)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.6120  data: 0.5881  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.15, Acc@5 = 97.74, loss = 1.56284566770626\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:14:24 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.36057692307692 train_acc5 90.36057692307692\n",
      "Epoch: [64]  [  0/390]  eta: 0:04:23  lr: 0.01  img/s: 2344.5987544872523  loss: 1.5147 (1.5147)  acc1: 92.9688 (92.9688)  acc5: 99.2188 (99.2188)  time: 0.6745  data: 0.6199  max mem: 0\n",
      "Epoch: [64]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4221.944543181139  loss: 1.5174 (1.5248)  acc1: 91.4062 (90.8725)  acc5: 99.2188 (99.0408)  time: 0.0305  data: 0.0004  max mem: 0\n",
      "Epoch: [64]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4541.670856949497  loss: 1.5257 (1.5258)  acc1: 91.4062 (90.7222)  acc5: 99.2188 (99.0400)  time: 0.0301  data: 0.0003  max mem: 0\n",
      "Epoch: [64]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4247.833337289437  loss: 1.5324 (1.5260)  acc1: 89.8438 (90.6172)  acc5: 98.4375 (99.0215)  time: 0.0302  data: 0.0003  max mem: 0\n",
      "Epoch: [64] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5315 (1.5315)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.6188  data: 0.5941  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.2, Acc@5 = 97.92, loss = 1.5619137483307077\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:14:38 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.59695512820512 train_acc5 90.59695512820512\n",
      "Epoch: [65]  [  0/390]  eta: 0:04:04  lr: 0.01  img/s: 2527.283867626983  loss: 1.5348 (1.5348)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.6263  data: 0.5756  max mem: 0\n",
      "Epoch: [65]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4184.0400268092335  loss: 1.5322 (1.5283)  acc1: 89.8438 (90.3465)  acc5: 99.2188 (99.0640)  time: 0.0300  data: 0.0003  max mem: 0\n",
      "Epoch: [65]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 5014.57951467374  loss: 1.5205 (1.5284)  acc1: 90.6250 (90.3801)  acc5: 99.2188 (98.9544)  time: 0.0307  data: 0.0003  max mem: 0\n",
      "Epoch: [65]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4433.505475085471  loss: 1.5310 (1.5280)  acc1: 89.8438 (90.4044)  acc5: 99.2188 (98.9592)  time: 0.0304  data: 0.0003  max mem: 0\n",
      "Epoch: [65] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:01:00  loss: 1.5300 (1.5300)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.7650  data: 0.7439  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.22, Acc@5 = 97.87, loss = 1.5622756360452386\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:14:51 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.42467948717949 train_acc5 90.42467948717949\n",
      "Epoch: [66]  [  0/390]  eta: 0:04:46  lr: 0.01  img/s: 2523.624890710639  loss: 1.5438 (1.5438)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.7355  data: 0.6847  max mem: 0\n",
      "Epoch: [66]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4509.7769937671155  loss: 1.5215 (1.5276)  acc1: 91.4062 (90.7797)  acc5: 98.4375 (98.9093)  time: 0.0292  data: 0.0003  max mem: 0\n",
      "Epoch: [66]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4487.011383201003  loss: 1.5279 (1.5283)  acc1: 89.0625 (90.3685)  acc5: 99.2188 (98.9700)  time: 0.0294  data: 0.0004  max mem: 0\n",
      "Epoch: [66]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4161.015873015873  loss: 1.5180 (1.5274)  acc1: 90.6250 (90.4641)  acc5: 99.2188 (98.9774)  time: 0.0301  data: 0.0003  max mem: 0\n",
      "Epoch: [66] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:45  loss: 1.5415 (1.5415)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.5776  data: 0.5505  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.22, Acc@5 = 97.78, loss = 1.5642863889283771\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:15:05 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.53084935897436 train_acc5 90.53084935897436\n",
      "Epoch: [67]  [  0/390]  eta: 0:04:29  lr: 0.01  img/s: 2596.401460524725  loss: 1.5373 (1.5373)  acc1: 90.6250 (90.6250)  acc5: 96.8750 (96.8750)  time: 0.6922  data: 0.6429  max mem: 0\n",
      "Epoch: [67]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4020.029442376955  loss: 1.5319 (1.5262)  acc1: 89.8438 (90.3775)  acc5: 98.4375 (98.9558)  time: 0.0287  data: 0.0004  max mem: 0\n",
      "Epoch: [67]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4894.25959487301  loss: 1.5266 (1.5260)  acc1: 89.8438 (90.3918)  acc5: 99.2188 (99.0711)  time: 0.0288  data: 0.0003  max mem: 0\n",
      "Epoch: [67]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 3981.2156528316436  loss: 1.5315 (1.5268)  acc1: 89.0625 (90.3499)  acc5: 99.2188 (99.0423)  time: 0.0296  data: 0.0003  max mem: 0\n",
      "Epoch: [67] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5536 (1.5536)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 0.6692  data: 0.6395  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.92, Acc@5 = 98.08, loss = 1.563140579416782\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:15:18 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.28044871794872 train_acc5 90.28044871794872\n",
      "Epoch: [68]  [  0/390]  eta: 0:04:08  lr: 0.01  img/s: 2164.950468378881  loss: 1.5179 (1.5179)  acc1: 92.9688 (92.9688)  acc5: 99.2188 (99.2188)  time: 0.6378  data: 0.5786  max mem: 0\n",
      "Epoch: [68]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4637.432404184195  loss: 1.5227 (1.5278)  acc1: 90.6250 (90.4703)  acc5: 99.2188 (98.9944)  time: 0.0279  data: 0.0003  max mem: 0\n",
      "Epoch: [68]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4138.467026910358  loss: 1.5212 (1.5284)  acc1: 91.4062 (90.4462)  acc5: 99.2188 (98.9078)  time: 0.0305  data: 0.0004  max mem: 0\n",
      "Epoch: [68]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4302.160508369994  loss: 1.5279 (1.5288)  acc1: 90.6250 (90.3473)  acc5: 98.4375 (98.8865)  time: 0.0292  data: 0.0004  max mem: 0\n",
      "Epoch: [68] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5207 (1.5207)  acc1: 89.8438 (89.8438)  acc5: 100.0000 (100.0000)  time: 0.5846  data: 0.5679  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.2, Acc@5 = 97.92, loss = 1.5627018152912961\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:15:32 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.40665064102564 train_acc5 90.40665064102564\n",
      "Epoch: [69]  [  0/390]  eta: 0:03:59  lr: 0.01  img/s: 3135.0492385312527  loss: 1.5379 (1.5379)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.6152  data: 0.5743  max mem: 0\n",
      "Epoch: [69]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 3975.7319253241703  loss: 1.5238 (1.5246)  acc1: 89.8438 (90.8648)  acc5: 99.2188 (99.1414)  time: 0.0303  data: 0.0004  max mem: 0\n",
      "Epoch: [69]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4360.089594178652  loss: 1.5297 (1.5261)  acc1: 89.0625 (90.6678)  acc5: 98.4375 (99.0438)  time: 0.0289  data: 0.0003  max mem: 0\n",
      "Epoch: [69]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 3874.5338760428394  loss: 1.5231 (1.5274)  acc1: 91.4062 (90.4822)  acc5: 98.4375 (98.9670)  time: 0.0310  data: 0.0004  max mem: 0\n",
      "Epoch: [69] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5450 (1.5450)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.5839  data: 0.5688  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.15, Acc@5 = 97.85, loss = 1.5616908631747282\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:15:45 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.48677884615384 train_acc5 90.48677884615384\n",
      "Epoch: [70]  [  0/390]  eta: 0:04:04  lr: 0.01  img/s: 2896.65004154482  loss: 1.5186 (1.5186)  acc1: 92.9688 (92.9688)  acc5: 99.2188 (99.2188)  time: 0.6265  data: 0.5822  max mem: 0\n",
      "Epoch: [70]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4284.889236515715  loss: 1.5361 (1.5284)  acc1: 89.0625 (90.6250)  acc5: 98.4375 (98.8320)  time: 0.0306  data: 0.0004  max mem: 0\n",
      "Epoch: [70]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 5376.5588959881425  loss: 1.5268 (1.5277)  acc1: 89.8438 (90.5784)  acc5: 98.4375 (98.9311)  time: 0.0287  data: 0.0003  max mem: 0\n",
      "Epoch: [70]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4488.849691892208  loss: 1.5167 (1.5269)  acc1: 89.8438 (90.6224)  acc5: 99.2188 (98.9722)  time: 0.0275  data: 0.0004  max mem: 0\n",
      "Epoch: [70] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:55  loss: 1.5593 (1.5593)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.7004  data: 0.6777  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.26, Acc@5 = 97.75, loss = 1.5630566213704362\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:15:58 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.56290064102564 train_acc5 90.56290064102564\n",
      "Epoch: [71]  [  0/390]  eta: 0:04:05  lr: 0.01  img/s: 2489.8592079694654  loss: 1.5127 (1.5127)  acc1: 93.7500 (93.7500)  acc5: 99.2188 (99.2188)  time: 0.6283  data: 0.5768  max mem: 0\n",
      "Epoch: [71]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4306.9578667008955  loss: 1.5317 (1.5261)  acc1: 89.0625 (90.8416)  acc5: 99.2188 (98.8243)  time: 0.0268  data: 0.0003  max mem: 0\n",
      "Epoch: [71]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 4363.385175552666  loss: 1.5234 (1.5257)  acc1: 92.9688 (90.8193)  acc5: 99.2188 (98.9078)  time: 0.0283  data: 0.0003  max mem: 0\n",
      "Epoch: [71]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4074.5504166603423  loss: 1.5230 (1.5265)  acc1: 90.6250 (90.6146)  acc5: 99.2188 (98.9073)  time: 0.0284  data: 0.0003  max mem: 0\n",
      "Epoch: [71] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5543 (1.5543)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.6584  data: 0.6402  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.75, Acc@5 = 98.05, loss = 1.5639386131793638\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:16:11 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.64102564102564 train_acc5 90.64102564102564\n",
      "Epoch: [72]  [  0/390]  eta: 0:03:59  lr: 0.01  img/s: 2965.400381120716  loss: 1.5392 (1.5392)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.6148  data: 0.5716  max mem: 0\n",
      "Epoch: [72]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4600.79108071745  loss: 1.5230 (1.5258)  acc1: 91.4062 (90.8803)  acc5: 98.4375 (98.9944)  time: 0.0264  data: 0.0003  max mem: 0\n",
      "Epoch: [72]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4144.7291536388975  loss: 1.5279 (1.5261)  acc1: 89.0625 (90.6755)  acc5: 99.2188 (99.0438)  time: 0.0298  data: 0.0003  max mem: 0\n",
      "Epoch: [72]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4080.124272316882  loss: 1.5201 (1.5261)  acc1: 90.6250 (90.6354)  acc5: 99.2188 (99.0552)  time: 0.0299  data: 0.0003  max mem: 0\n",
      "Epoch: [72] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:45  loss: 1.5386 (1.5386)  acc1: 86.7188 (86.7188)  acc5: 99.2188 (99.2188)  time: 0.5750  data: 0.5495  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.22, Acc@5 = 97.98, loss = 1.5630713565440117\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:16:25 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.64102564102564 train_acc5 90.64102564102564\n",
      "Epoch: [73]  [  0/390]  eta: 0:04:24  lr: 0.01  img/s: 2147.990573775411  loss: 1.5445 (1.5445)  acc1: 90.6250 (90.6250)  acc5: 97.6562 (97.6562)  time: 0.6772  data: 0.6176  max mem: 0\n",
      "Epoch: [73]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4093.1582229744668  loss: 1.5216 (1.5265)  acc1: 91.4062 (90.6869)  acc5: 99.2188 (99.0640)  time: 0.0291  data: 0.0004  max mem: 0\n",
      "Epoch: [73]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 3982.9878255966646  loss: 1.5172 (1.5251)  acc1: 91.4062 (90.8193)  acc5: 99.2188 (99.0633)  time: 0.0285  data: 0.0003  max mem: 0\n",
      "Epoch: [73]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 3988.6397622585437  loss: 1.5256 (1.5258)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.0500)  time: 0.0305  data: 0.0003  max mem: 0\n",
      "Epoch: [73] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5559 (1.5559)  acc1: 88.2812 (88.2812)  acc5: 97.6562 (97.6562)  time: 0.5867  data: 0.5703  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.83, Acc@5 = 97.87, loss = 1.564773245702816\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:16:38 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.5889423076923 train_acc5 90.5889423076923\n",
      "Epoch: [74]  [  0/390]  eta: 0:04:19  lr: 0.01  img/s: 2245.92712577706  loss: 1.5196 (1.5196)  acc1: 93.7500 (93.7500)  acc5: 99.2188 (99.2188)  time: 0.6650  data: 0.6080  max mem: 0\n",
      "Epoch: [74]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4532.46865344027  loss: 1.5171 (1.5283)  acc1: 91.4062 (90.3852)  acc5: 99.2188 (98.9016)  time: 0.0296  data: 0.0003  max mem: 0\n",
      "Epoch: [74]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4993.544148149526  loss: 1.5177 (1.5275)  acc1: 92.1875 (90.4268)  acc5: 99.2188 (99.0050)  time: 0.0289  data: 0.0004  max mem: 0\n",
      "Epoch: [74]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4374.584738235893  loss: 1.5248 (1.5273)  acc1: 89.8438 (90.4693)  acc5: 99.2188 (99.0345)  time: 0.0294  data: 0.0003  max mem: 0\n",
      "Epoch: [74] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5399 (1.5399)  acc1: 92.9688 (92.9688)  acc5: 97.6562 (97.6562)  time: 0.6576  data: 0.6275  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.05, Acc@5 = 97.98, loss = 1.5617183190357835\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:16:52 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.4667467948718 train_acc5 90.4667467948718\n",
      "Epoch: [75]  [  0/390]  eta: 0:04:20  lr: 0.01  img/s: 2185.0579037122357  loss: 1.5404 (1.5404)  acc1: 91.4062 (91.4062)  acc5: 97.6562 (97.6562)  time: 0.6688  data: 0.6101  max mem: 0\n",
      "Epoch: [75]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4790.795463266199  loss: 1.5241 (1.5272)  acc1: 90.6250 (90.6637)  acc5: 99.2188 (99.0254)  time: 0.0266  data: 0.0003  max mem: 0\n",
      "Epoch: [75]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4338.666343410834  loss: 1.5310 (1.5273)  acc1: 89.0625 (90.3957)  acc5: 99.2188 (99.0322)  time: 0.0317  data: 0.0005  max mem: 0\n",
      "Epoch: [75]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4480.832216333514  loss: 1.5300 (1.5270)  acc1: 89.0625 (90.4148)  acc5: 99.2188 (99.0085)  time: 0.0280  data: 0.0004  max mem: 0\n",
      "Epoch: [75] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5366 (1.5366)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.5925  data: 0.5760  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.11, Acc@5 = 97.79, loss = 1.563732545587081\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:17:05 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.37660256410257 train_acc5 90.37660256410257\n",
      "Epoch: [76]  [  0/390]  eta: 0:04:12  lr: 0.01  img/s: 2296.4693965720053  loss: 1.5325 (1.5325)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.6470  data: 0.5912  max mem: 0\n",
      "Epoch: [76]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 5123.205130162608  loss: 1.5289 (1.5269)  acc1: 90.6250 (90.6095)  acc5: 99.2188 (99.0563)  time: 0.0313  data: 0.0003  max mem: 0\n",
      "Epoch: [76]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 5334.143867737063  loss: 1.5252 (1.5273)  acc1: 89.8438 (90.5045)  acc5: 99.2188 (99.0283)  time: 0.0285  data: 0.0003  max mem: 0\n",
      "Epoch: [76]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 6091.115407306558  loss: 1.5195 (1.5271)  acc1: 89.8438 (90.5679)  acc5: 99.2188 (99.0163)  time: 0.0284  data: 0.0003  max mem: 0\n",
      "Epoch: [76] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5346 (1.5346)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.6247  data: 0.5949  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.32, Acc@5 = 97.91, loss = 1.5633862501458278\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:17:19 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.51482371794872 train_acc5 90.51482371794872\n",
      "Epoch: [77]  [  0/390]  eta: 0:04:05  lr: 0.01  img/s: 2256.1772428516197  loss: 1.5581 (1.5581)  acc1: 88.2812 (88.2812)  acc5: 97.6562 (97.6562)  time: 0.6303  data: 0.5735  max mem: 0\n",
      "Epoch: [77]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4131.587788491877  loss: 1.5207 (1.5282)  acc1: 90.6250 (90.4471)  acc5: 99.2188 (98.9403)  time: 0.0313  data: 0.0004  max mem: 0\n",
      "Epoch: [77]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4732.26658674823  loss: 1.5255 (1.5276)  acc1: 90.6250 (90.4618)  acc5: 98.4375 (98.9583)  time: 0.0296  data: 0.0003  max mem: 0\n",
      "Epoch: [77]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4526.8888664035885  loss: 1.5172 (1.5263)  acc1: 90.6250 (90.5964)  acc5: 99.2188 (99.0241)  time: 0.0295  data: 0.0003  max mem: 0\n",
      "Epoch: [77] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5292 (1.5292)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.6558  data: 0.6319  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.11, Acc@5 = 97.8, loss = 1.5631423630291903\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:17:32 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.5869391025641 train_acc5 90.5869391025641\n",
      "Epoch: [78]  [  0/390]  eta: 0:03:44  lr: 0.01  img/s: 3254.039203326343  loss: 1.5082 (1.5082)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 0.5745  data: 0.5351  max mem: 0\n",
      "Epoch: [78]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4948.757553970097  loss: 1.5247 (1.5277)  acc1: 89.8438 (90.4626)  acc5: 99.2188 (99.1646)  time: 0.0255  data: 0.0003  max mem: 0\n",
      "Epoch: [78]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 5218.41866251944  loss: 1.5198 (1.5264)  acc1: 90.6250 (90.6600)  acc5: 99.2188 (99.0749)  time: 0.0252  data: 0.0003  max mem: 0\n",
      "Epoch: [78]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4900.066737249462  loss: 1.5199 (1.5269)  acc1: 92.1875 (90.5913)  acc5: 99.2188 (99.0474)  time: 0.0268  data: 0.0004  max mem: 0\n",
      "Epoch: [78] Total time: 0:00:10\n",
      "Test:  [ 0/79]  eta: 0:00:38  loss: 1.5418 (1.5418)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.4822  data: 0.4584  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.33, Acc@5 = 97.87, loss = 1.5612281651436528\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:17:44 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.65705128205128 train_acc5 90.65705128205128\n",
      "Epoch: [79]  [  0/390]  eta: 0:04:20  lr: 0.01  img/s: 3621.706537504132  loss: 1.5581 (1.5581)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6670  data: 0.6316  max mem: 0\n",
      "Epoch: [79]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4165.244908567572  loss: 1.5241 (1.5252)  acc1: 89.8438 (90.5322)  acc5: 99.2188 (99.2265)  time: 0.0261  data: 0.0003  max mem: 0\n",
      "Epoch: [79]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 6783.983825722156  loss: 1.5231 (1.5260)  acc1: 90.6250 (90.5784)  acc5: 99.2188 (99.0516)  time: 0.0263  data: 0.0003  max mem: 0\n",
      "Epoch: [79]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4302.160508369994  loss: 1.5231 (1.5266)  acc1: 90.6250 (90.5160)  acc5: 98.4375 (99.0137)  time: 0.0297  data: 0.0003  max mem: 0\n",
      "Epoch: [79] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5351 (1.5351)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.6427  data: 0.6081  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.07, Acc@5 = 97.91, loss = 1.5644646883010864\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:17:57 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.49679487179488 train_acc5 90.49679487179488\n",
      "Epoch: [80]  [  0/390]  eta: 0:04:33  lr: 0.01  img/s: 2264.026685671633  loss: 1.5352 (1.5352)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.7006  data: 0.6439  max mem: 0\n",
      "Epoch: [80]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 5074.634075334373  loss: 1.5203 (1.5282)  acc1: 90.6250 (90.4239)  acc5: 99.2188 (98.8397)  time: 0.0295  data: 0.0003  max mem: 0\n",
      "Epoch: [80]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4350.69094563165  loss: 1.5210 (1.5270)  acc1: 90.6250 (90.5900)  acc5: 99.2188 (98.8689)  time: 0.0283  data: 0.0003  max mem: 0\n",
      "Epoch: [80]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 5890.8117669003805  loss: 1.5139 (1.5265)  acc1: 92.1875 (90.6328)  acc5: 99.2188 (98.9306)  time: 0.0257  data: 0.0003  max mem: 0\n",
      "Epoch: [80] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:34  loss: 1.5350 (1.5350)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.4320  data: 0.4098  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.41, Acc@5 = 97.89, loss = 1.5626263829726208\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:18:10 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.56891025641026 train_acc5 90.56891025641026\n",
      "Epoch: [81]  [  0/390]  eta: 0:04:07  lr: 0.01  img/s: 3746.2731459513775  loss: 1.5594 (1.5594)  acc1: 84.3750 (84.3750)  acc5: 98.4375 (98.4375)  time: 0.6352  data: 0.6010  max mem: 0\n",
      "Epoch: [81]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4035.955796785494  loss: 1.5194 (1.5247)  acc1: 90.6250 (90.8106)  acc5: 99.2188 (99.1414)  time: 0.0300  data: 0.0003  max mem: 0\n",
      "Epoch: [81]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4519.229542833573  loss: 1.5316 (1.5250)  acc1: 89.0625 (90.7183)  acc5: 99.2188 (99.1566)  time: 0.0282  data: 0.0003  max mem: 0\n",
      "Epoch: [81]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4755.278228520815  loss: 1.5259 (1.5251)  acc1: 90.6250 (90.7262)  acc5: 99.2188 (99.0864)  time: 0.0285  data: 0.0003  max mem: 0\n",
      "Epoch: [81] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5399 (1.5399)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.6550  data: 0.6316  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.92, Acc@5 = 98.01, loss = 1.5632485392727429\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:18:24 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.63902243589743 train_acc5 90.63902243589743\n",
      "Epoch: [82]  [  0/390]  eta: 0:04:42  lr: 0.01  img/s: 2367.604581115467  loss: 1.5406 (1.5406)  acc1: 89.0625 (89.0625)  acc5: 97.6562 (97.6562)  time: 0.7255  data: 0.6713  max mem: 0\n",
      "Epoch: [82]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 5002.4777257014  loss: 1.5289 (1.5294)  acc1: 89.8438 (90.0449)  acc5: 99.2188 (99.0099)  time: 0.0293  data: 0.0003  max mem: 0\n",
      "Epoch: [82]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4393.486845012561  loss: 1.5172 (1.5262)  acc1: 91.4062 (90.5239)  acc5: 99.2188 (99.0633)  time: 0.0299  data: 0.0004  max mem: 0\n",
      "Epoch: [82]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4158.244225853923  loss: 1.5300 (1.5270)  acc1: 89.0625 (90.4589)  acc5: 98.4375 (98.9852)  time: 0.0272  data: 0.0003  max mem: 0\n",
      "Epoch: [82] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5532 (1.5532)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6235  data: 0.5914  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.53, Acc@5 = 98.0, loss = 1.561598625364183\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:18:37 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.44871794871794 train_acc5 90.44871794871794\n",
      "Epoch: [83]  [  0/390]  eta: 0:04:50  lr: 0.01  img/s: 2702.012209790984  loss: 1.5243 (1.5243)  acc1: 92.1875 (92.1875)  acc5: 98.4375 (98.4375)  time: 0.7447  data: 0.6972  max mem: 0\n",
      "Epoch: [83]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4442.934796461349  loss: 1.5295 (1.5283)  acc1: 89.8438 (90.5012)  acc5: 99.2188 (99.0640)  time: 0.0299  data: 0.0004  max mem: 0\n",
      "Epoch: [83]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 3927.854319849579  loss: 1.5273 (1.5269)  acc1: 89.8438 (90.5045)  acc5: 99.2188 (99.1021)  time: 0.0292  data: 0.0003  max mem: 0\n",
      "Epoch: [83]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4174.605082268047  loss: 1.5258 (1.5268)  acc1: 90.6250 (90.5160)  acc5: 99.2188 (99.0449)  time: 0.0297  data: 0.0003  max mem: 0\n",
      "Epoch: [83] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5251 (1.5251)  acc1: 89.8438 (89.8438)  acc5: 100.0000 (100.0000)  time: 0.6296  data: 0.6002  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.31, Acc@5 = 97.93, loss = 1.5640920159182972\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:18:51 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.5448717948718 train_acc5 90.5448717948718\n",
      "Epoch: [84]  [  0/390]  eta: 0:04:16  lr: 0.01  img/s: 2435.4182804625234  loss: 1.5097 (1.5097)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.6566  data: 0.6040  max mem: 0\n",
      "Epoch: [84]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4887.887615284467  loss: 1.5226 (1.5264)  acc1: 91.4062 (90.5322)  acc5: 98.4375 (99.0563)  time: 0.0277  data: 0.0003  max mem: 0\n",
      "Epoch: [84]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4198.5025025025025  loss: 1.5273 (1.5279)  acc1: 89.8438 (90.3646)  acc5: 98.4375 (98.9817)  time: 0.0298  data: 0.0003  max mem: 0\n",
      "Epoch: [84]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4082.8237727670253  loss: 1.5262 (1.5281)  acc1: 89.8438 (90.2824)  acc5: 98.4375 (98.9618)  time: 0.0306  data: 0.0004  max mem: 0\n",
      "Epoch: [84] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:37  loss: 1.5410 (1.5410)  acc1: 86.7188 (86.7188)  acc5: 99.2188 (99.2188)  time: 0.4793  data: 0.4556  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.2, Acc@5 = 97.85, loss = 1.5617686027212987\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:19:04 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.38461538461539 train_acc5 90.38461538461539\n",
      "Epoch: [85]  [  0/390]  eta: 0:03:25  lr: 0.01  img/s: 2742.481454426571  loss: 1.5462 (1.5462)  acc1: 87.5000 (87.5000)  acc5: 97.6562 (97.6562)  time: 0.5257  data: 0.4790  max mem: 0\n",
      "Epoch: [85]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4479.785319125856  loss: 1.5235 (1.5238)  acc1: 91.4062 (91.1974)  acc5: 99.2188 (99.0331)  time: 0.0293  data: 0.0003  max mem: 0\n",
      "Epoch: [85]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 3873.834950825823  loss: 1.5209 (1.5250)  acc1: 90.6250 (91.0292)  acc5: 99.2188 (99.0205)  time: 0.0291  data: 0.0003  max mem: 0\n",
      "Epoch: [85]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4014.9186876958397  loss: 1.5326 (1.5259)  acc1: 89.8438 (90.8093)  acc5: 99.2188 (99.0033)  time: 0.0302  data: 0.0003  max mem: 0\n",
      "Epoch: [85] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5492 (1.5492)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.6323  data: 0.6018  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.0, Acc@5 = 97.96, loss = 1.5628840259358854\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:19:17 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.61498397435898 train_acc5 90.61498397435898\n",
      "Epoch: [86]  [  0/390]  eta: 0:04:55  lr: 0.01  img/s: 2403.2682996705344  loss: 1.5090 (1.5090)  acc1: 92.9688 (92.9688)  acc5: 100.0000 (100.0000)  time: 0.7564  data: 0.7031  max mem: 0\n",
      "Epoch: [86]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 3938.1980575687335  loss: 1.5288 (1.5284)  acc1: 89.8438 (90.3543)  acc5: 99.2188 (98.9712)  time: 0.0297  data: 0.0004  max mem: 0\n",
      "Epoch: [86]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4512.164863888119  loss: 1.5335 (1.5285)  acc1: 89.0625 (90.3840)  acc5: 98.4375 (98.8923)  time: 0.0290  data: 0.0003  max mem: 0\n",
      "Epoch: [86]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4182.736120416972  loss: 1.5309 (1.5279)  acc1: 89.8438 (90.5056)  acc5: 99.2188 (98.9410)  time: 0.0294  data: 0.0003  max mem: 0\n",
      "Epoch: [86] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5536 (1.5536)  acc1: 86.7188 (86.7188)  acc5: 98.4375 (98.4375)  time: 0.6131  data: 0.5924  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.41, Acc@5 = 97.94, loss = 1.56321055376077\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:19:31 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.5849358974359 train_acc5 90.5849358974359\n",
      "Epoch: [87]  [  0/390]  eta: 0:04:15  lr: 0.01  img/s: 2569.620983104389  loss: 1.5374 (1.5374)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.6560  data: 0.6061  max mem: 0\n",
      "Epoch: [87]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 3928.4291432209156  loss: 1.5253 (1.5273)  acc1: 90.6250 (90.5244)  acc5: 99.2188 (98.9403)  time: 0.0287  data: 0.0004  max mem: 0\n",
      "Epoch: [87]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 3966.3328235702625  loss: 1.5248 (1.5283)  acc1: 90.6250 (90.4462)  acc5: 99.2188 (98.9350)  time: 0.0278  data: 0.0003  max mem: 0\n",
      "Epoch: [87]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4575.695357578134  loss: 1.5224 (1.5278)  acc1: 91.4062 (90.4848)  acc5: 99.2188 (98.9592)  time: 0.0314  data: 0.0004  max mem: 0\n",
      "Epoch: [87] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:58  loss: 1.5388 (1.5388)  acc1: 91.4062 (91.4062)  acc5: 97.6562 (97.6562)  time: 0.7415  data: 0.7121  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.06, Acc@5 = 97.86, loss = 1.5650314816945716\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:19:44 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.51883012820512 train_acc5 90.51883012820512\n",
      "Epoch: [88]  [  0/390]  eta: 0:04:16  lr: 0.01  img/s: 2715.199223175271  loss: 1.5150 (1.5150)  acc1: 94.5312 (94.5312)  acc5: 99.2188 (99.2188)  time: 0.6582  data: 0.6110  max mem: 0\n",
      "Epoch: [88]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4489.750637664434  loss: 1.5230 (1.5282)  acc1: 89.8438 (90.4935)  acc5: 98.4375 (99.0486)  time: 0.0293  data: 0.0004  max mem: 0\n",
      "Epoch: [88]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4441.133894743808  loss: 1.5232 (1.5257)  acc1: 90.6250 (90.7688)  acc5: 99.2188 (99.1099)  time: 0.0284  data: 0.0003  max mem: 0\n",
      "Epoch: [88]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4593.626517673029  loss: 1.5245 (1.5266)  acc1: 90.6250 (90.6276)  acc5: 98.4375 (99.0319)  time: 0.0287  data: 0.0003  max mem: 0\n",
      "Epoch: [88] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5370 (1.5370)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.5930  data: 0.5779  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.06, Acc@5 = 97.82, loss = 1.5644185724137705\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:19:58 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.53886217948718 train_acc5 90.53886217948718\n",
      "Epoch: [89]  [  0/390]  eta: 0:04:23  lr: 0.01  img/s: 2322.949999134634  loss: 1.5307 (1.5307)  acc1: 92.1875 (92.1875)  acc5: 100.0000 (100.0000)  time: 0.6767  data: 0.6215  max mem: 0\n",
      "Epoch: [89]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4671.164173910017  loss: 1.5194 (1.5267)  acc1: 92.1875 (90.4471)  acc5: 100.0000 (99.1027)  time: 0.0295  data: 0.0003  max mem: 0\n",
      "Epoch: [89]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4352.736819062598  loss: 1.5262 (1.5260)  acc1: 90.6250 (90.5667)  acc5: 98.4375 (99.0244)  time: 0.0283  data: 0.0004  max mem: 0\n",
      "Epoch: [89]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4480.720025371814  loss: 1.5248 (1.5270)  acc1: 91.4062 (90.5393)  acc5: 99.2188 (98.9903)  time: 0.0311  data: 0.0004  max mem: 0\n",
      "Epoch: [89] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5503 (1.5503)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 0.5840  data: 0.5572  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.06, Acc@5 = 97.88, loss = 1.5631952904447723\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:20:11 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.5488782051282 train_acc5 90.5488782051282\n",
      "Epoch: [90]  [  0/390]  eta: 0:04:32  lr: 0.01  img/s: 2209.1907644321177  loss: 1.5360 (1.5360)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.6984  data: 0.6404  max mem: 0\n",
      "Epoch: [90]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 5397.749009672036  loss: 1.5297 (1.5285)  acc1: 89.8438 (90.4548)  acc5: 99.2188 (98.9480)  time: 0.0288  data: 0.0003  max mem: 0\n",
      "Epoch: [90]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4598.111597392921  loss: 1.5190 (1.5263)  acc1: 90.6250 (90.7027)  acc5: 99.2188 (99.0594)  time: 0.0298  data: 0.0003  max mem: 0\n",
      "Epoch: [90]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 3942.0729275277185  loss: 1.5155 (1.5269)  acc1: 91.4062 (90.4926)  acc5: 99.2188 (99.0137)  time: 0.0308  data: 0.0003  max mem: 0\n",
      "Epoch: [90] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5404 (1.5404)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.6512  data: 0.6229  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.48, Acc@5 = 97.94, loss = 1.563093917279304\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:20:25 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.40865384615384 train_acc5 90.40865384615384\n",
      "Epoch: [91]  [  0/390]  eta: 0:04:18  lr: 0.01  img/s: 2368.387926699076  loss: 1.5388 (1.5388)  acc1: 91.4062 (91.4062)  acc5: 99.2188 (99.2188)  time: 0.6635  data: 0.6094  max mem: 0\n",
      "Epoch: [91]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 5826.749932167703  loss: 1.5210 (1.5248)  acc1: 91.4062 (91.1510)  acc5: 98.4375 (99.0099)  time: 0.0284  data: 0.0003  max mem: 0\n",
      "Epoch: [91]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 4650.326657889266  loss: 1.5254 (1.5261)  acc1: 89.8438 (90.7766)  acc5: 98.4375 (98.9389)  time: 0.0283  data: 0.0003  max mem: 0\n",
      "Epoch: [91]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4274.415904331971  loss: 1.5310 (1.5271)  acc1: 90.6250 (90.6587)  acc5: 99.2188 (98.9358)  time: 0.0282  data: 0.0003  max mem: 0\n",
      "Epoch: [91] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:54  loss: 1.5256 (1.5256)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.6895  data: 0.6621  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.29, Acc@5 = 97.88, loss = 1.5631410185294816\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:20:38 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.59094551282051 train_acc5 90.59094551282051\n",
      "Epoch: [92]  [  0/390]  eta: 0:04:06  lr: 0.01  img/s: 2745.9039981178107  loss: 1.4988 (1.4988)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.6317  data: 0.5851  max mem: 0\n",
      "Epoch: [92]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4443.412831887705  loss: 1.5241 (1.5292)  acc1: 89.8438 (90.0526)  acc5: 99.2188 (98.9480)  time: 0.0289  data: 0.0004  max mem: 0\n",
      "Epoch: [92]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4764.816301897509  loss: 1.5196 (1.5280)  acc1: 92.1875 (90.3568)  acc5: 99.2188 (98.9661)  time: 0.0278  data: 0.0003  max mem: 0\n",
      "Epoch: [92]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4924.879022492936  loss: 1.5246 (1.5284)  acc1: 90.6250 (90.3291)  acc5: 99.2188 (98.9618)  time: 0.0299  data: 0.0003  max mem: 0\n",
      "Epoch: [92] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5494 (1.5494)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.6267  data: 0.6022  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.79, Acc@5 = 97.94, loss = 1.5647909611086301\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:20:51 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.37259615384616 train_acc5 90.37259615384616\n",
      "Epoch: [93]  [  0/390]  eta: 0:04:12  lr: 0.01  img/s: 2798.54936691705  loss: 1.5369 (1.5369)  acc1: 86.7188 (86.7188)  acc5: 98.4375 (98.4375)  time: 0.6476  data: 0.6018  max mem: 0\n",
      "Epoch: [93]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4875.5032147916745  loss: 1.5289 (1.5282)  acc1: 90.6250 (90.4084)  acc5: 99.2188 (98.9635)  time: 0.0288  data: 0.0004  max mem: 0\n",
      "Epoch: [93]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4074.0556997374374  loss: 1.5257 (1.5288)  acc1: 90.6250 (90.2324)  acc5: 99.2188 (98.9428)  time: 0.0313  data: 0.0004  max mem: 0\n",
      "Epoch: [93]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 5991.394779425715  loss: 1.5323 (1.5276)  acc1: 89.8438 (90.3784)  acc5: 99.2188 (98.9852)  time: 0.0297  data: 0.0004  max mem: 0\n",
      "Epoch: [93] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5371 (1.5371)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6391  data: 0.6097  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.59, Acc@5 = 97.86, loss = 1.5607391565660886\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:21:05 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.33653846153847 train_acc5 90.33653846153847\n",
      "Epoch: [94]  [  0/390]  eta: 0:04:19  lr: 0.01  img/s: 2790.0123268165426  loss: 1.5019 (1.5019)  acc1: 93.7500 (93.7500)  acc5: 99.2188 (99.2188)  time: 0.6666  data: 0.6206  max mem: 0\n",
      "Epoch: [94]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4996.937006701415  loss: 1.5220 (1.5267)  acc1: 90.6250 (90.6869)  acc5: 98.4375 (98.9944)  time: 0.0275  data: 0.0003  max mem: 0\n",
      "Epoch: [94]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4725.019687915299  loss: 1.5145 (1.5271)  acc1: 91.4062 (90.6405)  acc5: 99.2188 (98.9428)  time: 0.0298  data: 0.0003  max mem: 0\n",
      "Epoch: [94]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 3907.243690140025  loss: 1.5196 (1.5269)  acc1: 89.8438 (90.5523)  acc5: 99.2188 (99.0085)  time: 0.0282  data: 0.0003  max mem: 0\n",
      "Epoch: [94] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5385 (1.5385)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.6205  data: 0.6028  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.5, Acc@5 = 97.93, loss = 1.5624213535574418\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:21:18 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.53485576923077 train_acc5 90.53485576923077\n",
      "Epoch: [95]  [  0/390]  eta: 0:04:30  lr: 0.01  img/s: 2590.000829771427  loss: 1.5405 (1.5405)  acc1: 90.6250 (90.6250)  acc5: 97.6562 (97.6562)  time: 0.6925  data: 0.6430  max mem: 0\n",
      "Epoch: [95]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4137.9885619151855  loss: 1.5272 (1.5270)  acc1: 89.8438 (90.4703)  acc5: 99.2188 (98.9171)  time: 0.0295  data: 0.0004  max mem: 0\n",
      "Epoch: [95]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 5080.540843364372  loss: 1.5200 (1.5270)  acc1: 92.1875 (90.4890)  acc5: 99.2188 (98.9467)  time: 0.0297  data: 0.0003  max mem: 0\n",
      "Epoch: [95]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4127.712389958867  loss: 1.5178 (1.5266)  acc1: 91.4062 (90.5108)  acc5: 99.2188 (98.9774)  time: 0.0296  data: 0.0004  max mem: 0\n",
      "Epoch: [95] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5239 (1.5239)  acc1: 89.8438 (89.8438)  acc5: 100.0000 (100.0000)  time: 0.6345  data: 0.6055  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.03, Acc@5 = 98.0, loss = 1.5631057280528395\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:21:32 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.43669871794872 train_acc5 90.43669871794872\n",
      "Epoch: [96]  [  0/390]  eta: 0:04:26  lr: 0.01  img/s: 2541.9783524777226  loss: 1.5076 (1.5076)  acc1: 92.9688 (92.9688)  acc5: 99.2188 (99.2188)  time: 0.6824  data: 0.6319  max mem: 0\n",
      "Epoch: [96]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4397.3012916595  loss: 1.5242 (1.5255)  acc1: 90.6250 (90.7565)  acc5: 98.4375 (99.0408)  time: 0.0307  data: 0.0004  max mem: 0\n",
      "Epoch: [96]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 3879.517523448904  loss: 1.5243 (1.5262)  acc1: 89.8438 (90.6328)  acc5: 99.2188 (99.0089)  time: 0.0301  data: 0.0003  max mem: 0\n",
      "Epoch: [96]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4349.527772376693  loss: 1.5339 (1.5275)  acc1: 89.8438 (90.4874)  acc5: 99.2188 (98.9826)  time: 0.0303  data: 0.0004  max mem: 0\n",
      "Epoch: [96] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5546 (1.5546)  acc1: 88.2812 (88.2812)  acc5: 100.0000 (100.0000)  time: 0.6142  data: 0.5909  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.96, Acc@5 = 97.81, loss = 1.5634998309461376\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:21:45 max_test_acc1 86.6 test_acc5_at_max_test_acc1 97.81 train_acc1 90.55689102564102 train_acc5 90.55689102564102\n",
      "Epoch: [97]  [  0/390]  eta: 0:04:30  lr: 0.01  img/s: 2159.02145867516  loss: 1.5023 (1.5023)  acc1: 92.9688 (92.9688)  acc5: 100.0000 (100.0000)  time: 0.6939  data: 0.6346  max mem: 0\n",
      "Epoch: [97]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 3977.086709484336  loss: 1.5232 (1.5242)  acc1: 91.4062 (90.8957)  acc5: 99.2188 (99.0640)  time: 0.0307  data: 0.0004  max mem: 0\n",
      "Epoch: [97]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4337.264297428523  loss: 1.5245 (1.5261)  acc1: 90.6250 (90.5628)  acc5: 99.2188 (99.0711)  time: 0.0313  data: 0.0004  max mem: 0\n",
      "Epoch: [97]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4116.350610317119  loss: 1.5280 (1.5264)  acc1: 90.6250 (90.4900)  acc5: 98.4375 (98.9903)  time: 0.0295  data: 0.0003  max mem: 0\n",
      "Epoch: [97] Total time: 0:00:12\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5535 (1.5535)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 0.6070  data: 0.5849  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.61, Acc@5 = 97.81, loss = 1.5619039942946615\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:21:59 max_test_acc1 86.61 test_acc5_at_max_test_acc1 97.81 train_acc1 90.45472756410257 train_acc5 90.45472756410257\n",
      "Epoch: [98]  [  0/390]  eta: 0:04:32  lr: 0.01  img/s: 2769.1172386758685  loss: 1.5360 (1.5360)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 0.6989  data: 0.6527  max mem: 0\n",
      "Epoch: [98]  [100/390]  eta: 0:00:09  lr: 0.01  img/s: 4753.29944133089  loss: 1.5208 (1.5298)  acc1: 89.8438 (90.1532)  acc5: 99.2188 (98.8397)  time: 0.0256  data: 0.0003  max mem: 0\n",
      "Epoch: [98]  [200/390]  eta: 0:00:05  lr: 0.01  img/s: 4285.23125059864  loss: 1.5285 (1.5288)  acc1: 90.6250 (90.3724)  acc5: 99.2188 (98.9156)  time: 0.0297  data: 0.0004  max mem: 0\n",
      "Epoch: [98]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 4257.906478015354  loss: 1.5215 (1.5282)  acc1: 91.4062 (90.3499)  acc5: 99.2188 (99.0007)  time: 0.0292  data: 0.0004  max mem: 0\n",
      "Epoch: [98] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:43  loss: 1.5370 (1.5370)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.5508  data: 0.5321  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.25, Acc@5 = 98.07, loss = 1.5610051381437084\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:22:12 max_test_acc1 86.61 test_acc5_at_max_test_acc1 97.81 train_acc1 90.4326923076923 train_acc5 90.4326923076923\n",
      "Epoch: [99]  [  0/390]  eta: 0:04:23  lr: 0.01  img/s: 2379.852440267742  loss: 1.5175 (1.5175)  acc1: 88.2812 (88.2812)  acc5: 100.0000 (100.0000)  time: 0.6760  data: 0.6222  max mem: 0\n",
      "Epoch: [99]  [100/390]  eta: 0:00:10  lr: 0.01  img/s: 4234.53205451792  loss: 1.5260 (1.5272)  acc1: 89.8438 (90.4780)  acc5: 98.4375 (98.8707)  time: 0.0300  data: 0.0004  max mem: 0\n",
      "Epoch: [99]  [200/390]  eta: 0:00:06  lr: 0.01  img/s: 4330.896412639256  loss: 1.5195 (1.5258)  acc1: 91.4062 (90.6250)  acc5: 99.2188 (99.0011)  time: 0.0311  data: 0.0004  max mem: 0\n",
      "Epoch: [99]  [300/390]  eta: 0:00:02  lr: 0.01  img/s: 6078.082079498239  loss: 1.5255 (1.5270)  acc1: 90.6250 (90.5238)  acc5: 99.2188 (98.9903)  time: 0.0230  data: 0.0002  max mem: 0\n",
      "Epoch: [99] Total time: 0:00:11\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5362 (1.5362)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.5840  data: 0.5594  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.84, Acc@5 = 97.68, loss = 1.5628366832491718\n",
      "Namespace(batch_size=128, cache_dataset=True, data_path='../data', device='cuda:0', distributed=False, epochs=100, lr=0.01, output_dir='./logs', print_freq=100, resume='', start_epoch=0, tb=True, workers=16)\n",
      "./logs/b_128_lr0.01_2025_3_4_14__42\n",
      "Training time 0:22:25 max_test_acc1 86.61 test_acc5_at_max_test_acc1 97.81 train_acc1 90.40064102564102 train_acc5 90.40064102564102\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    save_max = False\n",
    "\n",
    "    train_loss, train_acc1, train_acc5 = train_one_epoch(net, criterion,data_loader, device, epoch, args.print_freq,lr=args.lr)\n",
    "\n",
    "    if utils.is_main_process():\n",
    "        train_tb_writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        train_tb_writer.add_scalar('train_acc1', train_acc1, epoch)\n",
    "        train_tb_writer.add_scalar('train_acc5', train_acc5, epoch)\n",
    "\n",
    "    test_loss, test_acc1, test_acc5 = evaluate(net, criterion, data_loader_test, device=device, header='Test:')\n",
    "    if te_tb_writer is not None:\n",
    "        if utils.is_main_process():\n",
    "            te_tb_writer.add_scalar('test_loss', test_loss, epoch)\n",
    "            te_tb_writer.add_scalar('test_acc1', test_acc1, epoch)\n",
    "            te_tb_writer.add_scalar('test_acc5', test_acc5, epoch)\n",
    "\n",
    "\n",
    "    if max_test_acc1 < test_acc1:\n",
    "        max_test_acc1 = test_acc1\n",
    "        test_acc5_at_max_test_acc1 = test_acc5\n",
    "        save_max = True\n",
    "\n",
    "    if output_dir:\n",
    "\n",
    "        checkpoint = {\n",
    "            'model': net.state_dict(),\n",
    "            'pre-train_epoch': epoch,\n",
    "            'args': args,\n",
    "            'max_test_acc1': max_test_acc1,\n",
    "            'test_acc5_at_max_test_acc1': test_acc5_at_max_test_acc1,\n",
    "        }\n",
    "\n",
    "        utils.save_on_master(\n",
    "            checkpoint,\n",
    "            os.path.join(output_dir, 'checkpoint_latest.pth'))\n",
    "        save_flag = False\n",
    "\n",
    "        if epoch % 64 == 0 or epoch == args.epochs - 1:\n",
    "            save_flag = True\n",
    "\n",
    "\n",
    "        if save_flag:\n",
    "            utils.save_on_master(\n",
    "                checkpoint,\n",
    "                os.path.join(output_dir, f'checkpoint_{epoch}.pth'))\n",
    "\n",
    "        if save_max:\n",
    "            utils.save_on_master(\n",
    "                checkpoint,\n",
    "                os.path.join(output_dir,f'train_maxacc1_{max_test_acc1}_checkpoint_max_test_acc1_all_pretrain.pth'))\n",
    "            torch.save(net,os.path.join(output_dir,f'train_maxacc1_{max_test_acc1}_checkpoint_max_test_acc1_all_pretrain.pth'))\n",
    "            torch.save(net.state_dict(),os.path.join(output_dir,f'train_maxacc1_{max_test_acc1}_checkpoint_max_test_acc1_state_pretrain.pth'))\n",
    "    print(args)\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print(output_dir)\n",
    "\n",
    "    print('Training time {}'.format(total_time_str), 'max_test_acc1', max_test_acc1,\n",
    "            'test_acc5_at_max_test_acc1', test_acc5_at_max_test_acc1,'train_acc1', train_acc1,\n",
    "            'train_acc5', train_acc1)\n",
    "\n",
    "    Train_logs= {\n",
    "        'Epoch:': epoch,\n",
    "        'max_test_acc1 ': max_test_acc1 ,\n",
    "        'test_acc5_at_max_test_acc1 ': test_acc5_at_max_test_acc1,\n",
    "        'train_acc1 ': train_acc1,\n",
    "        'train_acc5 ': train_acc1,\n",
    "        'args': args\n",
    "    }\n",
    "    with open(output_dir + '_logs/args.txt', 'a', encoding='utf-8') as args_txt:\n",
    "        args_txt.write('\\n')\n",
    "        args_txt.write(str(Train_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(output_dir,f'train_maxacc1_{max_test_acc1}_checkpoint_max_test_acc1_state_pretrain.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5607 (1.5607)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.6172  data: 0.5986  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.75, Acc@5 = 97.99, loss = 1.5639841903614093\n"
     ]
    }
   ],
   "source": [
    "net = NetworkA().to(device)\n",
    "functional.set_step_mode(net,step_mode='m')\n",
    "functional.set_backend(net, backend='cupy')\n",
    "weights = torch.load(model_dir)\n",
    "net.load_state_dict(weights)\n",
    "net.set_T(8)\n",
    "test_loss, test_acc1, test_acc5 = evaluate(net, criterion, data_loader_test, device=device, header='Test:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [ 0/79]  eta: 0:01:58  loss: 1.5917 (1.5917)  acc1: 81.2500 (81.2500)  acc5: 92.9688 (92.9688)  time: 1.5055  data: 0.6701  max mem: 0\n",
      "Test: Total time: 0:00:02\n",
      " * Acc@1 = 82.4, Acc@5 = 95.21, loss = 1.578768888606301\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5546 (1.5546)  acc1: 84.3750 (84.3750)  acc5: 95.3125 (95.3125)  time: 0.6296  data: 0.6120  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 82.79, Acc@5 = 95.45, loss = 1.5743959915788868\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5808 (1.5808)  acc1: 82.0312 (82.0312)  acc5: 94.5312 (94.5312)  time: 0.6468  data: 0.6261  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 83.1, Acc@5 = 95.61, loss = 1.5760362510439716\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5509 (1.5509)  acc1: 82.0312 (82.0312)  acc5: 96.8750 (96.8750)  time: 0.6346  data: 0.6212  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 82.33, Acc@5 = 95.39, loss = 1.5769244779514362\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5732 (1.5732)  acc1: 80.4688 (80.4688)  acc5: 94.5312 (94.5312)  time: 0.6036  data: 0.5904  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 82.6, Acc@5 = 95.35, loss = 1.576385129856158\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5589 (1.5589)  acc1: 84.3750 (84.3750)  acc5: 96.0938 (96.0938)  time: 0.6703  data: 0.6483  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 82.44, Acc@5 = 95.4, loss = 1.5757989672165882\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5821 (1.5821)  acc1: 77.3438 (77.3438)  acc5: 95.3125 (95.3125)  time: 0.6417  data: 0.6156  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 82.47, Acc@5 = 95.52, loss = 1.5760516154615185\n",
      "Test:  [ 0/79]  eta: 0:00:58  loss: 1.5605 (1.5605)  acc1: 83.5938 (83.5938)  acc5: 95.3125 (95.3125)  time: 0.7348  data: 0.7191  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 82.8, Acc@5 = 95.43, loss = 1.5746730188780194\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5593 (1.5593)  acc1: 84.3750 (84.3750)  acc5: 96.0938 (96.0938)  time: 0.6340  data: 0.6108  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 82.91, Acc@5 = 95.83, loss = 1.5719971852966501\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5377 (1.5377)  acc1: 86.7188 (86.7188)  acc5: 97.6562 (97.6562)  time: 0.6465  data: 0.6236  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 82.9, Acc@5 = 95.73, loss = 1.574043471601945\n",
      "Test:  [ 0/79]  eta: 0:00:55  loss: 1.5433 (1.5433)  acc1: 87.5000 (87.5000)  acc5: 97.6562 (97.6562)  time: 0.7009  data: 0.6501  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.71, Acc@5 = 96.7, loss = 1.5691277286674403\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5652 (1.5652)  acc1: 84.3750 (84.3750)  acc5: 96.0938 (96.0938)  time: 0.5938  data: 0.5676  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.25, Acc@5 = 96.48, loss = 1.5715357487714743\n",
      "Test:  [ 0/79]  eta: 0:00:34  loss: 1.5340 (1.5340)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.4376  data: 0.4221  max mem: 0\n",
      "Test: Total time: 0:00:00\n",
      " * Acc@1 = 85.58, Acc@5 = 96.59, loss = 1.5704778960988492\n",
      "Test:  [ 0/79]  eta: 0:00:36  loss: 1.5467 (1.5467)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.4619  data: 0.4447  max mem: 0\n",
      "Test: Total time: 0:00:00\n",
      " * Acc@1 = 85.58, Acc@5 = 96.8, loss = 1.568397174907636\n",
      "Test:  [ 0/79]  eta: 0:00:36  loss: 1.5370 (1.5370)  acc1: 89.0625 (89.0625)  acc5: 97.6562 (97.6562)  time: 0.4561  data: 0.4398  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.59, Acc@5 = 96.84, loss = 1.5670649597916422\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5489 (1.5489)  acc1: 89.0625 (89.0625)  acc5: 96.0938 (96.0938)  time: 0.6692  data: 0.6468  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.31, Acc@5 = 96.67, loss = 1.5672172941739047\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5603 (1.5603)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.6592  data: 0.6455  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.07, Acc@5 = 96.74, loss = 1.5702642670160607\n",
      "Test:  [ 0/79]  eta: 0:00:55  loss: 1.5404 (1.5404)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.7045  data: 0.6780  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.89, Acc@5 = 96.71, loss = 1.567394814913786\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5516 (1.5516)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.6504  data: 0.6297  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.98, Acc@5 = 96.64, loss = 1.5681271522859983\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5737 (1.5737)  acc1: 82.0312 (82.0312)  acc5: 96.0938 (96.0938)  time: 0.6124  data: 0.5923  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.59, Acc@5 = 96.54, loss = 1.5686665396147137\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5484 (1.5484)  acc1: 89.8438 (89.8438)  acc5: 97.6562 (97.6562)  time: 0.6342  data: 0.5795  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.94, Acc@5 = 97.29, loss = 1.5664077589783487\n",
      "Test:  [ 0/79]  eta: 0:00:54  loss: 1.5416 (1.5416)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.6857  data: 0.6633  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.71, Acc@5 = 97.02, loss = 1.5676146941848947\n",
      "Test:  [ 0/79]  eta: 0:00:46  loss: 1.5411 (1.5411)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.5862  data: 0.5640  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.66, Acc@5 = 96.98, loss = 1.5682466226288034\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5531 (1.5531)  acc1: 85.9375 (85.9375)  acc5: 96.8750 (96.8750)  time: 0.6475  data: 0.6229  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.73, Acc@5 = 97.14, loss = 1.5661834010595008\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5421 (1.5421)  acc1: 86.7188 (86.7188)  acc5: 98.4375 (98.4375)  time: 0.6455  data: 0.6254  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.07, Acc@5 = 97.24, loss = 1.5651869426799725\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5467 (1.5467)  acc1: 86.7188 (86.7188)  acc5: 98.4375 (98.4375)  time: 0.6597  data: 0.6391  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.37, Acc@5 = 97.19, loss = 1.5663349266293682\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5460 (1.5460)  acc1: 88.2812 (88.2812)  acc5: 96.8750 (96.8750)  time: 0.6218  data: 0.6014  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.57, Acc@5 = 97.22, loss = 1.5651521184776402\n",
      "Test:  [ 0/79]  eta: 0:00:41  loss: 1.5583 (1.5583)  acc1: 86.7188 (86.7188)  acc5: 98.4375 (98.4375)  time: 0.5219  data: 0.5122  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.48, Acc@5 = 97.24, loss = 1.5680410922328127\n",
      "Test:  [ 0/79]  eta: 0:00:45  loss: 1.5480 (1.5480)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.5702  data: 0.5519  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.54, Acc@5 = 97.13, loss = 1.5681443652020226\n",
      "Test:  [ 0/79]  eta: 0:00:53  loss: 1.5457 (1.5457)  acc1: 89.0625 (89.0625)  acc5: 98.4375 (98.4375)  time: 0.6834  data: 0.6648  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.87, Acc@5 = 97.35, loss = 1.5646913881543316\n",
      "Test:  [ 0/79]  eta: 0:00:54  loss: 1.5684 (1.5684)  acc1: 85.9375 (85.9375)  acc5: 97.6562 (97.6562)  time: 0.6898  data: 0.6314  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.68, Acc@5 = 97.49, loss = 1.5643764447562303\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5385 (1.5385)  acc1: 86.7188 (86.7188)  acc5: 98.4375 (98.4375)  time: 0.6222  data: 0.5986  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.87, Acc@5 = 97.58, loss = 1.563951106011113\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5523 (1.5523)  acc1: 85.1562 (85.1562)  acc5: 97.6562 (97.6562)  time: 0.6282  data: 0.6057  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.73, Acc@5 = 97.41, loss = 1.5669620369054094\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5447 (1.5447)  acc1: 86.7188 (86.7188)  acc5: 98.4375 (98.4375)  time: 0.6069  data: 0.5936  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.54, Acc@5 = 97.53, loss = 1.5654794475700282\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5375 (1.5375)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.6148  data: 0.5969  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.02, Acc@5 = 97.27, loss = 1.5656333256371413\n",
      "Test:  [ 0/79]  eta: 0:00:53  loss: 1.5402 (1.5402)  acc1: 88.2812 (88.2812)  acc5: 100.0000 (100.0000)  time: 0.6771  data: 0.6534  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.02, Acc@5 = 97.49, loss = 1.5646244800543483\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5425 (1.5425)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.6503  data: 0.6316  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.95, Acc@5 = 97.65, loss = 1.564592464060723\n",
      "Test:  [ 0/79]  eta: 0:00:53  loss: 1.5400 (1.5400)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 0.6785  data: 0.6542  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.83, Acc@5 = 97.62, loss = 1.5657868672020827\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5501 (1.5501)  acc1: 85.9375 (85.9375)  acc5: 100.0000 (100.0000)  time: 0.6095  data: 0.5863  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.9, Acc@5 = 97.35, loss = 1.5653743095035795\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5411 (1.5411)  acc1: 85.1562 (85.1562)  acc5: 99.2188 (99.2188)  time: 0.6319  data: 0.6055  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.0, Acc@5 = 97.5, loss = 1.5652560131459297\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5346 (1.5346)  acc1: 90.6250 (90.6250)  acc5: 98.4375 (98.4375)  time: 0.6496  data: 0.5993  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.47, Acc@5 = 97.65, loss = 1.5627029802225814\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5470 (1.5470)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.6218  data: 0.5964  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.02, Acc@5 = 97.5, loss = 1.5674287458009357\n",
      "Test:  [ 0/79]  eta: 0:00:53  loss: 1.5521 (1.5521)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.6784  data: 0.6558  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.77, Acc@5 = 97.37, loss = 1.5656005461004716\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5344 (1.5344)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.6463  data: 0.6249  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.22, Acc@5 = 97.66, loss = 1.5624273306206813\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5418 (1.5418)  acc1: 84.3750 (84.3750)  acc5: 99.2188 (99.2188)  time: 0.6155  data: 0.5948  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.87, Acc@5 = 97.64, loss = 1.5641523659983767\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5484 (1.5484)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 0.6143  data: 0.5923  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.61, Acc@5 = 97.59, loss = 1.5650868098947066\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5450 (1.5450)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6512  data: 0.6291  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.82, Acc@5 = 97.65, loss = 1.5637572641614117\n",
      "Test:  [ 0/79]  eta: 0:00:53  loss: 1.5447 (1.5447)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 0.6753  data: 0.6613  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.13, Acc@5 = 97.74, loss = 1.5626367451269416\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5399 (1.5399)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.6197  data: 0.5989  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.93, Acc@5 = 97.44, loss = 1.5666531777080102\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5522 (1.5522)  acc1: 88.2812 (88.2812)  acc5: 96.8750 (96.8750)  time: 0.6603  data: 0.6366  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.0, Acc@5 = 97.36, loss = 1.5662477152257026\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5451 (1.5451)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.6134  data: 0.5723  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.2, Acc@5 = 97.63, loss = 1.563491530056241\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5508 (1.5508)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.6026  data: 0.5781  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.99, Acc@5 = 97.73, loss = 1.5635764040524447\n",
      "Test:  [ 0/79]  eta: 0:00:54  loss: 1.5433 (1.5433)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.6886  data: 0.6651  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.99, Acc@5 = 97.98, loss = 1.5614623404756378\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5519 (1.5519)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.6206  data: 0.6020  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.17, Acc@5 = 97.66, loss = 1.5631678904159159\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5330 (1.5330)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.6410  data: 0.6165  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.82, Acc@5 = 97.62, loss = 1.5653104269051854\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5475 (1.5475)  acc1: 89.8438 (89.8438)  acc5: 97.6562 (97.6562)  time: 0.6208  data: 0.6037  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.93, Acc@5 = 97.8, loss = 1.5635448090637787\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5492 (1.5492)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6590  data: 0.6387  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.08, Acc@5 = 97.88, loss = 1.5635123132150384\n",
      "Test:  [ 0/79]  eta: 0:00:55  loss: 1.5399 (1.5399)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.6974  data: 0.6733  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.08, Acc@5 = 97.78, loss = 1.5627383657648592\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5550 (1.5550)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.6329  data: 0.6056  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.86, Acc@5 = 97.92, loss = 1.5647716763653332\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5460 (1.5460)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.6454  data: 0.6253  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.02, Acc@5 = 97.88, loss = 1.5624746717984164\n",
      "Test:  [ 0/79]  eta: 0:00:54  loss: 1.5533 (1.5533)  acc1: 86.7188 (86.7188)  acc5: 98.4375 (98.4375)  time: 0.6852  data: 0.6384  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.23, Acc@5 = 97.75, loss = 1.5626526754113692\n",
      "Test:  [ 0/79]  eta: 0:00:54  loss: 1.5400 (1.5400)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.6837  data: 0.6580  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.09, Acc@5 = 97.69, loss = 1.5626106941247289\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5267 (1.5267)  acc1: 89.8438 (89.8438)  acc5: 100.0000 (100.0000)  time: 0.5985  data: 0.5836  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.35, Acc@5 = 97.86, loss = 1.5617792545994627\n",
      "Test:  [ 0/79]  eta: 0:00:35  loss: 1.5348 (1.5348)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.4509  data: 0.4396  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.35, Acc@5 = 97.87, loss = 1.5618229334867453\n",
      "Test:  [ 0/79]  eta: 0:00:35  loss: 1.5437 (1.5437)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 0.4556  data: 0.4329  max mem: 0\n",
      "Test: Total time: 0:00:00\n",
      " * Acc@1 = 85.77, Acc@5 = 97.79, loss = 1.5634862848475009\n",
      "Test:  [ 0/79]  eta: 0:00:34  loss: 1.5472 (1.5472)  acc1: 85.9375 (85.9375)  acc5: 98.4375 (98.4375)  time: 0.4318  data: 0.4177  max mem: 0\n",
      "Test: Total time: 0:00:00\n",
      " * Acc@1 = 86.06, Acc@5 = 97.89, loss = 1.562895361381241\n",
      "Test:  [ 0/79]  eta: 0:00:37  loss: 1.5459 (1.5459)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.4716  data: 0.4576  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.41, Acc@5 = 97.9, loss = 1.5627247487442404\n",
      "Test:  [ 0/79]  eta: 0:00:33  loss: 1.5571 (1.5571)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.4269  data: 0.4103  max mem: 0\n",
      "Test: Total time: 0:00:00\n",
      " * Acc@1 = 86.23, Acc@5 = 97.81, loss = 1.5631475207171863\n",
      "Test:  [ 0/79]  eta: 0:00:37  loss: 1.5595 (1.5595)  acc1: 85.1562 (85.1562)  acc5: 99.2188 (99.2188)  time: 0.4749  data: 0.4556  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.31, Acc@5 = 97.79, loss = 1.5628494688227206\n",
      "Test:  [ 0/79]  eta: 0:00:35  loss: 1.5563 (1.5563)  acc1: 85.1562 (85.1562)  acc5: 100.0000 (100.0000)  time: 0.4449  data: 0.4241  max mem: 0\n",
      "Test: Total time: 0:00:00\n",
      " * Acc@1 = 85.88, Acc@5 = 97.83, loss = 1.5639833860759493\n",
      "Test:  [ 0/79]  eta: 0:00:39  loss: 1.5397 (1.5397)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 0.4994  data: 0.4729  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.17, Acc@5 = 97.94, loss = 1.5630132835122603\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5439 (1.5439)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.6507  data: 0.6196  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.1, Acc@5 = 97.88, loss = 1.5645679491984694\n",
      "Test:  [ 0/79]  eta: 0:00:55  loss: 1.5370 (1.5370)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.6972  data: 0.6718  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.98, Acc@5 = 97.78, loss = 1.5640350610395022\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5373 (1.5373)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.6213  data: 0.5966  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.24, Acc@5 = 97.99, loss = 1.562396011775053\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5298 (1.5298)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 0.6703  data: 0.6440  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.96, Acc@5 = 97.93, loss = 1.5645781876165656\n",
      "Test:  [ 0/79]  eta: 0:00:54  loss: 1.5364 (1.5364)  acc1: 86.7188 (86.7188)  acc5: 98.4375 (98.4375)  time: 0.6858  data: 0.6563  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.52, Acc@5 = 97.95, loss = 1.5617862411692172\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5447 (1.5447)  acc1: 85.9375 (85.9375)  acc5: 99.2188 (99.2188)  time: 0.6464  data: 0.6183  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.47, Acc@5 = 97.94, loss = 1.5612148544456386\n",
      "Test:  [ 0/79]  eta: 0:00:57  loss: 1.5436 (1.5436)  acc1: 86.7188 (86.7188)  acc5: 99.2188 (99.2188)  time: 0.7316  data: 0.7073  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.96, Acc@5 = 97.88, loss = 1.5628031778939162\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5452 (1.5452)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 0.6358  data: 0.6154  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.04, Acc@5 = 97.94, loss = 1.5633267933809305\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5362 (1.5362)  acc1: 88.2812 (88.2812)  acc5: 100.0000 (100.0000)  time: 0.6475  data: 0.6207  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.32, Acc@5 = 97.87, loss = 1.5632633272605607\n",
      "Test:  [ 0/79]  eta: 0:00:51  loss: 1.5475 (1.5475)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6493  data: 0.5981  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.42, Acc@5 = 98.03, loss = 1.5619971691807615\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5341 (1.5341)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.2188)  time: 0.6177  data: 0.5888  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.93, Acc@5 = 97.86, loss = 1.5642785422409637\n",
      "Test:  [ 0/79]  eta: 0:00:56  loss: 1.5409 (1.5409)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.7140  data: 0.6859  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.97, Acc@5 = 97.85, loss = 1.5620680036424082\n",
      "Test:  [ 0/79]  eta: 0:00:55  loss: 1.5480 (1.5480)  acc1: 87.5000 (87.5000)  acc5: 99.2188 (99.2188)  time: 0.7061  data: 0.6802  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.96, Acc@5 = 97.88, loss = 1.56296675114692\n",
      "Test:  [ 0/79]  eta: 0:00:54  loss: 1.5475 (1.5475)  acc1: 83.5938 (83.5938)  acc5: 99.2188 (99.2188)  time: 0.6894  data: 0.6618  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.74, Acc@5 = 98.03, loss = 1.5637321638155588\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5354 (1.5354)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (100.0000)  time: 0.6118  data: 0.5932  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.26, Acc@5 = 97.82, loss = 1.563591714146771\n",
      "Test:  [ 0/79]  eta: 0:00:58  loss: 1.5365 (1.5365)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.7408  data: 0.7107  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.83, Acc@5 = 98.04, loss = 1.5626683159719539\n",
      "Test:  [ 0/79]  eta: 0:00:47  loss: 1.5337 (1.5337)  acc1: 89.8438 (89.8438)  acc5: 100.0000 (100.0000)  time: 0.6024  data: 0.5769  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.76, Acc@5 = 97.99, loss = 1.5641437286063085\n",
      "Test:  [ 0/79]  eta: 0:00:57  loss: 1.5498 (1.5498)  acc1: 85.9375 (85.9375)  acc5: 99.2188 (99.2188)  time: 0.7223  data: 0.7002  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.67, Acc@5 = 97.79, loss = 1.5616487717326684\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5357 (1.5357)  acc1: 91.4062 (91.4062)  acc5: 99.2188 (99.2188)  time: 0.6288  data: 0.5998  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.95, Acc@5 = 97.89, loss = 1.563297736493847\n",
      "Test:  [ 0/79]  eta: 0:00:49  loss: 1.5473 (1.5473)  acc1: 89.0625 (89.0625)  acc5: 97.6562 (97.6562)  time: 0.6280  data: 0.5803  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.37, Acc@5 = 97.81, loss = 1.5628037437607971\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5401 (1.5401)  acc1: 91.4062 (91.4062)  acc5: 99.2188 (99.2188)  time: 0.6650  data: 0.6337  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.43, Acc@5 = 98.0, loss = 1.5626867043821118\n",
      "Test:  [ 0/79]  eta: 0:00:57  loss: 1.5357 (1.5357)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.7302  data: 0.7029  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.15, Acc@5 = 97.98, loss = 1.5623893345458597\n",
      "Test:  [ 0/79]  eta: 0:00:48  loss: 1.5255 (1.5255)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 0.6197  data: 0.5891  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.49, Acc@5 = 97.93, loss = 1.5615692998789534\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5370 (1.5370)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.6359  data: 0.6024  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.94, Acc@5 = 98.03, loss = 1.5627969473223142\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5409 (1.5409)  acc1: 89.0625 (89.0625)  acc5: 97.6562 (97.6562)  time: 0.6338  data: 0.6142  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.42, Acc@5 = 98.0, loss = 1.5616552301600009\n",
      "Test:  [ 0/79]  eta: 0:00:50  loss: 1.5543 (1.5543)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.6373  data: 0.6122  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.06, Acc@5 = 97.99, loss = 1.5628520069243033\n",
      "Test:  [ 0/79]  eta: 0:00:52  loss: 1.5530 (1.5530)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.2188)  time: 0.6625  data: 0.6332  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.17, Acc@5 = 97.97, loss = 1.560997970496552\n",
      "Test:  [ 0/79]  eta: 0:00:34  loss: 1.5637 (1.5637)  acc1: 87.5000 (87.5000)  acc5: 98.4375 (98.4375)  time: 0.4361  data: 0.4136  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 85.97, Acc@5 = 97.99, loss = 1.5642607634580588\n",
      "Test:  [ 0/79]  eta: 0:00:37  loss: 1.5354 (1.5354)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 0.4799  data: 0.4539  max mem: 0\n",
      "Test: Total time: 0:00:01\n",
      " * Acc@1 = 86.36, Acc@5 = 98.03, loss = 1.5618168749386752\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "for time in range(1,11):\n",
    "    acc = 0\n",
    "    for i in range(10):\n",
    "        net = NetworkA().to(device)\n",
    "        functional.reset_net(net)\n",
    "        functional.set_step_mode(net,step_mode='m')\n",
    "        functional.set_backend(net, backend='cupy')\n",
    "        weights = torch.load(model_dir)\n",
    "        net.load_state_dict(weights)\n",
    "        net.set_T(time)\n",
    "        # print(net.T)\n",
    "        test_loss, test_acc1, test_acc5 = evaluate(net, criterion, data_loader_test, device=device, header='Test:')\n",
    "        acc += test_acc1\n",
    "    acc_list.append(acc/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'a') as file:\n",
    "    line = ' '.join(map(str, acc_list))\n",
    "    file.write(line + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_com",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
